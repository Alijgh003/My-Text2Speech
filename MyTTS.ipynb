{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alijgh003/My-Text2Speech/blob/master/MyTTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T6H30LhoMTCS",
        "outputId": "74174139-b383-4f4d-fe82-c9ff74cb4e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting TTS\n",
            "  Downloading TTS-0.22.0-cp311-cp311-manylinux1_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.0.12)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.15.3)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from TTS) (2.6.0+cu124)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.13.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.11.0)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.6.1)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (7.5.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (4.67.1)\n",
            "Collecting anyascii>=0.3.0 (from TTS)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.11.15)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (24.2)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.1.1)\n",
            "Collecting pysbd>=0.3.4 (from TTS)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.5.9.post2)\n",
            "Collecting pandas<2.0,>=1.4 (from TTS)\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.10.0)\n",
            "Collecting trainer>=0.0.32 (from TTS)\n",
            "  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting coqpit>=0.0.16 (from TTS)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from TTS) (0.42.1)\n",
            "Collecting pypinyin (from TTS)\n",
            "  Downloading pypinyin-0.55.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hangul-romanize (from TTS)\n",
            "  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from TTS) (3.9.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bangla (from TTS)\n",
            "  Downloading bangla-0.0.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting bnnumerizer (from TTS)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer (from TTS)\n",
            "  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.8.1)\n",
            "Requirement already satisfied: transformers>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (4.53.2)\n",
            "Collecting encodec>=0.1.1 (from TTS)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.3.2 (from TTS)\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting num2words (from TTS)\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.11/dist-packages (from spacy[ja]>=3->TTS) (3.8.7)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from TTS) (2.0.2)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.60.0)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.17.0)\n",
            "Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting numpy>=1.24.3 (from TTS)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.20.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.3)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect>=5.6.0->TTS) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect>=5.6.0->TTS) (4.4.4)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.14.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (2.9.0.post0)\n",
            "Collecting docopt>=0.6.2 (from num2words->TTS)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57.0->TTS) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0,>=1.4->TTS) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->TTS) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.0->TTS) (1.17.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.5.0)\n",
            "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict_core>=20211220 (from spacy[ja]>=3->TTS)\n",
            "  Downloading sudachidict_core-20250825-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (3.18.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1->TTS)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->TTS) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->TTS) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.33.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.5.3)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.1->TTS) (0.5.13)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.22)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.3.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.33.0->TTS) (1.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.17.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.10.0->TTS) (4.3.8)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2025.7.14)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3->spacy[ja]>=3->TTS) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3->spacy[ja]>=3->TTS) (0.1.5)\n",
            "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy>=3->spacy[ja]>=3->TTS)\n",
            "  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy>=3->spacy[ja]>=3->TTS)\n",
            "  Downloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (7.3.0.post1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (5.29.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (0.1.2)\n",
            "Downloading TTS-0.22.0-cp311-cp311-manylinux1_x86_64.whl (937 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m173.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bangla-0.0.5-py3-none-any.whl (5.1 kB)\n",
            "Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n",
            "Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pypinyin-0.55.0-py2.py3-none-any.whl (840 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sudachidict_core-20250825-py3-none-any.whl (72.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m146.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75785 sha256=1a4c4c4eb073bb6f5515207a70bea5982cad0d0bc1492661073a52f8e02737bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/a0/bc/4dacab52579ab464cffafbe7a8e3792dd36ad9ac288b264843\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=6bcb94f309f16df84754848180a080c6e80c055a5ae41b11232cb264b141ca96\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/a4/88/480018a664e58ca7ce6708759193ee51b017b3b72aa3df8a85\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5260 sha256=b18feda71459b292e523b70efc665f603779b3b6bec8a8f09205b246ae033253\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/b9/e3/4145416693824818c0b931988a692676ecd4bbf2ea41d1eedd\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=67ee74657f7f810f903dd9dc248d4897ff99983f0d770145e1c6b2d6d4d7361d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=e031d5064bcce444c3cc5bea831b6765859a3fa5bbe57ca6ecbd628e5e709555\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/10/89/a5908dd7a9a032229684b7679396785e19f816667f788087fb\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498314 sha256=8723e1b67b353b47c6580e3d78b21a6401c5354f5292a914563914bf02fb4701\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/fa/df/5fdf5d3cc26ba859b8698a1f28581d1a6aa081edc6df9847ab\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326858 sha256=f5b8ed272146e7027cc2ab299ba6b67e221f2f89cb6e0ea5897fbb07098b5e8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/30/52/dc5cd222b4bbde285838fed1f96636e96f85cd75493e79a978\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173927 sha256=05a77b2da977db570021015d9e0dfc07cfedf63126e38f8672eac73de6caab45\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/eb/59/30b5d15e56347e595f613036cbea0f807ad9621c75cd75d912\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968767 sha256=932f04eff1c77ec735d034e4a74daba87f0d715ee83117f464e445ee37557aaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/e7/a0/7c416a3eeaa94ca71bf7bcbc6289cced2263d8ba35e82444bb\n",
            "Successfully built gruut encodec bnnumerizer docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: sudachipy, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict_core, python-crfsuite, pysbd, pypinyin, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, num2words, networkx, jsonlines, gruut-ipa, coqpit, anyascii, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, g2pkk, dateparser, blis, nvidia-cusolver-cu12, gruut, thinc, trainer, encodec, TTS\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "plotnine 0.14.6 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.3 bangla-0.0.5 blis-1.2.1 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 networkx-2.8.8 num2words-0.5.14 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pandas-1.5.3 pypinyin-0.55.0 pysbd-0.3.4 python-crfsuite-0.9.11 sudachidict_core-20250825 sudachipy-0.6.10 thinc-8.3.4 trainer-0.0.36 unidecode-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "5249eebae36344cbadd46e245139bd2e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ELtBfBVMYKp"
      },
      "outputs": [],
      "source": [
        "from TTS.api import TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncHUEsPcPUQs"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGGS18evPY7r",
        "outputId": "2598ab52-302d-4819-a398-5eb4eb0182f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TTS.utils.manage.ModelManager object at 0x7f0126c11bd0>\n"
          ]
        }
      ],
      "source": [
        "# Get device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# List available 🐸TTS models\n",
        "print(TTS().list_models())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K2iAo0xXK3g"
      },
      "outputs": [],
      "source": [
        "# === 1) Knobs: set how you want it to behave ===\n",
        "# Switch between raw text OR file upload by setting one of these\n",
        "\n",
        "USE_UPLOAD         = True        # If True, you'll upload a .txt file below\n",
        "RAW_TEXT           = \"\"\"Paste a long text here if not using upload.\"\"\"\n",
        "\n",
        "OUTPUT_DIR         = \"out4\"       # Where paragraph files (and audio) will be saved\n",
        "MAX_CHARS          = 5000        # Max characters per paragraph\n",
        "MAX_SENTENCES      = 20           # Max sentences per paragraph (soft cap)\n",
        "MAKE_AUDIO         = True       # If True, create one .wav per paragraph\n",
        "MODEL_NAME         = \"tts_models/multilingual/multi-dataset/your_tts\"  # You can change it\n",
        "LANGUAGE           = \"en\"        # For multilingual models (e.g., \"en\" for XTTS); otherwise None\n",
        "TRY_TTS_SPLITTER   = True        # Try TTS built-in sentence splitter if available\n",
        "\n",
        "# GPU usage for TTS inference (if audio enabled)\n",
        "USE_GPU            = True        # Colab has a GPU runtime option\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fumfWUcRXbRo",
        "outputId": "7dd059c2-feb7-489f-8c2c-31116746497b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e3d706fd-457b-4b98-9931-d571e71213b5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e3d706fd-457b-4b98-9931-d571e71213b5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1.2410.12229v3.txt to 1.2410.12229v3 (1).txt\n"
          ]
        }
      ],
      "source": [
        "# === 2) Utilities: upload file, splitting, and paragraph packing ===\n",
        "\n",
        "import os, re, math\n",
        "from pathlib import Path\n",
        "\n",
        "# -- (A) Optional upload helper\n",
        "text_source = None\n",
        "if USE_UPLOAD:\n",
        "    from google.colab import files  # Colab only\n",
        "    up = files.upload()             # prompts for file(s)\n",
        "    if up:\n",
        "        fname = list(up.keys())[0]\n",
        "        text_source = Path(fname).read_text(encoding=\"utf-8\")\n",
        "else:\n",
        "    text_source = RAW_TEXT\n",
        "\n",
        "if not text_source or not str(text_source).strip():\n",
        "    raise ValueError(\"No input text provided (set USE_UPLOAD or fill RAW_TEXT).\")\n",
        "\n",
        "# -- (B) Fallback sentence splitter (regex)\n",
        "def fallback_split_sentences(text: str):\n",
        "    # Normalize whitespace and split on sentence enders . ! ?\n",
        "    t = re.sub(r'\\s+', ' ', text.strip())\n",
        "    # Split at boundary after punctuation followed by space\n",
        "    parts = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', t) if s.strip()]\n",
        "    return parts if parts else [t]  # never return empty\n",
        "\n",
        "# -- (C) Pack sentences into paragraphs by size/limits\n",
        "def pack_sentences(sentences, max_chars=1200, max_sentences=4):\n",
        "    paragraphs, buf, count, chars = [], [], 0, 0\n",
        "    for s in sentences:\n",
        "        add_len = len(s) + (1 if buf else 0)  # +1 space if not first in buf\n",
        "        if (buf and (chars + add_len > max_chars)) or (count >= max_sentences):\n",
        "            paragraphs.append(\" \".join(buf))\n",
        "            buf, count, chars = [s], 1, len(s)\n",
        "        else:\n",
        "            buf.append(s)\n",
        "            count += 1\n",
        "            chars += add_len\n",
        "    if buf:\n",
        "        paragraphs.append(\" \".join(buf))\n",
        "    return paragraphs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "VYbpZuIJXnKy",
        "outputId": "94dc490c-1f8a-419e-d077-3e98d43a3a57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nAbstract\\nIn recent years, the introduction of knowledge graphs (KGs) has\\nsignificantly advanced recommender systems by facilitating the discovery of potential associations between items. However, existing\\nmethods still face several limitations. First, most KGs suffer from\\nmissing facts or limited scopes. Second, existing methods convert\\ntextual information in KGs into IDs, resulting in the loss of natural\\nsemantic connections between different items. Third, existing methods struggle to capture high-order connections in the global KG.\\nTo address these limitations, we propose a novel method called CoLaKG, which leverages large language models (LLMs) to improve\\nKG-based recommendations. The extensive knowledge and remarkable reasoning capabilities of LLMs enable our method to supplement missing facts in KGs, and their powerful text understanding\\nabilities allow for better utilization of semantic information. Specifically, CoLaKG extracts useful information from KGs at both local\\nand global levels. By employing the item-centered subgraph extraction and prompt engineering, it can accurately understand the\\nlocal information. In addition, through the semantic-based retrieval\\nmodule, each item is enriched by related items from the entire\\nknowledge graph, effectively harnessing global information. Furthermore, the local and global information are effectively integrated\\ninto the recommendation model through a representation fusion\\nmodule and a retrieval-augmented representation learning module, respectively. Extensive experiments on four real-world datasets\\n∗ Work done during an internship at Tencent.\\n† Corresponding author.\\n\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSIGIR ’25, Padua, Italy\\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-1592-1/2025/07\\nhttps://doi.org/10.1145/3726302.3729932\\n\\ndemonstrate the superiority of our method. The code of our method\\nis available at https://github.com/ziqiangcui/CoLaKG.\\n\\nCCS Concepts\\n• Information systems → Recommender systems.\\n\\nKeywords\\nLarge Language Models, Knowledge Graphs, Recommendation\\n\\n1\\n\\nIntroduction\\n\\nThe rapid advancement of online platforms has led to an increasingly critical issue of information overload. Recommender systems\\naddress this problem by modeling user preferences based on historical data. Collaborative filtering (CF) [9, 10, 23], as one of the\\nmost classic and efficient methods, has been extensively employed\\nin existing recommender systems. However, CF-based methods exclusively rely on user-item interaction records, often suffering from\\nthe data sparsity issue [26]. To address this issue, recent studies\\n[34, 44, 46] have incorporated knowledge graphs (KGs) as external knowledge sources into recommendation models, achieving\\nsignificant progress. Typically, these methods capture diverse and\\nhigh-order relationships between items by modeling the structure\\nand attribute information in KGs, thereby enhancing the learning\\nprocess of user and item representations [44].\\nDespite the effectiveness of existing KG-enhanced recommendation methods, they still face several challenges. i) First, many KGs\\nsuffer from missing facts and limited scopes [8, 32], as constructing\\nKGs often requires manual effort and domain expertise. The absence\\nof key attributes, such as the genres of a movie, can cause items\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\nItem node\\n\\nAttribute node\\n\\nx-hop\\n\\n𝑟!\\n\\na. Missing facts:\\n\\n2-hop\\n\\nH\\n\\nF\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nA\\n\\n𝑟%\\n\\nD\\nE\\n\\n𝑟!\\n\\n𝑟$\\n\\n1-hop\\n\\n𝑟!\\n𝑟\"\\n\\nA\\n\\n𝑟$\\n\\n𝑟#\\n\\nP\\n\\nG\\n\\n𝑟$\\n\\nC\\n\\n𝑟!\\n\\nB\\n\\n𝑟!\\n\\nC\\n\\nb. Textual Entities (e.g., Title/Tag/Genre..)\\nSimilar semantic\\n\\nB\\n\\nD\\n\\nHorror\\nID: 51\\n\\n𝑟!\\n\\nThriller\\nID: 320\\n\\nE\\n\\nDifferent IDs\\nc. Distant neighbors\\nTarget item\\nB\\n•\\n•\\n\\nC\\n\\nG\\n\\nH\\n\\nA\\n\\n2-hop\\n\\nA\\n\\nx-hop\\n\\nH\\n\\nB, C, G have no connections with A in the KG\\nH and A are too distant in the KG\\n\\nFigure 1: An illustrative diagram demonstrating the potential\\nissues of existing KG-based recommendation methods.\\nthat originally share the same attribute to lose their connections. As\\nillustrated in Figure 1, items A and B should have a connecting path\\n(A-P-B) in the KG. However, due to item A missing the P attribute,\\nA and B are now not associated with each other. Such biased knowledge will lead to suboptimal performance of the recommendation\\nmodel. ii) Second, existing methods [33–35, 43, 44] convert textual\\nentities and relations into IDs for use, which can result in losing\\nnatural semantic connections between items. For instance, in Figure\\n1, “horror” and “thriller” are two semantically related attributes of\\nItem F and Item G, respectively. However, similar semantics are\\nnot reflected in different entity IDs (51 and 320), which further results in the related items F and G having no connections in the KG.\\niii) Third, existing methods [25, 33–35, 43, 44] struggle to capture\\nhigh-order relationships in the entire KG. Most of them propagate\\nand aggregate information by stacking multiple layers of graph\\nneural networks (GNNs). The layer-by-layer propagation is not\\nonly inefficient but also accumulates a large amount of irrelevant\\nnode information, leading to the over-smoothing issue [8, 33]. For\\ninstance, assumming that points A and H in Figure 1 have a strong\\nsemantic connection, the considerable distance between them in\\nthe KG presents big challenges for existing methods in capturing\\nthis relation. As shown in the lower left corner of Figure 1, due\\nto the aforementioned three limitations, existing methods fail to\\nidentify the potential connections between historically interacted\\nitems and the target item through the KG, resulting in suboptimal\\nrecommendations.\\nEmpowered by extensive knowledge and remarkable reasoning\\nabilities, large language models (LLMs) have demonstrated significant promise in semantic understanding and knowledge extraction.\\nRecently, many studies have leveraged LLMs to improve recommendation models. For example, some studies utilize LLMs to generate\\nsemantic representations of item profiles [22], while other studies\\n[48] employ LLMs to determine whether a complementary relationship exists between two items, thereby recommending complementary products based on users’ historical behaviors. However, these\\nmethods do not fully exploit the semantic and structural information of KGs. As one of the most common and important sources of\\nknowledge, KGs contain a wealth of semantic associations among\\nentities and relations, which are often overlooked by existing methods that typically consider only item profiles. Additionally, KGs\\nserve as task-relevant knowledge repositories, effectively aiding\\n\\nLLMs in acquiring task-specific knowledge and mitigating the issue of hallucinations caused by excessive divergence. Nevertheless,\\nleveraging LLMs to handle the diverse semantic relationships in\\nKGs is highly challenging, as it is unrealistic to input the vast number of entities and connections in KGs into a language model.\\nTo bridge this gap, in this paper, we propose a novel method\\nnamed Comprehending Knowledge Graphs with Large Language\\nModels for Recommendation (CoLaKG). The core idea is to leverage\\nLLMs for understanding the semantic and structural information of\\nKGs to enhance the representation learning of items and users. Our\\nmethod comprises two stages: i) Comprehending KGs with LLMs.\\nGiven the impracticality of inputting the entire KG into an LLM,\\nCoLaKG leverages KG information by two components: a Local KG\\nComprehension module which involves extracting item-centered\\nKG subgraphs for analysis by LLMs, and a Retrieval-based Global\\nKG Utilization module which retrieves semantically related items\\nfrom the entire KG to effectively leverage global KG information. ii)\\nIncorporating semantic embeddings into the recommendation model.\\nThis stage integrates the semantic embeddings of KGs with the\\nID embeddings, thereby leveraging both collaborative signals and\\nsemantic information. First, we align and fuse the item ID representations with the item-centered KG subgraph representations. Then,\\nwe employ a retrieval-augmented representation method to further\\nenhance item representations with semantically similar neighbors\\nfrom the entire KG.\\nIt is important to note that these two stages are decoupled, meaning that our method does not involve LLM inference during the\\nrecommendation process. This allows our method to be efficiently\\napplied in real-world recommendation scenarios.\\nOur contributions are summarized as follows:\\n• We propose a novel method that utilizes LLMs to comprehend and\\ntransform the semantic and structural information of KGs. This\\napproach addresses the issues of missing facts and the inability\\nto leverage semantic information from text in current KG-based\\nrecommendation methods.\\n• We propose leveraging both local KG information and global\\nKG information with LLMs. In addition, we design a retrievalaugmented method to enhance item representation learning.\\n• Extensive experiments are conducted on four real-world datasets\\nto validate the superiority of our method. Further analysis demonstrates the rationale behind our approach.\\n\\n2 Related Work\\n2.1 Knowledge-aware Recommendation\\nExisting knowledge-aware recommendation methods can be categorized into three types [8]: embedding-based methods, path-based\\nmethods, and GNN-based methods. Embedding-based methods\\n[3, 19, 29, 46] enhance the representations of users and items by\\nleveraging the relations and entities within the KGs. Notable examples include CKE [46], which integrates various types of side\\ninformation into a collaborative filtering framework using TransR\\n[20]. Another example is DKN [29], which improves news representations by combining textual embeddings of sentences and\\nknowledge-level embeddings of entities. Path-based methods leverage KGs to explore long-range connectivity [13, 36, 45, 47]. For\\nexample, Personalized Entity Recommendation (PER) [45] treats a\\n\\n\\x0cComprehending Knowledge Graphs with Large Language Models for Recommender Systems\\nItem-centered KG Subgraphs\\n\\n𝑟$\\n\\n…\\n\\nTh\\neR\\nea\\n\\nde\\nr\\n\\nKa\\n\\nte\\n\\n𝑟&\\n\\n…\\n\\n…\\n\\nn\\nio\\npt\\nce\\nIn\\n\\n…\\n|𝒰|\\n\\nPrompt Engineering\\n\\n𝑣\"\\n𝑟($! ,$\")\\n\\n𝑟\\n𝑁% (𝑣& )\\n\\nAdapter\\n\\n❄\\n𝑠\\'\\n\\n𝑠\"!\\n\\n𝑟* Acted by\\n|𝒱|\\n\\n𝑟) Directed\\n𝑟( Is the genre of\\n\\nUser\\n\\n𝑠\\'\\n\\nItem\\n\\n𝒫(#)\\nText\\nEmbedding\\nModel\\n\\n𝑒\"\\n\\n𝑠\"\\n\\n𝑣!\\n\\nUser Preference\\nComprehension\\n\\nRetrieval-Augmented Representation Learning\\n\\n❄\\n\\n𝑣!\\n\\nLLM\\n\\n…\\n\\nRetrieval-based Global KG Utilization\\n𝑟(\"! ,\"\" )\\n\\n𝑟+ Genre\\n\\nText\\nEmbedding\\nModel\\n\\nAssume you are a film expert …\\n\\n𝑟# 𝑟!\\n\\nRelations\\n𝑟, Directed by\\n\\n𝑠\"\\n\\n𝒫(#)\\n\\nKG-based user interactions:\\n{“Titanic”:\\n(Titanic, Directed by, James Cameron); (Titanic,\\nActed by, Leonardo | Kate); (Titanic, Genres,\\nRomance | Disaster);… }\\n{“Inception”:\\n(Inception, Directed by, Nolan); (Inception,\\nActed by, Leonardo DiCaprio | Ellen Page);\\n(Inception, Genres, Science Fiction | Action);… }\\n\\n…\\n𝑟\"\\n\\nLocal KG\\nComprehension\\n\\nℎ\"\\n\\n𝑠\"(\\n𝑠\\'(\\n\\nEntities other\\nthan items\\nAddition\\n\\n|𝒰|\\n\\nInner product\\n\\n❄ Freeze\\n\\nU-I Modeling ℱ(-)\\nℎ)\"\\n\\nℎ\"(\\n…\\n\\n…\\n\\n…\\n\\n𝑟# 𝑟!\\n\\nLLM\\n\\n𝑟\\' Starred in\\n\\n|𝒱|\\n\\nUser-centered Subgraphs\\n\\n𝑟! 𝑟\"\\n\\nPrompt Engineering\\nAssume you are a film expert …\\nAssume you are a film expert …\\nOne-hop knowledge in the format of triples :\\n(Titanic, Directed by, James Cameron);\\n(Titanic, Acted by, Leonardo DiCaprio …);\\n(Titanic, Genres, Romance); (Titanic, plot, …);\\n…\\nTwo-hop knowledge:\\nJames Cameron also directed… ;\\nLeonardo DiCaprio also starred in… ;\\nMovies in the similar genres also include … ;\\nMovies with the similar plot also include … ;\\n...\\n\\nℎ\\'\\n\\n𝛼!\"\\n\\n𝑒\\'\\n\\nℎ\\'\\n\\nℎ)\\'\\n\\n𝑦+\\'\"\\n\\n…\\n\\nt\\nAlien\\nlie\\ns\\nJu\\nAv\\n𝑟$ …\\n&\\nala\\neo James n\\nnc\\nm\\no\\ner\\nhe\\nRo\\nm\\na\\n𝑟&\\nC\\n𝑟&\\n𝑟&\\n𝑟! Disaster\\ne\\n𝑟\"\\nc\\n𝑟&\\nan 𝑟\"\\nm\\n𝑟#\\n𝑟#\\nRo\\nTitanic\\nLeo\\n𝑟%\\n𝑟%\\nnard\\no\\n𝑟%\\n𝑟%\\n\\nUser-Item-Entity\\nGraph\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\n𝑠\"\"\\n\\nFigure 2: The framework of our proposed CoLaKG.\\nKnowledge Graph (KG) as a heterogeneous information network\\nand extracts meta-path-based latent features to represent the connectivity between users and items along various types of relational\\npaths. MCRec [13] constructs meta-paths and learns the explicit\\nrepresentations of meta-paths to depict the interaction context\\nof user-item pairs. Despite their effectiveness, these approaches\\nheavily rely on domain knowledge and human effort for metapath design. Recently, GNN-based methods have been proposed,\\nwhich enhance entity and relation representations by aggregating\\nembeddings from multi-hop neighbors [30, 34, 35]. For instance,\\nKGAT [34] employs the graph attention mechanism to propagate\\nembeddings and utilizes multi-layer perceptrons to generate final\\nrecommendation scores in an end-to-end manner. Similarly, KGIN\\n[35] adopts an adaptive aggregation method to capture fine-grained\\nuser intentions. Additionally, some methods [33, 43, 44, 51] employ\\ncontrastive learning to mitigate potential knowledge noise and\\nidentify informative knowledge connections.\\n\\n2.2\\n\\nLLMs for Recommendation\\n\\nIn light of the emergence of large language models and their remarkable achievements in the field of NLP, scholars have begun\\nto explore the potential application of LLMs in recommender systems [4, 6, 39, 49]. Due to the powerful reasoning capabilities and\\nextensive world knowledge of LLMs, they have been already naturally applied to zero-shot [11, 12, 31] and few-shot recommendation\\nscenarios [2, 18]. In these studies, LLMs are directly used as a recommendation model [17, 50], where the output of LLMs is expected to\\noffer a reasonable recommendation result [40]. However, when the\\ndataset is sufficiently large, their performance often falls short of\\nthat achieved by traditional recommendation models. Another line\\nof research involves leveraging LLMs as feature extractors. These\\nmethods [1, 14, 15, 22, 22, 37, 38, 52] generate intermediate decision\\n\\nresults or semantic embeddings of users and items, which are then\\ninput into traditional recommendation models to produce the final\\nrecommendations. Unlike existing methods, our approach aims to\\nleverage the extensive knowledge and reasoning capabilities of\\nLLMs to understand KGs and transform them into semantic embeddings, thereby addressing existing issues in KG-based recommender\\nsystems.\\n\\n3\\n\\nPreliminaries\\n\\nUser-Item Interaction Graph. Let U and V denote the user set\\nand item set, respectively, in a recommender system. We construct\\na user-item bipartite graph G = {(𝑢, 𝑦𝑢𝑣 , 𝑣)|𝑢 ∈ U, 𝑣 ∈ V} to\\nrepresent the collaborative signals between users and items. Here,\\n𝑦𝑢𝑣 = 1 if user 𝑢 interacted with item 𝑣, and vice versa.\\nKnowledge Graph. We capture real-world knowledge about items\\nusing a heterogeneous graph composed of triplets, represented as\\nG𝑘 = {(ℎ, 𝑟, 𝑡)}. In this context, ℎ and 𝑡 are knowledge entities\\nbelonging to the set E, while 𝑟 is a relation from the set R that\\nlinks them, as exemplified by the triplet (James Cameron, directed,\\nTitanic). Notably, the item set is a subset of the entity set, denoted\\nas V ⊂ E. This form of knowledge graph enables us to model the\\nintricate relationships between items and entities.\\nTask Formulation. Following the task format of most KG-aware\\nrecommendation models, we formulate the task as follows: Given\\nthe user-item interaction graph G and the corresponding knowledge graph G𝑘 , our objective is to learn a recommendation model\\nthat predicts the probability of user 𝑢 interacting with item 𝑣.\\n\\n4\\n\\nMethodology\\n\\nIn this section, we introduce our proposed method CoLaKG in\\ndetail. An overview of our method is illustrated in Figure 2. CoLaKG extracts useful information from the KG at both local and\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nAssume you are an expert in movie recommendation. You will be given a certain movie with its\\nfirst-order information (in the form of triples) and some second-order relationships (movies related\\nto this movie). Please complete the missing knowledge, summarize the movie and analyze what\\nkind of users would like it.\\nFirst-order info: (Titanic, Directed by, James Cameron), (Titanic, Acted by, Leonardo DiCaprio), …\\nSecond-order info: The movies with the same director (James Cameron) also include: Aliens, …\\nThe movies with the same actor (Leonardo DiCaprio) also include: Inception, …\\n\\n…\\n\\nFigure 3: The prompt template for Local KG comprehension.\\nglobal levels. By employing item-centered subgraph extraction and\\nprompt engineering, it accurately captures the local KG. Subsequently, through retrieval-based neighbor enhancement, it supplements the current item by capturing related items from the entire\\nKG, thereby effectively utilizing global information. The local and\\nglobal information extracted by the LLM are effectively integrated\\ninto the recommendation model through a representation fusion\\nmodule and a retrieval-augmented representation learning module,\\nrespectively, thereby improving recommendation performance.\\n\\n4.1\\n\\nKG Comprehension with LLMs\\n\\nKnowledge graphs have been widely utilized in recommender systems to provide semantic information and model latent associations\\nbetween items. However, KGs are predominantly manually curated,\\nleading to missing facts and limited knowledge scopes. Additionally,\\nthe highly structured nature of KGs poses challenges for utilizing\\ntextual information. To address these limitations, we propose the\\nuse of LLMs to enhance the understanding and refinement of KGs\\nfor improved recommendations. To fully leverage KGs, our approach comprises two components: 1) Local KG Comprehension,\\nwhich involves extracting item-centered KG subgraphs for analysis by LLMs, and 2) Retrieval-based Global KG Utilization, which\\nutilizes the semantic similarity of KG subgraphs to retrieve semantically related items from the entire KG, thereby leveraging global\\nKG information.\\n4.1.1 Local KG Comprehension. In this part, we introduce using LLMs to comprehend local KG information. Here, the local\\nKG information of an item refers to the KG subgraph centered\\non that item, including connections within two hops. First, we\\nrepresent the first-order KG subgraph centered on each item (i.e.,\\nego network [21]) using triples. Specifically, given an item 𝑣 ∈ V,\\nwe use T𝑣 = {(𝑣, 𝑟, 𝑒)|(𝑣, 𝑟, 𝑒) ∈ G𝑘 } to denote the set of triplets\\nwhere 𝑣 is the head entity. In the context of recommendation, the\\nfirst-order neighboring entities of an item in a KG are usually attributes. Therefore, we use 𝑒 to represent these attribute entities\\nto distinguish them from those item entities 𝑣. During generating triples, in cases where the attribute or relation is absent, the\\nterm “missing” is employed as a placeholder. Next, we consider the\\nsecond-order relations in KGs. The number of entities in an ego\\nnetwork centered on a single entity increases exponentially with\\nthe growth of the radius. However, the input length of an LLM is\\nstrictly limited. Consequently, including all second-order neighbors\\nassociated with the central item in the prompt becomes impractical. To address this issue, we adopt a simple but effective strategy,\\nrandom sampling, to explore second-order connections of 𝑣. Let\\nE𝑣 = {𝑒 | (𝑣, 𝑟, 𝑒) ∈ T𝑣 } denote the set of first-order connected\\nneighbors of 𝑣. For each 𝑒 ∈ E𝑣 , we randomly sample 𝑚 triples\\nfrom the set T𝑒 to construct the triples of second-order connections, denoted as T𝑒𝑚 . Here, T𝑒 = {(𝑒, 𝑟, 𝑣 ′ ) | (𝑒, 𝑟, 𝑣 ′ ) ∈ G𝑘 , 𝑣 ′ ≠ 𝑣 }\\nrepresents the set of triples where 𝑒 ∈ E𝑣 is the head entity.\\n\\nAfter converting first-order and second-order relationships into\\ntriples, we transform these triples into textual form. For first-order\\nrelations, we concatenate all the first-order triples in T𝑣 to form\\na single text, denoted as D𝑣 . For second-order relations, we use a\\ntemplate to transform the second-order triples T𝑒𝑚 into coherent\\nsentences D𝑣′ , facilitating the understanding of the LLM. In addition\\nto D𝑣 and D𝑣′ , we have carefully designed a system prompt I𝑣 as\\nthe instruction to guide the generation. By combining I𝑣 , D𝑣 , and\\nD𝑣′ , we obtain the prompt, which is shown in Figure 3. The prompt\\nenables the LLM to fully understand, complete, and refine the KG,\\nthereby generating the final comprehension for the 𝑣-centered KG\\nsubgraph. This process can be formulated as follows:\\nC𝑣 = LLMs(I𝑣 , D𝑣 , D𝑣′ ).\\n\\n(1)\\n\\nOnce we have obtained the LLM’s comprehension of the KG subgraphs, we need to convert these textual answers into continuous vectors for utilization in downstream recommendation models.\\nHere, we employ a pre-trained text embedding model P to transform C𝑣 into embedding vectors s𝑣 , which can be formulated as:\\ns𝑣 = P (C𝑣 ).\\n\\n(2)\\n\\n4.1.2 Rerieval-based Global KG Utilization. This section introduces the utilization of global KG information. In recommendation\\nscenarios, items that are distant in the KG can still have close semantic associations. However, the number of KG nodes increases\\nexponentially with the number of hops, and the input length of\\nLLMs is limited by the number of tokens. This makes it impractical to input the entire KG into an LLM directly. To address this\\nchallenge, we propose a method called retrieval-based global KG\\nutilization.\\nFor each item, we have obtained the semantic embedding corresponding to its local KG. Based on this, we can directly compute\\nthe semantic relationships between any two item-centered KG subgraphs. specifically, we employ the cosine similarity as a metric to\\nquantify the relations. Given two different items 𝑣𝑖 and 𝑣 𝑗 , their\\nsemantical relation 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) in the global KG is computed as:\\n𝑟 (𝑣𝑖 ,𝑣 𝑗 ) = sim(s𝑣𝑖 , s𝑣 𝑗 ), 𝑣𝑖 , 𝑣 𝑗 ∈ V and 𝑣𝑖 ≠ 𝑣 𝑗 ,\\n\\n(3)\\n\\nwhere sim denotes the cosine similarity function. Once we obtain\\nthe semantic associations between any two items in the entire KG,\\nwe treat the semantic similarity between the two items as the edge\\nweight between them, allowing us to construct an item-item graph:\\nG𝑣 = {(𝑣𝑖 , 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) , 𝑣 𝑗 )|𝑣𝑖 , 𝑣 𝑗 ∈ V, 𝑣𝑖 ≠ 𝑣 𝑗 }.\\n\\n(4)\\n\\nFrom a high-level perspective, we transform high-order associations\\nbetween items in the KG into direct semantic connections on the\\nconstructed item-item graph G𝑣 . Based on this foundation, it is\\nessential to retrieve items that are semantically strongly related\\nto the given item, as items with lower semantic relevance may\\nintroduce noise. Specifically, given an item 𝑣𝑖 , we rank all other\\nitems 𝑣 𝑗 ∈ V where 𝑣 𝑗 ≠ 𝑣𝑖 in descending order based on the\\nsemantic similarity 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) . Subsequently, we retrieve the top-𝑘\\nitems with the highest similarity scores, forming the neighbor set\\nof 𝑣𝑖 : N𝑘 (𝑣𝑖 ), where 0 < 𝑘 < |V | is an adjustable hyperparameter,\\nrepresenting the number of retrieved neighbors.\\nIn this manner, we filter out items with low semantic associations to the current item in the entire KG. Traditional KG-based\\n\\n\\x0cComprehending Knowledge Graphs with Large Language Models for Recommender Systems\\n\\nrecommendation methods aggregate items through layer-by-layer\\ninformation propagation on the KG. High-order relations require\\nmany propagation and aggregation steps to be captured. In contrast,\\nour retrieval-based method can directly recall strongly semantically related neighbors of any order across the entire KG. The\\nretrieved neighbors are then leveraged to enhance item representations, which will be introduced in Section 4.3.2.\\n\\n4.2\\n\\nUser Preference Comprehension\\n\\nThe introduction of KGs allows for the expansion of user-item\\nbipartite graphs and enables us to understand user preferences from\\na knowledge-driven perspective. Given a user 𝑢, we first extract\\nthe subgraph corresponding to user 𝑢 from the user-item bipartite\\ngraph, denoted as B𝑢 . For each item 𝑣 ∈ B𝑢 , we extract its firstorder KG subgraph and represent it as a set of triples, denoted\\nas T𝑣 . We then concatenate all triples in T𝑣 to form a single text,\\ndenoted as D𝑣 . The detailed approach is the same as described in\\nSection 4.1.1. Subsequently, we represent user 𝑢 with all items the\\nuser has interacted with in the training set and the corresponding\\nknowledge triples D𝑣 :\\nD𝑢 = ⊕𝑣 ∈ G𝑢 {name𝑣 : D𝑣 },\\n\\n(5)\\n\\nwhere ⊕ denotes concatenation operation, and name𝑣 denotes the\\ntext name of item 𝑣. Additionally, we have meticulously designed a\\nsystem prompt, denoted as I𝑢 , to serve as an instruction for guiding\\nthe generation of user preferences. By combining D𝑢 and I𝑢 , we\\nenable the LLM to comprehend the user preference for 𝑢, which\\ncan be formulated as:\\nC𝑢 = LLMs(I𝑢 , D𝑢 ).\\n\\n(6)\\n\\nFurthermore, we also utilize the text embedding function P to\\ntransform the textual answers C𝑢 into embedding vectors s𝑢 , which\\ncan be expressed as:\\ns𝑢 = P (C𝑢 ).\\n(7)\\n\\n4.3\\n\\nRetrieval-Augmented Representation\\n\\n4.3.1 Cross-Modal Representation Alignment. In a traditional\\nrecommendation model, each item and user is associated with an\\nID embedding. Let e𝑣 ∈ R𝑑 represent the ID embedding of item\\n𝑣 and e𝑢 ∈ R𝑑 represent the ID embedding of user 𝑢. In addition,\\nwe also obtain the semantic embedding s𝑣 ∈ R𝑑𝑠 w.r.t. the comprehension of 𝑣-centric KG subgraph, and the semantic embedding\\ns𝑢 ∈ R𝑑𝑠 w.r.t. the comprehension of user 𝑢’s preference. Since\\nID embeddings and semantic embeddings belong to two different\\nmodalities and typically possess different embedding dimensions,\\nwe employ a learnable adapter network to align the embedding\\nspaces. Specifically, the adapter consists of a linear map and a nonlinear activation function, formulated as:\\ns′𝑣 = 𝜎 (W1 s𝑣 );\\n\\ns𝑢′ = 𝜎 (W2 s𝑢 ),\\n\\n(8)\\n\\nwhere both W1 ∈ R𝑑 ×𝑑𝑠 and W2 ∈ R𝑑 ×𝑑𝑠 are are weight matrices, 𝜎 represents the non-linear activation function ELU [5]. Note\\nthat during the training process, we fix s𝑣 and s𝑢 , training solely\\nthe projection parameters W1 and W2 , and the parameters of the\\nrecommendation model. After mapping the representations to the\\nsame space, we need to fuse the representations of the two modalities, leveraging both the collaborative signals and the semantic\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\ninformation to form a complementary representation. To achieve\\nthis, we employ a straightforward mean pooling technique to fuse\\ntheir embeddings:\\nh𝑣 =\\n\\n1\\n(e𝑣 + s′𝑣 );\\n2\\n\\nh𝑢 =\\n\\n1\\n(e𝑢 + s𝑢′ ),\\n2\\n\\n(9)\\n\\nwhere h𝑣 ∈ R𝑑 and h𝑢 ∈ R𝑑 represent the merged embeddings of\\nitem 𝑣 and user 𝑢, respectively.\\n4.3.2 Item Representation Augmentation with Retrieved\\nNeighbors. For each item, we have retrieved its semantic-related\\nitems in Section 4.1.2. To fully utilize these neighbors, we propose\\nto aggregate their information to enhance item representations.\\nConsidering the varying contributions of different neighbors to the\\ncentral item, we employ the attention mechanism. Specifically, for\\nitem 𝑣𝑖 and its top-𝑘 neighbor set N𝑘 (𝑣𝑖 ), we compute attention\\ncoefficients that indicate the importance of item 𝑣 𝑗 ∈ N𝑘 (𝑣𝑖 ) to\\nitem 𝑣𝑖 as follows:\\n𝑤𝑖 𝑗 = 𝑎(Ws𝑣𝑖 ∥Ws𝑣 𝑗 ).\\n\\n(10)\\n\\nHere, W ∈ R𝑑𝑎 ×𝑑 is a learnable weight matrix to capture higherlevel features of s𝑣𝑖 and s𝑣 𝑗 , ∥ is the concatenation operation, 𝑎\\ndenotes the attention function: R𝑑𝑎 × R𝑑𝑎 → R, where we adopt a\\nsingle-layer neural network and apply the LeakyReLU activation\\nfunction following [27]. Note that the computation of attention\\nweights is exclusively dependent on the semantic representation\\nof items, as our objective is to calculate the semantic associations\\nbetween items, rather than the associations present in collaborative signals. In addition, we employ the softmax function for easy\\ncomparison of coefficients across different items:\\n𝛼𝑖 𝑗 = softmax 𝑗 (𝑤𝑖 𝑗 ).\\n\\n(11)\\n\\nThe attention scores 𝛼𝑖 𝑗 are then utilized to compute a linear combination of the corresponding neighbor embeddings. Finally, the\\nweighted average of neighbor embeddings and the embedding of\\nitem 𝑣𝑖 itself are combined to form the final output representation\\nfor item 𝑣𝑖 :\\n\\x12 \\x10\\n\\x11\\x13\\n∑︁\\n1\\nh𝑣𝑖 +\\nh′𝑣𝑖 = 𝜎\\n𝛼 𝑖 𝑗 h𝑣 𝑗 ,\\n(12)\\n𝑗 ∈ N𝑘 (𝑣𝑖 )\\n2\\nwhere 𝜎 denotes the non-linear activation function.\\n\\n4.4\\n\\nUser-Item Modeling\\n\\nHaving successfully integrated the semantic information from the\\nKG into both user and item representations, we can use them as inputs for traditional recommendation models to generate prediction\\nresults. This process can be formulated as follows:\\n𝑦ˆ𝑢𝑣 = F (h𝑢 , h′𝑣 ),\\n\\n(13)\\n\\nwhere 𝑦ˆ𝑢𝑣 is the predicted probability of user 𝑢 interacting with\\nitem 𝑣, h𝑢 is the representation for user 𝑢, h′𝑣 is the augmented\\nrepresentation for item 𝑣, and F denotes the function of the recommendation model.\\nSpecifically, we select the classic model, LightGCN [10], as the\\narchitecture for our recommendation method due to its simplicity\\nand effectiveness. The trainable parameters of original LightGCN\\nare only the embeddings of users and items, similar to standard\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nTable 1: Dataset statistics.\\nStatistics\\n\\nMovieLens\\n\\nLast-FM\\n\\nMIND\\n\\nFunds\\n\\n# Users\\n# Items\\n# Interactions\\n\\n6,040\\n3,260\\n998,539\\n\\n1,859\\n2,813\\n86,608\\n\\n44,603\\n15,174\\n1,285,064\\n\\n209,999\\n5,701\\n1,225,318\\n\\n# Entities\\n# Relations\\n# Triples\\n\\n12,068\\n12\\n62,958\\n\\n32,810\\n14\\n307,140\\n\\n8,111\\n12\\n65,697\\n\\n5 Experiments\\n5.1 Experimental Settings\\n\\nKnowledge Graph\\n9,614\\n2\\n118,500\\n\\nmatrix factorization. First, we adopt the simple weighted sum aggregator to learn the user-item interaction graph, which is defined\\nas:\\n(𝑙+1)\\n\\nh𝑢\\n\\n=\\n\\n1\\n\\n∑︁\\n√︁\\n𝑣 ∈ M𝑢\\n\\n(𝑙 )\\n\\n|M𝑢 ||M 𝑣 |\\n\\n(𝑙+1)\\n\\nh𝑣 ; h𝑣\\n\\n=\\n\\n1\\n\\n∑︁\\n√︁\\n𝑢 ∈ M𝑣\\n\\n|M 𝑣 ||M𝑢 |\\n\\n(𝑙 )\\n\\nh𝑢 ,\\n\\n(14)\\n(𝑙 )\\n(𝑙 )\\nwhere h𝑢 and h𝑣 represent the embeddings of user 𝑢 and item 𝑣\\nafter 𝑙 layers of propagation, respectively. The initial embeddings\\n(0)\\n(0)\\nh𝑢 = h𝑢 and h𝑣 = h′𝑣 are obtained in Section 4.3. M𝑢 denotes\\nthe set of items with which user 𝑢 has interacted, while M 𝑣 signifies\\nthe set of users who have interacted\\n√︁ with item 𝑣. The symmetric\\nnormalization term is given by 1/ |M𝑢 ||M 𝑣 |. Subsequently, the\\nembeddings acquired at each layer are combined to construct the\\nfinal representation:\\nh̃𝑢 =\\n\\n𝐿\\n\\n𝐿\\n\\n𝑙=0\\n\\n𝑙=0\\n\\n1 ∑︁ (𝑙 )\\n1 ∑︁ ′ (𝑙 )\\nh𝑢 ; h̃𝑣 =\\nh𝑣 ,\\n𝐿+1\\n𝐿+1\\n\\n(15)\\n\\nwhere 𝐿 represents the number of hidden layers. Ultimately, the\\nmodel prediction is determined by the inner product of the final\\nuser and item representations:\\n𝑦ˆ𝑢𝑣 = h̃𝑢⊤ h̃𝑣 .\\n\\n4.5\\n\\n(16)\\n\\nModel Training\\n\\nOur approach can be divided into two stages. In the first stage, we\\nemploy the LLM to comprehend the KGs, generating corresponding\\nsemantic embeddings for each item and user, denoted as s𝑣 and\\ns𝑢 , respectively. In the second stage, these semantic embeddings\\nare integrated into the recommendation model to enhance its performance. Only the second stage necessitates supervised training,\\nwhere we adopt the widely-used Bayesian Personalized Ranking\\n(BPR) loss:\\nL=\\n\\n∑︁\\n\\n−ln𝜎 (𝑦ˆ𝑢𝑣 + − 𝑦ˆ𝑢𝑣 − ) + 𝜆∥Θ∥ 22 .\\n\\n(17)\\n\\n(𝑢,𝑣 + ,𝑣 − ) ∈ O\\n\\nHere, O = {(𝑢, 𝑣 +, 𝑣 − )|(𝑢, 𝑣 + ) ∈ R +, (𝑢, 𝑣 − ) ∈ R − } represents\\nthe training set, R + denotes the observed (positive) interactions\\nbetween user 𝑢 and item 𝑣, while R − indicates the sampled unobserved (negative) interaction set. 𝜎 (·) is the sigmoid function.\\n𝜆∥Θ∥ 22 is the regularization term, where 𝜆 serves as the weight\\ncoefficient and Θ constitutes the model parameter set.\\n\\n5.1.1 Datasets. We conducted experiments on four real-world\\ndatasets, including three public datasets (MovieLens1 , MIND2 , LastFM3 ), and one industrial dataset (Fund). The statistics for these\\ndatasets are presented in Table 1. These datasets cover a wide\\nrange of application scenarios. Specifically, MovieLens is a wellestablished benchmark that collects movie ratings provided by users.\\nMIND is a large-scale news recommendation dataset constructed\\nfrom user click logs on Microsoft News. Last-FM is a well-known\\nmusic recommendation dataset that includes user listening history\\nand artist tags. The Fund dataset is sampled from the data of a\\nlarge-scale online financial platform aiming to recommend funds\\nfor users. We adopt the similar setting as numerous previous studies\\n[10, 42], filtering out items and users with fewer than five interaction records. For each dataset, we randomly select 80% of each\\nuser’s historical interactions to form the training set, while the\\nremaining 20% constitute the test set, following [10]. Each observed\\nuser-item interaction is considered a positive instance, and we apply a negative sampling strategy by pairing it with one negative\\nitem that the user has not interacted with.\\n5.1.2 Evaluation Metrics. To evaluate the performance of the models, we employ widely recognized evaluation metrics: Recall and\\nNormalized Discounted Cumulative Gain (NDCG), and report values of Recall@k and NDCG@k for k=10 and 20, following [10, 34].\\nTo ensure unbiased evaluation, we adopt the all-ranking protocol.\\nAll items that are not interacted by a user are the candidates.\\n5.1.3 Baseline Methods. To ensure a comprehensive assessment,\\nwe compare our method with 12 baseline methods, which can be divided into three categories: classical methods (BPR-MF, NFM, LightGCN), KG-enhanced methods (CKE, RippleNet, KGAT, KGIN, KGCL,\\nKGRec), and LLM-based methods (RLMRec, KAR, CLLM4Rec).\\nBPR-MF [23] employs matrix factorization to model users and\\nitems, and uses the pairwise Bayesian Personalized Ranking (BPR)\\nloss to optimize the model.\\nNFM [9] is an advanced factorization model that subsumes FM [24]\\nunder neural networks.\\nLightGCN [10] facilitates message propagation between users and\\nitems by simplifying GCN [16].\\nCKE [46] is an embedding-based method that uses TransR to guide\\nentity representation in KGs to enhance performance.\\nRippleNet [28] automatically discovers users’ hierarchical interests by iteratively propagating users’ preferences in the KG.\\nKGAT [34] designs an attentive message passing scheme over the\\nknowledge-aware collaborative graph for node embedding fusion.\\nKGIN [35] adopts an adaptive aggregation method to capture finegrained user intentions.\\nKGCL [44] uses contrastive learning for knowledge graphs to reduce potential noise and guide user preference learning.\\n\\n1 https://grouplens.org/datasets/movielens/\\n2 https://msnews.github.io/\\n3 https://grouplens.org/datasets/hetrec-2011/\\n\\n\\x0cComprehending Knowledge Graphs with Large Language Models for Recommender Systems\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nTable 2: Performance comparison of different methods, where R denotes Recall and N denotes NDCG. The best results are bolded,\\nand the second best results are underlined. The results show our improvement is statistically significant with a significance\\nlevel of 0.01.\\nMovieLens\\n\\nModel\\n\\nLast-FM\\n\\nMIND\\n\\nFunds\\n\\nR@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20\\nBPR-MF\\n0.1257 0.3100 0.2048 0.3062 0.1307 0.1352 0.1971 0.1685 0.0315 0.0238 0.0537 0.0310 0.4514 0.3402 0.5806 0.3809\\nNFM\\n0.1346 0.3558 0.2129 0.3379 0.2246 0.2327 0.3273 0.2830 0.0495 0.0356 0.0802 0.0458 0.4388 0.3187 0.5756 0.3651\\nLightGCN 0.1598 0.3901 0.2512 0.3769 0.2589 0.2799 0.3642 0.3321 0.0624 0.0492 0.0998 0.0609 0.4992 0.3778 0.6353 0.4204\\nCKE\\n0.1524 0.3783 0.2373 0.3609 0.2342 0.2545 0.3266 0.3001 0.0526 0.0417 0.0822 0.0510 0.4926 0.3702 0.6294 0.4130\\nRippleNet 0.1415 0.3669 0.2201 0.3423 0.2267 0.2341 0.3248 0.2861 0.0472 0.0364 0.0785 0.0451 0.4764 0.3591 0.6124 0.4003\\nKGAT\\n0.1536 0.3782 0.2451 0.3661 0.2470 0.2595 0.3433 0.3075 0.0594 0.0456 0.0955 0.0571 0.5037 0.3751 0.6418 0.4182\\nKGIN\\n0.1631 0.3959 0.2562 0.3831 0.2562 0.2742 0.3611 0.3215 0.0640 0.0518 0.1022 0.0639 0.5079 0.3857 0.6428 0.4259\\nKGCL\\n0.1554 0.3797 0.2465 0.3677 0.2599 0.2763 0.3652 0.3284 0.0671 0.0543 0.1059 0.0670 0.5071 0.3877 0.6355 0.4273\\nKGRec\\n0.1640 0.3968 0.2571 0.3842 0.2571 0.2748 0.3617 0.3251 0.0627 0.0506 0.1003 0.0625 0.5104 0.3913 0.6467 0.4304\\nRLMRec\\n0.1613 0.3920 0.2524 0.3787 0.2597 0.2812 0.3651 0.3335 0.0619 0.0486 0.0990 0.0602 0.4988 0.3784 0.6351 0.4210\\nKAR\\n0.1582 0.3869 0.2511 0.3722 0.2532 0.2770 0.3612 0.3324 0.0615 0.0480 0.1002 0.0613 0.5033 0.3812 0.6312 0.4175\\nCLLM4Rec 0.1563 0.3841 0.2433 0.3637 0.2571 0.2793 0.3642 0.3268 0.0631 0.0494 0.1012 0.0628 0.4996 0.3791 0.6273 0.4103\\nCoLaKG 0.1699 0.4130 0.2642 0.3974 0.2738 0.2948 0.3803 0.3471 0.0698 0.0562 0.1087 0.0684 0.5273 0.4012 0.6524 0.4392\\n\\nTable 3: Validation of the generalizability of our method:\\nExperimental results of integrating CoLaKG with various\\nrecommendation backbones.\\nModel\\n\\nMovieLens\\n\\nLast-FM\\n\\nMIND\\n\\nR@20 N@20 R@20 N@20 R@20 N@20\\nBPR-MF\\nBPR-MF+Ours\\n\\n0.2048 0.3062 0.1971 0.1685 0.0537 0.0310\\n0.2213 0.3255 0.2104 0.1812 0.0609 0.3986\\n\\nNFM\\nNFM+Ours\\n\\n0.2129 0.3379 0.3273 0.2830 0.0802 0.0458\\n0.2285 0.3527 0.3478 0.2996 0.0859 0.0487\\n\\nLightGCN\\n0.2512 0.3769 0.3642 0.3321 0.0998 0.0609\\nLightGCN+Ours 0.2642 0.3974 0.3803 0.3471 0.1087 0.0684\\n\\nKGRec [43] is a state-of-the-art KG-based recommendation model\\nwhich devises a self-supervised rationalization method to identify\\ninformative knowledge connections.\\nRLMRec [22] is an LLM-based model. It directly utilizes LLMs\\nto generate text profiles and combine them with recommendation\\nmodels through contrastive learning. Since their method is modelagnostic, to ensure a fair comparison, we chose LightGCN as its\\nbackbone model, consistent with our method.\\nKAR [41] is an LLM-based model, which utilizes LLMs to enhance\\nrecommender systems with open-world knowledge\\nCLLM4Rec [52] is also an LLM-based method. It combines ID\\ninformation and semantic information and uses the LLM directly\\nas the recommender to generate the recommendation results.\\n5.1.4 Implementation Details. We implement all baseline methods according to their released code. The embedding size 𝑑 for all\\nrecommendation methods is set to 64 for a fair comparison. All\\nexperiments are conducted with a single V100 GPU. We set the\\nbatch size to 1024 for the Last-FM dataset and 4096 for the other\\ndatasets to expedite training. The Dropout rate is chosen from the\\n\\nset {0.2, 0.4, 0.6, 0.8} for both the embedding layer and the hidden\\nlayers. We employ the Adam optimizer with a learning rate of 0.001.\\nThe maximum number of epochs is set to 2000. The number of\\nhidden layers for the recommendation model 𝐿 is set to 3. For the\\nLLM, we select DeepSeek-V2, a robust large language model that\\ndemonstrates exceptional performance on both standard benchmarks and open-ended generation evaluations. For more detailed\\ninformation about DeepSeek, please refer to their official website4 .\\nSpecifically, we utilize DeepSeek-V2 by invoking its API5 . To reduce\\ntext randomness of the LLM, we set the temperature 𝜏 to 0 and\\nthe top-𝑝 to 0.001. For the text embedding model P, we use the\\npre-trained sup-simcse-roberta-large6 [7]. We use identical settings\\nfor the baselines that also involve LLMs and text embeddings to\\nensure fairness in comparison.\\n\\n5.2\\n\\nComparison Results\\n\\nWe compare 12 baseline methods across four datasets and run each\\nexperiment five times. The average results are reported in Table 2.\\nBased on the results, we make the following observations:\\n• Our method consistently outperforms all the baseline models\\nacross all four datasets. The performance ceiling of traditional\\nmethods (BPR-MF, NFM, LightGCN) is generally lower than that\\nof KG-based methods, as the former rely solely on collaborative\\nsignals without incorporating semantic knowledge. However,\\nsome KG-based methods do not perform as well as LightGCN,\\nindicating that effectively leveraging KG is a challenging task.\\n• Among the KG-based baselines, KGCL and KGRec are notable for\\nincorporating self-supervised learning into general KG-based recommendation frameworks. However, they struggle with missing\\nfacts, understanding semantic information, and modeling higherorder item associations within the KG. In contrast, our method\\nleverages LLMs to address these challenges without requiring\\n4 https://github.com/deepseek-ai/DeepSeek-V2\\n5 https://api-docs.deepseek.com/\\n6 https://huggingface.co/princeton-nlp/sup-simcse-roberta-large\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nTable 4: Ablation study on all four datasets.\\nMetric w/o s𝑣 w/o s𝑢 w/o N𝑘 (𝑣) w/o D𝑣′ CoLaKG\\nML\\n\\nR@20 0.2553 0.2613\\nN@20 0.3811 0.3948\\n\\n0.2603\\n0.3902\\n\\n0.2628\\n0.3960\\n\\n0.2642\\n0.3974\\n\\nLast-FM\\n\\nR@20 0.3628 0.3785\\nN@20 0.3278 0.3465\\n\\n0.3725\\n0.3403\\n\\n0.3789\\n0.3459\\n\\n0.3803\\n0.3471\\n\\nMIND\\n\\nR@20 0.1043 0.1048\\nN@20 0.0640 0.0658\\n\\n0.1064\\n0.0662\\n\\n0.1076\\n0.0671\\n\\n0.1087\\n0.0684\\n\\nFunds\\n\\nR@20 0.6382 0.6481\\nN@20 0.4247 0.4351\\n\\n0.6455\\n0.4305\\n\\n0.6499\\n0.4378\\n\\n0.6524\\n0.4392\\n\\nself-supervised tasks, leading to significant improvements across\\nall datasets and metrics.\\n• For LLM-based recommendation methods, we have selected several representative approaches: RLMRec, KAR, and CLLM4Rec.\\nIt is evident that these methods exhibit only marginal improvements over traditional techniques. In contrast, our method demonstrates a substantial performance enhancement compared to\\nthese LLM-based baselines, thereby further validating the superiority of our approach. Our method effectively leverages LLMs to\\ncomprehend both the local subgraphs and global relationships\\nwithin knowledge graphs, resulting in significant performance\\nimprovements.\\n\\n5.3\\n\\nValidation of the Generalizability\\n\\nIn this section, we validate the versatility of CoLaKG. Specifically,\\nwe integrate our method into three different classical recommendation model backbones and observe the performance improvements\\nof these models across three public datasets. The results, as shown\\nin Table 3, clearly indicate significant performance improvements\\nwhen our method is combined with various recommendation backbones. This experiment demonstrates that our approach, which\\nleverages LLMs to understand KGs and enhance recommendation\\nmodels, can be flexibly applied to different recommendation models\\nto improve their performance.\\n\\n5.4\\n\\nAblation Study\\n\\nIn this section, we demonstrate the effectiveness of our model by\\ncomparing its performance with four different versions across all\\nfour datasets. The results are shown in Table 4, where “w/o s𝑣 ”\\ndenotes removing the semantic embeddings of items, “w/o s𝑢 ” denotes removing the semantic embeddings of users, “w/o N𝑘 (𝑣)”\\nmeans removing the neighbor augmentation of items based on the\\nconstructed item-item graph, and “w/o D𝑣′ ” means removing the\\nsecond-order triples from the LLM’s prompts. When the semantic embeddings of items are removed, the model’s performance\\nsignificantly decreases across all datasets, underscoring the critical role of semantic information captured by LLMs from the KG.\\nSimilarly, the removal of user semantic embeddings also results\\nin a performance decline, affirming that LLMs can effectively infer user preferences from the KG. Furthermore, removing N𝑘 (𝑣)\\nleads to a performance drop across all datasets, highlighting the\\nsignificance of the item representation augmentation module based\\non the constructed semantic-relational item-item graph. Without\\nthis module, the model can only capture local KG information from\\n\\nFigure 4: Hyperparameter study of the number of retrieved\\nneighbors (𝑘) and sampled number of 2-hop items within the\\nprompt (𝑚) on four datasets.\\nitem-centered subgraphs and cannot leverage the semantic relations\\npresent in the global KG. The inclusion of this module facilitates\\nthe effective integration of both local and global KG information.\\nLastly, removing second-order KG triples from the prompts causes\\na slight performance decline. This finding suggests that incorporating second-order information from the KG allows the LLMs to\\nproduce a higher-quality comprehension of the local KG.\\n\\n5.5\\n\\nHyperparameter Study\\n\\nIn this section, we investigate the impact of the hyperparameter\\n𝑘 and 𝑚 across four datasets. Here, 𝑘 represents the number of\\nsemantically related neighbors, as defined in Section 4.1.2, 𝑚 is\\nthe number of second-order neighbors used in the prompt, which\\nis defined in Section 4.1.1. The results are presented in Figure 4.\\nWe observe that as 𝑘 increases, both Recall@20 and NDCG@20\\ninitially rise and then slightly decline across all datasets. The performance is worst when 𝑘 = 0 and best when 𝑘 is between 10 and\\n30. When 𝑘 = 0, no neighbors are used, which is equivalent to the\\nablation study without N𝑘 (𝑣), thereby not incorporating any global\\nsemantic associations from the KG. When 𝑘 > 0, the introduction\\nof semantically related items enhances the item’s representations,\\nleading to a noticeable improvement. However, as 𝑘 continues to\\nincrease, some noise may be introduced because the relevance of\\nneighbors decreases with their ranking. Consequently, items with\\nlower relevance may interfere with the recommendation performance. Our findings suggest that a range of 10-30 neighbors is\\noptimal. As the value of 𝑚 increases, the metrics initially rise and\\nthen slightly decline. When 𝑚 = 0, the prompt used to understand\\nthe KG subgraph includes only first-order neighbors, resulting in\\nthe poorest performance. This indicates the positive impact of incorporating second-order neighbors. However, as 𝑚 continues to\\ngrow, the marginal benefits diminish, and additional noise may be\\nintroduced, leading to a slight decrease in performance.\\n\\n5.6\\n\\nRobustness to Varying Degrees of Sparsity\\n\\nOne of the key functions of KGs is to alleviate the issue of data\\nsparsity. To further examine the robustness of our model against\\nusers with varying levels of activity, particularly its performance\\nwith less active users, we sort users based on their interaction\\n\\n\\x0cComprehending Knowledge Graphs with Large Language Models for Recommender Systems\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\nDisconnected or Distant Paths\\nto “Apollo 13” in original KG\\n\\nCase 1\\n\\nConnected to “Apollo 13” by\\nshared genres in original KG\\n\\nApollo 13\\n\\nThe Right Stuff\\n\\nBirdy\\n\\nThe Great Escape\\n\\nTop Gun\\n\\nStar Trek: First Contact\\n\\nNeighbors of “Apollo 13”\\nConnected to “A Little Princess”\\nby shared genres in original KG\\n\\nDisconnected or Distant Paths to\\n“A Little Princess” in original KG\\n\\nA Simple Wish Quest for Camelot Now and Then\\n\\nThe Story of Cinderella The Princess Bride\\n\\nCase 2\\n\\nFigure 5: Performance comparison on different user groups,\\nwhere a smaller group ID indicates fewer interaction records.\\n\\nA Little Princess\\n\\nNeighbors of “A Little Princess”\\n\\nFigure 7: Case study.\\n\\nFigure 6: Performance comparison using complete KG and\\nincomplete KG on two datasets. Models with the \"-r\" suffix\\nindicate their performance in the presence of missing facts.\\nfrequency and divide them into four equal groups. A lower group\\nID indicates lower user activity (01 being the lowest, 04 the highest).\\nWe analyze the evaluation results on two relatively sparse datasets,\\nLast-FM and MIND, as shown in Figure 5. By comparing our model\\nwith three representative and strong baseline models, we observe\\nthat our model consistently outperforms the baselines in each user\\ngroup. Notably, the improvement ratio of our model in the sparser\\ngroups (01 and 02) is higher compared to the denser groups (03 and\\n04). For the group with the most limited data (Group 01), our model\\nachieves the most significant lead. This indicates that the average\\nimprovement of our model is primarily driven by enhancements in\\nthe sparser groups, demonstrating the positive impact of CoLaKG\\nin addressing data sparsity.\\n\\n5.7\\n\\nRobustness to Missing Facts\\n\\nTo further demonstrate the robustness of our proposed ColaKG in\\nscenarios where facing the challenge of missing facts in KG, we\\nconducted comparative experiments on the Movielens and Last-FM\\ndatasets. Specifically, we randomly dropped 30% of the KG entities and relations to construct datasets with significant missing\\nfacts based on the original datasets. The comparison results of\\nour method with representative KG-based baselines on the constructed datasets are shown in Figure 6. From the experimental\\nresults, we can see that removing KG facts has a negative impact on\\nall KG-based methods. However, in the case of an incomplete KG,\\nour method, CoLaKG, still outperforms the other baselines. Furthermore, by comparing the performance degradation of different\\nmethods on incomplete and complete KGs, we find that our method\\nexperiences the smallest proportion of performance decline. This\\ndemonstrates that our method can effectively mitigate the impact\\nof missing facts on performance, further proving the robustness\\nand superiority of the proposed approach.\\n\\n5.8\\n\\nCase Study\\n\\nIn this section, we conduct an in-depth analysis of the rationality\\nof our method through two real cases. In the first case, we present\\n\\nthe movie “Apollo 13” and its five semantically related neighbor\\nitems in the item-item graph identified by our method. The first\\nthree movies belong to the same genre as “Apollo 13”, making them\\n2-hop neighbors in the KG. In contrast, the other two movies, “Top\\nGun” and “Star Trek”, do not share any genre or other attributes\\nwith “Apollo 13”, indicating they are distant or unconnected in the\\nKG. However, “Top Gun” and “Star Trek” are semantically related to\\n“Apollo 13” as they all highlight themes of human resilience, courage,\\nand the spirit of adventure. Traditional KG-based recommendation\\nmethods, which rely on layer-by-layer information propagation,\\nstruggle to capture such high-order neighbors. In contrast, our\\nmethod leverages similarity calculations based on item-centered KG\\nsemantic embeddings, successfully identifying these two strongly\\nrelated movies. This demonstrates that our approach can effectively\\nand efficiently capture semantically relevant information from the\\nglobal KG. In the second case, we examine the movie “A Little\\nPrincess” and its related neighbors. Among the five related movies\\nidentified, “The Story of Cinderella” and “The Princess Bride” should\\nshare the same genre as “A Little Princess”. However, due to missing\\ngenres in the KG, these movies lack a path to “A Little Princess”\\nwithin the KG. Despite this, our method successfully identifies these\\ntwo movies. This demonstrates that our approach, by leveraging\\nLLMs to complete and interpret the KG, can effectively address\\nchallenges posed by missing key attributes.\\n\\n6\\n\\nConclusion\\n\\nIn this paper, we analyze the limitations of existing KG-based recommendation methods and propose a novel approach, CoLaKG,\\nto address these issues. CoLaKG comprehends item-centered KG\\nsubgraphs to obtain semantic embeddings for both items and users.\\nThese semantic embeddings are then used to construct a semantic\\nrelational item-item graph, effectively leveraging global KG information. We conducted extensive experiments on four datasets to\\nvalidate the effectiveness and robustness of our method. The results demonstrate that our approach significantly enhances the\\nperformance of recommendation models.\\n\\n7\\n\\nAcknowledgments\\n\\nThis work was supported by the Early Career Scheme (No. CityU\\n21219323) and the General Research Fund (No. CityU 11220324)\\nof the University Grants Committee (UGC), and the NSFC Young\\nScientists Fund (No. 9240127).\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nReferences\\n[1] Arkadeep Acharya, Brijraj Singh, and Naoyuki Onoe. 2023. Llm based generation\\nof item-description for recommendation system. In Proceedings of the 17th ACM\\nConference on Recommender Systems. 1204–1207.\\n[2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan\\nHe. 2023. Tallrec: An effective and efficient tuning framework to align large\\nlanguage model with recommendation. In Proceedings of the 17th ACM Conference\\non Recommender Systems. 1007–1014.\\n[3] Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019.\\nUnifying knowledge graph learning and recommendation: Towards a better\\nunderstanding of user preferences. In The world wide web conference. 151–161.\\n[4] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao\\nPu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, et al. 2023. When large language\\nmodels meet personalization: Perspectives of challenges and opportunities. arXiv\\npreprint arXiv:2307.16376 (2023).\\n[5] Djork-Arné Clevert. 2015. Fast and accurate deep network learning by exponential\\nlinear units (elus). arXiv preprint arXiv:1511.07289 (2015).\\n[6] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang\\nTang, and Qing Li. 2023. Recommender systems in the era of large language\\nmodels (llms). arXiv preprint arXiv:2307.02046 (2023).\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive\\nLearning of Sentence Embeddings. In Empirical Methods in Natural Language\\nProcessing (EMNLP).\\n[8] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong,\\nand Qing He. 2020. A survey on knowledge graph-based recommender systems.\\nIEEE Transactions on Knowledge and Data Engineering 34, 8 (2020), 3549–3568.\\n[9] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse\\npredictive analytics. In Proceedings of the 40th International ACM SIGIR conference\\non Research and Development in Information Retrieval. 355–364.\\n[10] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network for\\nrecommendation. In Proceedings of the 43rd International ACM SIGIR conference\\non research and development in Information Retrieval. 639–648.\\n[11] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng,\\nBodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large\\nlanguage models as zero-shot conversational recommenders. In Proceedings of the\\n32nd ACM international conference on information and knowledge management.\\n720–730.\\n[12] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,\\nand Wayne Xin Zhao. 2024. Large language models are zero-shot rankers for\\nrecommender systems. In European Conference on Information Retrieval. Springer,\\n364–381.\\n[13] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. 2018. Leveraging metapath based context for top-n recommendation with a neural co-attention model.\\nIn Proceedings of the 24th ACM SIGKDD international conference on knowledge\\ndiscovery & data mining. 1531–1540.\\n[14] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan,\\nAng Li, Zuoli Tang, and Jun Zhou. 2024. Enhancing sequential recommendation\\nvia llm-based semantic embedding learning. In Companion Proceedings of the\\nACM on Web Conference 2024. 103–111.\\n[15] Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and\\nChanyoung Park. 2024. Large Language Models meet Collaborative Filtering: An\\nEfficient All-round LLM-based Recommender System. In Proceedings of the 30th\\nACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1395–1406.\\n[16] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph\\nconvolutional networks. arXiv preprint arXiv:1609.02907 (2016).\\n[17] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023. Large language models\\nfor generative recommendation: A survey and visionary discussions. arXiv\\npreprint arXiv:2309.01157 (2023).\\n[18] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan,\\nRuiming Tang, Yong Yu, and Weinan Zhang. 2024. Rella: Retrieval-enhanced\\nlarge language models for lifelong sequential behavior comprehension in recommendation. In Proceedings of the ACM on Web Conference 2024. 3497–3508.\\n[19] Xiaolin Lin, Jinwei Luo, Junwei Pan, Weike Pan, Zhong Ming, Xun Liu, Shudong\\nHuang, and Jie Jiang. 2024. Multi-sequence attentive user representation learning\\nfor side-information integrated sequential recommendation. In Proceedings of the\\n17th ACM International Conference on Web Search and Data Mining. 414–423.\\n[20] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning\\nentity and relation embeddings for knowledge graph completion. In Proceedings\\nof the AAAI conference on artificial intelligence, Vol. 29.\\n[21] Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang.\\n2018. Deepinf: Social influence prediction with deep learning. In Proceedings of\\nthe 24th ACM SIGKDD international conference on knowledge discovery & data\\nmining. 2110–2119.\\n[22] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei\\nYin, and Chao Huang. 2024. Representation learning with large language models\\n\\nfor recommendation. In Proceedings of the ACM on Web Conference 2024. 3464–\\n3475.\\n[23] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\\n2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint\\narXiv:1205.2618 (2012).\\n[24] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme.\\n2011. Fast context-aware recommendations with factorization machines. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. 635–644.\\n[25] Yu Tian, Yuhao Yang, Xudong Ren, Pengfei Wang, Fangzhao Wu, Qian Wang, and\\nChenliang Li. 2021. Joint knowledge pruning and recurrent graph convolution\\nfor news recommendation. In Proceedings of the 44th international ACM SIGIR\\nconference on research and development in information retrieval. 51–60.\\n[26] Riku Togashi, Mayu Otani, and Shin’ichi Satoh. 2021. Alleviating cold-start\\nproblems in recommendation through pseudo-labelling over knowledge graph.\\nIn Proceedings of the 14th ACM international conference on web search and data\\nmining. 931–939.\\n[27] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\\narXiv:1710.10903 (2017).\\n[28] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie,\\nand Minyi Guo. 2018. Ripplenet: Propagating user preferences on the knowledge\\ngraph for recommender systems. In Proceedings of the 27th ACM international\\nconference on information and knowledge management. 417–426.\\n[29] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep\\nknowledge-aware network for news recommendation. In Proceedings of the 2018\\nworld wide web conference. 1835–1844.\\n[30] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge\\ngraph convolutional networks for recommender systems. In The world wide web\\nconference. 3307–3313.\\n[31] Lei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using\\nlarge pretrained language models. arXiv preprint arXiv:2304.03153 (2023).\\n[32] Maolin Wang, Dun Zeng, Zenglin Xu, Ruocheng Guo, and Xiangyu Zhao. 2023.\\nFederated knowledge graph completion via latent embedding sharing and tensor\\nfactorization. In 2023 IEEE International Conference on Data Mining (ICDM). IEEE,\\n1361–1366.\\n[33] Shuyao Wang, Yongduo Sui, Chao Wang, and Hui Xiong. 2024. Unleashing\\nthe Power of Knowledge Graph for Recommendation via Invariant Learning. In\\nProceedings of the ACM on Web Conference 2024. 3745–3755.\\n[34] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat:\\nKnowledge graph attention network for recommendation. In Proceedings of the\\n25th ACM SIGKDD international conference on knowledge discovery & data mining.\\n950–958.\\n[35] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu,\\nXiangnan He, and Tat-Seng Chua. 2021. Learning intents behind interactions\\nwith knowledge graph for recommendation. In Proceedings of the web conference\\n2021. 878–887.\\n[36] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng\\nChua. 2019. Explainable reasoning over knowledge graphs for recommendation.\\nIn Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5329–5336.\\n[37] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models\\nwith graph augmentation for recommendation. In Proceedings of the 17th ACM\\nInternational Conference on Web Search and Data Mining. 806–815.\\n[38] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. 2024.\\nExploring large language model for graph data understanding in online job\\nrecommendations. In Proceedings of the AAAI Conference on Artificial Intelligence,\\nVol. 38. 9178–9186.\\n[39] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,\\nChuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A survey on large\\nlanguage models for recommendation. arXiv preprint arXiv:2305.19860 (2023).\\n[40] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,\\nChuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2024. A survey on large\\nlanguage models for recommendation. World Wide Web 27, 5 (2024), 60.\\n[41] Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo\\nChen, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. Towards open-world\\nrecommendation with knowledge augmentation from large language models. In\\nProceedings of the 18th ACM Conference on Recommender Systems. 12–22.\\n[42] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin\\nDing, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In\\n2022 IEEE 38th international conference on data engineering (ICDE). IEEE, 1259–\\n1273.\\n[43] Yuhao Yang, Chao Huang, Lianghao Xia, and Chunzhen Huang. 2023. Knowledge\\ngraph self-supervised rationalization for recommendation. In Proceedings of the\\n29th ACM SIGKDD conference on knowledge discovery and data mining. 3046–3056.\\n[44] Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. 2022. Knowledge\\ngraph contrastive learning for recommendation. In Proceedings of the 45th international ACM SIGIR conference on research and development in information\\n\\n\\x0cComprehending Knowledge Graphs with Large Language Models for Recommender Systems\\n\\nretrieval. 1434–1443.\\n[45] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation:\\nA heterogeneous information network approach. In Proceedings of the 7th ACM\\ninternational conference on Web search and data mining. 283–292.\\n[46] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.\\n2016. Collaborative knowledge base embedding for recommender systems. In\\nProceedings of the 22nd ACM SIGKDD international conference on knowledge\\ndiscovery and data mining. 353–362.\\n[47] Haotian Zhang, Shuanghong Shen, Bihan Xu, Zhenya Huang, Jinze Wu, Jing\\nSha, and Shijin Wang. 2024. Item-Difficulty-Aware Learning Path Recommendation: From a Real Walking Perspective. In Proceedings of the 30th ACM SIGKDD\\nConference on Knowledge Discovery and Data Mining. 4167–4178.\\n[48] Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang, and Lihong Gu. 2024. Breaking\\nthe Barrier: Utilizing Large Language Models for Industrial Recommendation\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nSystems through an Inferential Knowledge Graph. arXiv preprint arXiv:2402.13750\\n(2024).\\n[49] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\\n[50] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen\\nWen, Fei Wang, Xiangyu Zhao, Jiliang Tang, et al. 2023. Recommender systems\\nin the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023).\\n[51] Xinjun Zhu, Yuntao Du, Yuren Mao, Lu Chen, Yujia Hu, and Yunjun Gao. 2023.\\nKnowledge-refined Denoising Network for Robust Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development\\nin Information Retrieval. 362–371.\\n[52] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2024. Collaborative large language model for recommender systems. In Proceedings of the\\nACM on Web Conference 2024. 3162–3172.\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "text_source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_-Hzx81cIF"
      },
      "outputs": [],
      "source": [
        "text_source = text_source.replace(\"Comprehending Knowledge Graphs with Large Language Models for Recommender Systems\", \"\").replace(\" SIGIR ’25, July 13–18, 2025, Padua, Italy\", \"\").replace(\" Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_source"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "_Cg4_ahoMI2r",
        "outputId": "8e40e19a-8785-4361-f89c-26d352a43950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nAbstract\\nIn recent years, the introduction of knowledge graphs (KGs) has\\nsignificantly advanced recommender systems by facilitating the discovery of potential associations between items. However, existing\\nmethods still face several limitations. First, most KGs suffer from\\nmissing facts or limited scopes. Second, existing methods convert\\ntextual information in KGs into IDs, resulting in the loss of natural\\nsemantic connections between different items. Third, existing methods struggle to capture high-order connections in the global KG.\\nTo address these limitations, we propose a novel method called CoLaKG, which leverages large language models (LLMs) to improve\\nKG-based recommendations. The extensive knowledge and remarkable reasoning capabilities of LLMs enable our method to supplement missing facts in KGs, and their powerful text understanding\\nabilities allow for better utilization of semantic information. Specifically, CoLaKG extracts useful information from KGs at both local\\nand global levels. By employing the item-centered subgraph extraction and prompt engineering, it can accurately understand the\\nlocal information. In addition, through the semantic-based retrieval\\nmodule, each item is enriched by related items from the entire\\nknowledge graph, effectively harnessing global information. Furthermore, the local and global information are effectively integrated\\ninto the recommendation model through a representation fusion\\nmodule and a retrieval-augmented representation learning module, respectively. Extensive experiments on four real-world datasets\\n∗ Work done during an internship at Tencent.\\n† Corresponding author.\\n\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSIGIR ’25, Padua, Italy\\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-1592-1/2025/07\\nhttps://doi.org/10.1145/3726302.3729932\\n\\ndemonstrate the superiority of our method. The code of our method\\nis available at https://github.com/ziqiangcui/CoLaKG.\\n\\nCCS Concepts\\n• Information systems → Recommender systems.\\n\\nKeywords\\nLarge Language Models, Knowledge Graphs, Recommendation\\n\\n1\\n\\nIntroduction\\n\\nThe rapid advancement of online platforms has led to an increasingly critical issue of information overload. Recommender systems\\naddress this problem by modeling user preferences based on historical data. Collaborative filtering (CF) [9, 10, 23], as one of the\\nmost classic and efficient methods, has been extensively employed\\nin existing recommender systems. However, CF-based methods exclusively rely on user-item interaction records, often suffering from\\nthe data sparsity issue [26]. To address this issue, recent studies\\n[34, 44, 46] have incorporated knowledge graphs (KGs) as external knowledge sources into recommendation models, achieving\\nsignificant progress. Typically, these methods capture diverse and\\nhigh-order relationships between items by modeling the structure\\nand attribute information in KGs, thereby enhancing the learning\\nprocess of user and item representations [44].\\nDespite the effectiveness of existing KG-enhanced recommendation methods, they still face several challenges. i) First, many KGs\\nsuffer from missing facts and limited scopes [8, 32], as constructing\\nKGs often requires manual effort and domain expertise. The absence\\nof key attributes, such as the genres of a movie, can cause items\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\nItem node\\n\\nAttribute node\\n\\nx-hop\\n\\n𝑟!\\n\\na. Missing facts:\\n\\n2-hop\\n\\nH\\n\\nF\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nA\\n\\n𝑟%\\n\\nD\\nE\\n\\n𝑟!\\n\\n𝑟$\\n\\n1-hop\\n\\n𝑟!\\n𝑟\"\\n\\nA\\n\\n𝑟$\\n\\n𝑟#\\n\\nP\\n\\nG\\n\\n𝑟$\\n\\nC\\n\\n𝑟!\\n\\nB\\n\\n𝑟!\\n\\nC\\n\\nb. Textual Entities (e.g., Title/Tag/Genre..)\\nSimilar semantic\\n\\nB\\n\\nD\\n\\nHorror\\nID: 51\\n\\n𝑟!\\n\\nThriller\\nID: 320\\n\\nE\\n\\nDifferent IDs\\nc. Distant neighbors\\nTarget item\\nB\\n•\\n•\\n\\nC\\n\\nG\\n\\nH\\n\\nA\\n\\n2-hop\\n\\nA\\n\\nx-hop\\n\\nH\\n\\nB, C, G have no connections with A in the KG\\nH and A are too distant in the KG\\n\\nFigure 1: An illustrative diagram demonstrating the potential\\nissues of existing KG-based recommendation methods.\\nthat originally share the same attribute to lose their connections. As\\nillustrated in Figure 1, items A and B should have a connecting path\\n(A-P-B) in the KG. However, due to item A missing the P attribute,\\nA and B are now not associated with each other. Such biased knowledge will lead to suboptimal performance of the recommendation\\nmodel. ii) Second, existing methods [33–35, 43, 44] convert textual\\nentities and relations into IDs for use, which can result in losing\\nnatural semantic connections between items. For instance, in Figure\\n1, “horror” and “thriller” are two semantically related attributes of\\nItem F and Item G, respectively. However, similar semantics are\\nnot reflected in different entity IDs (51 and 320), which further results in the related items F and G having no connections in the KG.\\niii) Third, existing methods [25, 33–35, 43, 44] struggle to capture\\nhigh-order relationships in the entire KG. Most of them propagate\\nand aggregate information by stacking multiple layers of graph\\nneural networks (GNNs). The layer-by-layer propagation is not\\nonly inefficient but also accumulates a large amount of irrelevant\\nnode information, leading to the over-smoothing issue [8, 33]. For\\ninstance, assumming that points A and H in Figure 1 have a strong\\nsemantic connection, the considerable distance between them in\\nthe KG presents big challenges for existing methods in capturing\\nthis relation. As shown in the lower left corner of Figure 1, due\\nto the aforementioned three limitations, existing methods fail to\\nidentify the potential connections between historically interacted\\nitems and the target item through the KG, resulting in suboptimal\\nrecommendations.\\nEmpowered by extensive knowledge and remarkable reasoning\\nabilities, large language models (LLMs) have demonstrated significant promise in semantic understanding and knowledge extraction.\\nRecently, many studies have leveraged LLMs to improve recommendation models. For example, some studies utilize LLMs to generate\\nsemantic representations of item profiles [22], while other studies\\n[48] employ LLMs to determine whether a complementary relationship exists between two items, thereby recommending complementary products based on users’ historical behaviors. However, these\\nmethods do not fully exploit the semantic and structural information of KGs. As one of the most common and important sources of\\nknowledge, KGs contain a wealth of semantic associations among\\nentities and relations, which are often overlooked by existing methods that typically consider only item profiles. Additionally, KGs\\nserve as task-relevant knowledge repositories, effectively aiding\\n\\nLLMs in acquiring task-specific knowledge and mitigating the issue of hallucinations caused by excessive divergence. Nevertheless,\\nleveraging LLMs to handle the diverse semantic relationships in\\nKGs is highly challenging, as it is unrealistic to input the vast number of entities and connections in KGs into a language model.\\nTo bridge this gap, in this paper, we propose a novel method\\nnamed Comprehending Knowledge Graphs with Large Language\\nModels for Recommendation (CoLaKG). The core idea is to leverage\\nLLMs for understanding the semantic and structural information of\\nKGs to enhance the representation learning of items and users. Our\\nmethod comprises two stages: i) Comprehending KGs with LLMs.\\nGiven the impracticality of inputting the entire KG into an LLM,\\nCoLaKG leverages KG information by two components: a Local KG\\nComprehension module which involves extracting item-centered\\nKG subgraphs for analysis by LLMs, and a Retrieval-based Global\\nKG Utilization module which retrieves semantically related items\\nfrom the entire KG to effectively leverage global KG information. ii)\\nIncorporating semantic embeddings into the recommendation model.\\nThis stage integrates the semantic embeddings of KGs with the\\nID embeddings, thereby leveraging both collaborative signals and\\nsemantic information. First, we align and fuse the item ID representations with the item-centered KG subgraph representations. Then,\\nwe employ a retrieval-augmented representation method to further\\nenhance item representations with semantically similar neighbors\\nfrom the entire KG.\\nIt is important to note that these two stages are decoupled, meaning that our method does not involve LLM inference during the\\nrecommendation process. This allows our method to be efficiently\\napplied in real-world recommendation scenarios.\\nOur contributions are summarized as follows:\\n• We propose a novel method that utilizes LLMs to comprehend and\\ntransform the semantic and structural information of KGs. This\\napproach addresses the issues of missing facts and the inability\\nto leverage semantic information from text in current KG-based\\nrecommendation methods.\\n• We propose leveraging both local KG information and global\\nKG information with LLMs. In addition, we design a retrievalaugmented method to enhance item representation learning.\\n• Extensive experiments are conducted on four real-world datasets\\nto validate the superiority of our method. Further analysis demonstrates the rationale behind our approach.\\n\\n2 Related Work\\n2.1 Knowledge-aware Recommendation\\nExisting knowledge-aware recommendation methods can be categorized into three types [8]: embedding-based methods, path-based\\nmethods, and GNN-based methods. Embedding-based methods\\n[3, 19, 29, 46] enhance the representations of users and items by\\nleveraging the relations and entities within the KGs. Notable examples include CKE [46], which integrates various types of side\\ninformation into a collaborative filtering framework using TransR\\n[20]. Another example is DKN [29], which improves news representations by combining textual embeddings of sentences and\\nknowledge-level embeddings of entities. Path-based methods leverage KGs to explore long-range connectivity [13, 36, 45, 47]. For\\nexample, Personalized Entity Recommendation (PER) [45] treats a\\n\\n\\x0c\\nItem-centered KG Subgraphs\\n\\n𝑟$\\n\\n…\\n\\nTh\\neR\\nea\\n\\nde\\nr\\n\\nKa\\n\\nte\\n\\n𝑟&\\n\\n…\\n\\n…\\n\\nn\\nio\\npt\\nce\\nIn\\n\\n…\\n|𝒰|\\n\\nPrompt Engineering\\n\\n𝑣\"\\n𝑟($! ,$\")\\n\\n𝑟\\n𝑁% (𝑣& )\\n\\nAdapter\\n\\n❄\\n𝑠\\'\\n\\n𝑠\"!\\n\\n𝑟* Acted by\\n|𝒱|\\n\\n𝑟) Directed\\n𝑟( Is the genre of\\n\\nUser\\n\\n𝑠\\'\\n\\nItem\\n\\n𝒫(#)\\nText\\nEmbedding\\nModel\\n\\n𝑒\"\\n\\n𝑠\"\\n\\n𝑣!\\n\\nUser Preference\\nComprehension\\n\\nRetrieval-Augmented Representation Learning\\n\\n❄\\n\\n𝑣!\\n\\nLLM\\n\\n…\\n\\nRetrieval-based Global KG Utilization\\n𝑟(\"! ,\"\" )\\n\\n𝑟+ Genre\\n\\nText\\nEmbedding\\nModel\\n\\nAssume you are a film expert …\\n\\n𝑟# 𝑟!\\n\\nRelations\\n𝑟, Directed by\\n\\n𝑠\"\\n\\n𝒫(#)\\n\\nKG-based user interactions:\\n{“Titanic”:\\n(Titanic, Directed by, James Cameron); (Titanic,\\nActed by, Leonardo | Kate); (Titanic, Genres,\\nRomance | Disaster);… }\\n{“Inception”:\\n(Inception, Directed by, Nolan); (Inception,\\nActed by, Leonardo DiCaprio | Ellen Page);\\n(Inception, Genres, Science Fiction | Action);… }\\n\\n…\\n𝑟\"\\n\\nLocal KG\\nComprehension\\n\\nℎ\"\\n\\n𝑠\"(\\n𝑠\\'(\\n\\nEntities other\\nthan items\\nAddition\\n\\n|𝒰|\\n\\nInner product\\n\\n❄ Freeze\\n\\nU-I Modeling ℱ(-)\\nℎ)\"\\n\\nℎ\"(\\n…\\n\\n…\\n\\n…\\n\\n𝑟# 𝑟!\\n\\nLLM\\n\\n𝑟\\' Starred in\\n\\n|𝒱|\\n\\nUser-centered Subgraphs\\n\\n𝑟! 𝑟\"\\n\\nPrompt Engineering\\nAssume you are a film expert …\\nAssume you are a film expert …\\nOne-hop knowledge in the format of triples :\\n(Titanic, Directed by, James Cameron);\\n(Titanic, Acted by, Leonardo DiCaprio …);\\n(Titanic, Genres, Romance); (Titanic, plot, …);\\n…\\nTwo-hop knowledge:\\nJames Cameron also directed… ;\\nLeonardo DiCaprio also starred in… ;\\nMovies in the similar genres also include … ;\\nMovies with the similar plot also include … ;\\n...\\n\\nℎ\\'\\n\\n𝛼!\"\\n\\n𝑒\\'\\n\\nℎ\\'\\n\\nℎ)\\'\\n\\n𝑦+\\'\"\\n\\n…\\n\\nt\\nAlien\\nlie\\ns\\nJu\\nAv\\n𝑟$ …\\n&\\nala\\neo James n\\nnc\\nm\\no\\ner\\nhe\\nRo\\nm\\na\\n𝑟&\\nC\\n𝑟&\\n𝑟&\\n𝑟! Disaster\\ne\\n𝑟\"\\nc\\n𝑟&\\nan 𝑟\"\\nm\\n𝑟#\\n𝑟#\\nRo\\nTitanic\\nLeo\\n𝑟%\\n𝑟%\\nnard\\no\\n𝑟%\\n𝑟%\\n\\nUser-Item-Entity\\nGraph\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\n𝑠\"\"\\n\\nFigure 2: The framework of our proposed CoLaKG.\\nKnowledge Graph (KG) as a heterogeneous information network\\nand extracts meta-path-based latent features to represent the connectivity between users and items along various types of relational\\npaths. MCRec [13] constructs meta-paths and learns the explicit\\nrepresentations of meta-paths to depict the interaction context\\nof user-item pairs. Despite their effectiveness, these approaches\\nheavily rely on domain knowledge and human effort for metapath design. Recently, GNN-based methods have been proposed,\\nwhich enhance entity and relation representations by aggregating\\nembeddings from multi-hop neighbors [30, 34, 35]. For instance,\\nKGAT [34] employs the graph attention mechanism to propagate\\nembeddings and utilizes multi-layer perceptrons to generate final\\nrecommendation scores in an end-to-end manner. Similarly, KGIN\\n[35] adopts an adaptive aggregation method to capture fine-grained\\nuser intentions. Additionally, some methods [33, 43, 44, 51] employ\\ncontrastive learning to mitigate potential knowledge noise and\\nidentify informative knowledge connections.\\n\\n2.2\\n\\nLLMs for Recommendation\\n\\nIn light of the emergence of large language models and their remarkable achievements in the field of NLP, scholars have begun\\nto explore the potential application of LLMs in recommender systems [4, 6, 39, 49]. Due to the powerful reasoning capabilities and\\nextensive world knowledge of LLMs, they have been already naturally applied to zero-shot [11, 12, 31] and few-shot recommendation\\nscenarios [2, 18]. In these studies, LLMs are directly used as a recommendation model [17, 50], where the output of LLMs is expected to\\noffer a reasonable recommendation result [40]. However, when the\\ndataset is sufficiently large, their performance often falls short of\\nthat achieved by traditional recommendation models. Another line\\nof research involves leveraging LLMs as feature extractors. These\\nmethods [1, 14, 15, 22, 22, 37, 38, 52] generate intermediate decision\\n\\nresults or semantic embeddings of users and items, which are then\\ninput into traditional recommendation models to produce the final\\nrecommendations. Unlike existing methods, our approach aims to\\nleverage the extensive knowledge and reasoning capabilities of\\nLLMs to understand KGs and transform them into semantic embeddings, thereby addressing existing issues in KG-based recommender\\nsystems.\\n\\n3\\n\\nPreliminaries\\n\\nUser-Item Interaction Graph. Let U and V denote the user set\\nand item set, respectively, in a recommender system. We construct\\na user-item bipartite graph G = {(𝑢, 𝑦𝑢𝑣 , 𝑣)|𝑢 ∈ U, 𝑣 ∈ V} to\\nrepresent the collaborative signals between users and items. Here,\\n𝑦𝑢𝑣 = 1 if user 𝑢 interacted with item 𝑣, and vice versa.\\nKnowledge Graph. We capture real-world knowledge about items\\nusing a heterogeneous graph composed of triplets, represented as\\nG𝑘 = {(ℎ, 𝑟, 𝑡)}. In this context, ℎ and 𝑡 are knowledge entities\\nbelonging to the set E, while 𝑟 is a relation from the set R that\\nlinks them, as exemplified by the triplet (James Cameron, directed,\\nTitanic). Notably, the item set is a subset of the entity set, denoted\\nas V ⊂ E. This form of knowledge graph enables us to model the\\nintricate relationships between items and entities.\\nTask Formulation. Following the task format of most KG-aware\\nrecommendation models, we formulate the task as follows: Given\\nthe user-item interaction graph G and the corresponding knowledge graph G𝑘 , our objective is to learn a recommendation model\\nthat predicts the probability of user 𝑢 interacting with item 𝑣.\\n\\n4\\n\\nMethodology\\n\\nIn this section, we introduce our proposed method CoLaKG in\\ndetail. An overview of our method is illustrated in Figure 2. CoLaKG extracts useful information from the KG at both local and\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nAssume you are an expert in movie recommendation. You will be given a certain movie with its\\nfirst-order information (in the form of triples) and some second-order relationships (movies related\\nto this movie). Please complete the missing knowledge, summarize the movie and analyze what\\nkind of users would like it.\\nFirst-order info: (Titanic, Directed by, James Cameron), (Titanic, Acted by, Leonardo DiCaprio), …\\nSecond-order info: The movies with the same director (James Cameron) also include: Aliens, …\\nThe movies with the same actor (Leonardo DiCaprio) also include: Inception, …\\n\\n…\\n\\nFigure 3: The prompt template for Local KG comprehension.\\nglobal levels. By employing item-centered subgraph extraction and\\nprompt engineering, it accurately captures the local KG. Subsequently, through retrieval-based neighbor enhancement, it supplements the current item by capturing related items from the entire\\nKG, thereby effectively utilizing global information. The local and\\nglobal information extracted by the LLM are effectively integrated\\ninto the recommendation model through a representation fusion\\nmodule and a retrieval-augmented representation learning module,\\nrespectively, thereby improving recommendation performance.\\n\\n4.1\\n\\nKG Comprehension with LLMs\\n\\nKnowledge graphs have been widely utilized in recommender systems to provide semantic information and model latent associations\\nbetween items. However, KGs are predominantly manually curated,\\nleading to missing facts and limited knowledge scopes. Additionally,\\nthe highly structured nature of KGs poses challenges for utilizing\\ntextual information. To address these limitations, we propose the\\nuse of LLMs to enhance the understanding and refinement of KGs\\nfor improved recommendations. To fully leverage KGs, our approach comprises two components: 1) Local KG Comprehension,\\nwhich involves extracting item-centered KG subgraphs for analysis by LLMs, and 2) Retrieval-based Global KG Utilization, which\\nutilizes the semantic similarity of KG subgraphs to retrieve semantically related items from the entire KG, thereby leveraging global\\nKG information.\\n4.1.1 Local KG Comprehension. In this part, we introduce using LLMs to comprehend local KG information. Here, the local\\nKG information of an item refers to the KG subgraph centered\\non that item, including connections within two hops. First, we\\nrepresent the first-order KG subgraph centered on each item (i.e.,\\nego network [21]) using triples. Specifically, given an item 𝑣 ∈ V,\\nwe use T𝑣 = {(𝑣, 𝑟, 𝑒)|(𝑣, 𝑟, 𝑒) ∈ G𝑘 } to denote the set of triplets\\nwhere 𝑣 is the head entity. In the context of recommendation, the\\nfirst-order neighboring entities of an item in a KG are usually attributes. Therefore, we use 𝑒 to represent these attribute entities\\nto distinguish them from those item entities 𝑣. During generating triples, in cases where the attribute or relation is absent, the\\nterm “missing” is employed as a placeholder. Next, we consider the\\nsecond-order relations in KGs. The number of entities in an ego\\nnetwork centered on a single entity increases exponentially with\\nthe growth of the radius. However, the input length of an LLM is\\nstrictly limited. Consequently, including all second-order neighbors\\nassociated with the central item in the prompt becomes impractical. To address this issue, we adopt a simple but effective strategy,\\nrandom sampling, to explore second-order connections of 𝑣. Let\\nE𝑣 = {𝑒 | (𝑣, 𝑟, 𝑒) ∈ T𝑣 } denote the set of first-order connected\\nneighbors of 𝑣. For each 𝑒 ∈ E𝑣 , we randomly sample 𝑚 triples\\nfrom the set T𝑒 to construct the triples of second-order connections, denoted as T𝑒𝑚 . Here, T𝑒 = {(𝑒, 𝑟, 𝑣 ′ ) | (𝑒, 𝑟, 𝑣 ′ ) ∈ G𝑘 , 𝑣 ′ ≠ 𝑣 }\\nrepresents the set of triples where 𝑒 ∈ E𝑣 is the head entity.\\n\\nAfter converting first-order and second-order relationships into\\ntriples, we transform these triples into textual form. For first-order\\nrelations, we concatenate all the first-order triples in T𝑣 to form\\na single text, denoted as D𝑣 . For second-order relations, we use a\\ntemplate to transform the second-order triples T𝑒𝑚 into coherent\\nsentences D𝑣′ , facilitating the understanding of the LLM. In addition\\nto D𝑣 and D𝑣′ , we have carefully designed a system prompt I𝑣 as\\nthe instruction to guide the generation. By combining I𝑣 , D𝑣 , and\\nD𝑣′ , we obtain the prompt, which is shown in Figure 3. The prompt\\nenables the LLM to fully understand, complete, and refine the KG,\\nthereby generating the final comprehension for the 𝑣-centered KG\\nsubgraph. This process can be formulated as follows:\\nC𝑣 = LLMs(I𝑣 , D𝑣 , D𝑣′ ).\\n\\n(1)\\n\\nOnce we have obtained the LLM’s comprehension of the KG subgraphs, we need to convert these textual answers into continuous vectors for utilization in downstream recommendation models.\\nHere, we employ a pre-trained text embedding model P to transform C𝑣 into embedding vectors s𝑣 , which can be formulated as:\\ns𝑣 = P (C𝑣 ).\\n\\n(2)\\n\\n4.1.2 Rerieval-based Global KG Utilization. This section introduces the utilization of global KG information. In recommendation\\nscenarios, items that are distant in the KG can still have close semantic associations. However, the number of KG nodes increases\\nexponentially with the number of hops, and the input length of\\nLLMs is limited by the number of tokens. This makes it impractical to input the entire KG into an LLM directly. To address this\\nchallenge, we propose a method called retrieval-based global KG\\nutilization.\\nFor each item, we have obtained the semantic embedding corresponding to its local KG. Based on this, we can directly compute\\nthe semantic relationships between any two item-centered KG subgraphs. specifically, we employ the cosine similarity as a metric to\\nquantify the relations. Given two different items 𝑣𝑖 and 𝑣 𝑗 , their\\nsemantical relation 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) in the global KG is computed as:\\n𝑟 (𝑣𝑖 ,𝑣 𝑗 ) = sim(s𝑣𝑖 , s𝑣 𝑗 ), 𝑣𝑖 , 𝑣 𝑗 ∈ V and 𝑣𝑖 ≠ 𝑣 𝑗 ,\\n\\n(3)\\n\\nwhere sim denotes the cosine similarity function. Once we obtain\\nthe semantic associations between any two items in the entire KG,\\nwe treat the semantic similarity between the two items as the edge\\nweight between them, allowing us to construct an item-item graph:\\nG𝑣 = {(𝑣𝑖 , 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) , 𝑣 𝑗 )|𝑣𝑖 , 𝑣 𝑗 ∈ V, 𝑣𝑖 ≠ 𝑣 𝑗 }.\\n\\n(4)\\n\\nFrom a high-level perspective, we transform high-order associations\\nbetween items in the KG into direct semantic connections on the\\nconstructed item-item graph G𝑣 . Based on this foundation, it is\\nessential to retrieve items that are semantically strongly related\\nto the given item, as items with lower semantic relevance may\\nintroduce noise. Specifically, given an item 𝑣𝑖 , we rank all other\\nitems 𝑣 𝑗 ∈ V where 𝑣 𝑗 ≠ 𝑣𝑖 in descending order based on the\\nsemantic similarity 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) . Subsequently, we retrieve the top-𝑘\\nitems with the highest similarity scores, forming the neighbor set\\nof 𝑣𝑖 : N𝑘 (𝑣𝑖 ), where 0 < 𝑘 < |V | is an adjustable hyperparameter,\\nrepresenting the number of retrieved neighbors.\\nIn this manner, we filter out items with low semantic associations to the current item in the entire KG. Traditional KG-based\\n\\n\\x0c\\n\\nrecommendation methods aggregate items through layer-by-layer\\ninformation propagation on the KG. High-order relations require\\nmany propagation and aggregation steps to be captured. In contrast,\\nour retrieval-based method can directly recall strongly semantically related neighbors of any order across the entire KG. The\\nretrieved neighbors are then leveraged to enhance item representations, which will be introduced in Section 4.3.2.\\n\\n4.2\\n\\nUser Preference Comprehension\\n\\nThe introduction of KGs allows for the expansion of user-item\\nbipartite graphs and enables us to understand user preferences from\\na knowledge-driven perspective. Given a user 𝑢, we first extract\\nthe subgraph corresponding to user 𝑢 from the user-item bipartite\\ngraph, denoted as B𝑢 . For each item 𝑣 ∈ B𝑢 , we extract its firstorder KG subgraph and represent it as a set of triples, denoted\\nas T𝑣 . We then concatenate all triples in T𝑣 to form a single text,\\ndenoted as D𝑣 . The detailed approach is the same as described in\\nSection 4.1.1. Subsequently, we represent user 𝑢 with all items the\\nuser has interacted with in the training set and the corresponding\\nknowledge triples D𝑣 :\\nD𝑢 = ⊕𝑣 ∈ G𝑢 {name𝑣 : D𝑣 },\\n\\n(5)\\n\\nwhere ⊕ denotes concatenation operation, and name𝑣 denotes the\\ntext name of item 𝑣. Additionally, we have meticulously designed a\\nsystem prompt, denoted as I𝑢 , to serve as an instruction for guiding\\nthe generation of user preferences. By combining D𝑢 and I𝑢 , we\\nenable the LLM to comprehend the user preference for 𝑢, which\\ncan be formulated as:\\nC𝑢 = LLMs(I𝑢 , D𝑢 ).\\n\\n(6)\\n\\nFurthermore, we also utilize the text embedding function P to\\ntransform the textual answers C𝑢 into embedding vectors s𝑢 , which\\ncan be expressed as:\\ns𝑢 = P (C𝑢 ).\\n(7)\\n\\n4.3\\n\\nRetrieval-Augmented Representation\\n\\n4.3.1 Cross-Modal Representation Alignment. In a traditional\\nrecommendation model, each item and user is associated with an\\nID embedding. Let e𝑣 ∈ R𝑑 represent the ID embedding of item\\n𝑣 and e𝑢 ∈ R𝑑 represent the ID embedding of user 𝑢. In addition,\\nwe also obtain the semantic embedding s𝑣 ∈ R𝑑𝑠 w.r.t. the comprehension of 𝑣-centric KG subgraph, and the semantic embedding\\ns𝑢 ∈ R𝑑𝑠 w.r.t. the comprehension of user 𝑢’s preference. Since\\nID embeddings and semantic embeddings belong to two different\\nmodalities and typically possess different embedding dimensions,\\nwe employ a learnable adapter network to align the embedding\\nspaces. Specifically, the adapter consists of a linear map and a nonlinear activation function, formulated as:\\ns′𝑣 = 𝜎 (W1 s𝑣 );\\n\\ns𝑢′ = 𝜎 (W2 s𝑢 ),\\n\\n(8)\\n\\nwhere both W1 ∈ R𝑑 ×𝑑𝑠 and W2 ∈ R𝑑 ×𝑑𝑠 are are weight matrices, 𝜎 represents the non-linear activation function ELU [5]. Note\\nthat during the training process, we fix s𝑣 and s𝑢 , training solely\\nthe projection parameters W1 and W2 , and the parameters of the\\nrecommendation model. After mapping the representations to the\\nsame space, we need to fuse the representations of the two modalities, leveraging both the collaborative signals and the semantic\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\ninformation to form a complementary representation. To achieve\\nthis, we employ a straightforward mean pooling technique to fuse\\ntheir embeddings:\\nh𝑣 =\\n\\n1\\n(e𝑣 + s′𝑣 );\\n2\\n\\nh𝑢 =\\n\\n1\\n(e𝑢 + s𝑢′ ),\\n2\\n\\n(9)\\n\\nwhere h𝑣 ∈ R𝑑 and h𝑢 ∈ R𝑑 represent the merged embeddings of\\nitem 𝑣 and user 𝑢, respectively.\\n4.3.2 Item Representation Augmentation with Retrieved\\nNeighbors. For each item, we have retrieved its semantic-related\\nitems in Section 4.1.2. To fully utilize these neighbors, we propose\\nto aggregate their information to enhance item representations.\\nConsidering the varying contributions of different neighbors to the\\ncentral item, we employ the attention mechanism. Specifically, for\\nitem 𝑣𝑖 and its top-𝑘 neighbor set N𝑘 (𝑣𝑖 ), we compute attention\\ncoefficients that indicate the importance of item 𝑣 𝑗 ∈ N𝑘 (𝑣𝑖 ) to\\nitem 𝑣𝑖 as follows:\\n𝑤𝑖 𝑗 = 𝑎(Ws𝑣𝑖 ∥Ws𝑣 𝑗 ).\\n\\n(10)\\n\\nHere, W ∈ R𝑑𝑎 ×𝑑 is a learnable weight matrix to capture higherlevel features of s𝑣𝑖 and s𝑣 𝑗 , ∥ is the concatenation operation, 𝑎\\ndenotes the attention function: R𝑑𝑎 × R𝑑𝑎 → R, where we adopt a\\nsingle-layer neural network and apply the LeakyReLU activation\\nfunction following [27]. Note that the computation of attention\\nweights is exclusively dependent on the semantic representation\\nof items, as our objective is to calculate the semantic associations\\nbetween items, rather than the associations present in collaborative signals. In addition, we employ the softmax function for easy\\ncomparison of coefficients across different items:\\n𝛼𝑖 𝑗 = softmax 𝑗 (𝑤𝑖 𝑗 ).\\n\\n(11)\\n\\nThe attention scores 𝛼𝑖 𝑗 are then utilized to compute a linear combination of the corresponding neighbor embeddings. Finally, the\\nweighted average of neighbor embeddings and the embedding of\\nitem 𝑣𝑖 itself are combined to form the final output representation\\nfor item 𝑣𝑖 :\\n\\x12 \\x10\\n\\x11\\x13\\n∑︁\\n1\\nh𝑣𝑖 +\\nh′𝑣𝑖 = 𝜎\\n𝛼 𝑖 𝑗 h𝑣 𝑗 ,\\n(12)\\n𝑗 ∈ N𝑘 (𝑣𝑖 )\\n2\\nwhere 𝜎 denotes the non-linear activation function.\\n\\n4.4\\n\\nUser-Item Modeling\\n\\nHaving successfully integrated the semantic information from the\\nKG into both user and item representations, we can use them as inputs for traditional recommendation models to generate prediction\\nresults. This process can be formulated as follows:\\n𝑦ˆ𝑢𝑣 = F (h𝑢 , h′𝑣 ),\\n\\n(13)\\n\\nwhere 𝑦ˆ𝑢𝑣 is the predicted probability of user 𝑢 interacting with\\nitem 𝑣, h𝑢 is the representation for user 𝑢, h′𝑣 is the augmented\\nrepresentation for item 𝑣, and F denotes the function of the recommendation model.\\nSpecifically, we select the classic model, LightGCN [10], as the\\narchitecture for our recommendation method due to its simplicity\\nand effectiveness. The trainable parameters of original LightGCN\\nare only the embeddings of users and items, similar to standard\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nTable 1: Dataset statistics.\\nStatistics\\n\\nMovieLens\\n\\nLast-FM\\n\\nMIND\\n\\nFunds\\n\\n# Users\\n# Items\\n# Interactions\\n\\n6,040\\n3,260\\n998,539\\n\\n1,859\\n2,813\\n86,608\\n\\n44,603\\n15,174\\n1,285,064\\n\\n209,999\\n5,701\\n1,225,318\\n\\n# Entities\\n# Relations\\n# Triples\\n\\n12,068\\n12\\n62,958\\n\\n32,810\\n14\\n307,140\\n\\n8,111\\n12\\n65,697\\n\\n5 Experiments\\n5.1 Experimental Settings\\n\\nKnowledge Graph\\n9,614\\n2\\n118,500\\n\\nmatrix factorization. First, we adopt the simple weighted sum aggregator to learn the user-item interaction graph, which is defined\\nas:\\n(𝑙+1)\\n\\nh𝑢\\n\\n=\\n\\n1\\n\\n∑︁\\n√︁\\n𝑣 ∈ M𝑢\\n\\n(𝑙 )\\n\\n|M𝑢 ||M 𝑣 |\\n\\n(𝑙+1)\\n\\nh𝑣 ; h𝑣\\n\\n=\\n\\n1\\n\\n∑︁\\n√︁\\n𝑢 ∈ M𝑣\\n\\n|M 𝑣 ||M𝑢 |\\n\\n(𝑙 )\\n\\nh𝑢 ,\\n\\n(14)\\n(𝑙 )\\n(𝑙 )\\nwhere h𝑢 and h𝑣 represent the embeddings of user 𝑢 and item 𝑣\\nafter 𝑙 layers of propagation, respectively. The initial embeddings\\n(0)\\n(0)\\nh𝑢 = h𝑢 and h𝑣 = h′𝑣 are obtained in Section 4.3. M𝑢 denotes\\nthe set of items with which user 𝑢 has interacted, while M 𝑣 signifies\\nthe set of users who have interacted\\n√︁ with item 𝑣. The symmetric\\nnormalization term is given by 1/ |M𝑢 ||M 𝑣 |. Subsequently, the\\nembeddings acquired at each layer are combined to construct the\\nfinal representation:\\nh̃𝑢 =\\n\\n𝐿\\n\\n𝐿\\n\\n𝑙=0\\n\\n𝑙=0\\n\\n1 ∑︁ (𝑙 )\\n1 ∑︁ ′ (𝑙 )\\nh𝑢 ; h̃𝑣 =\\nh𝑣 ,\\n𝐿+1\\n𝐿+1\\n\\n(15)\\n\\nwhere 𝐿 represents the number of hidden layers. Ultimately, the\\nmodel prediction is determined by the inner product of the final\\nuser and item representations:\\n𝑦ˆ𝑢𝑣 = h̃𝑢⊤ h̃𝑣 .\\n\\n4.5\\n\\n(16)\\n\\nModel Training\\n\\nOur approach can be divided into two stages. In the first stage, we\\nemploy the LLM to comprehend the KGs, generating corresponding\\nsemantic embeddings for each item and user, denoted as s𝑣 and\\ns𝑢 , respectively. In the second stage, these semantic embeddings\\nare integrated into the recommendation model to enhance its performance. Only the second stage necessitates supervised training,\\nwhere we adopt the widely-used Bayesian Personalized Ranking\\n(BPR) loss:\\nL=\\n\\n∑︁\\n\\n−ln𝜎 (𝑦ˆ𝑢𝑣 + − 𝑦ˆ𝑢𝑣 − ) + 𝜆∥Θ∥ 22 .\\n\\n(17)\\n\\n(𝑢,𝑣 + ,𝑣 − ) ∈ O\\n\\nHere, O = {(𝑢, 𝑣 +, 𝑣 − )|(𝑢, 𝑣 + ) ∈ R +, (𝑢, 𝑣 − ) ∈ R − } represents\\nthe training set, R + denotes the observed (positive) interactions\\nbetween user 𝑢 and item 𝑣, while R − indicates the sampled unobserved (negative) interaction set. 𝜎 (·) is the sigmoid function.\\n𝜆∥Θ∥ 22 is the regularization term, where 𝜆 serves as the weight\\ncoefficient and Θ constitutes the model parameter set.\\n\\n5.1.1 Datasets. We conducted experiments on four real-world\\ndatasets, including three public datasets (MovieLens1 , MIND2 , LastFM3 ), and one industrial dataset (Fund). The statistics for these\\ndatasets are presented in Table 1. These datasets cover a wide\\nrange of application scenarios. Specifically, MovieLens is a wellestablished benchmark that collects movie ratings provided by users.\\nMIND is a large-scale news recommendation dataset constructed\\nfrom user click logs on Microsoft News. Last-FM is a well-known\\nmusic recommendation dataset that includes user listening history\\nand artist tags. The Fund dataset is sampled from the data of a\\nlarge-scale online financial platform aiming to recommend funds\\nfor users. We adopt the similar setting as numerous previous studies\\n[10, 42], filtering out items and users with fewer than five interaction records. For each dataset, we randomly select 80% of each\\nuser’s historical interactions to form the training set, while the\\nremaining 20% constitute the test set, following [10]. Each observed\\nuser-item interaction is considered a positive instance, and we apply a negative sampling strategy by pairing it with one negative\\nitem that the user has not interacted with.\\n5.1.2 Evaluation Metrics. To evaluate the performance of the models, we employ widely recognized evaluation metrics: Recall and\\nNormalized Discounted Cumulative Gain (NDCG), and report values of Recall@k and NDCG@k for k=10 and 20, following [10, 34].\\nTo ensure unbiased evaluation, we adopt the all-ranking protocol.\\nAll items that are not interacted by a user are the candidates.\\n5.1.3 Baseline Methods. To ensure a comprehensive assessment,\\nwe compare our method with 12 baseline methods, which can be divided into three categories: classical methods (BPR-MF, NFM, LightGCN), KG-enhanced methods (CKE, RippleNet, KGAT, KGIN, KGCL,\\nKGRec), and LLM-based methods (RLMRec, KAR, CLLM4Rec).\\nBPR-MF [23] employs matrix factorization to model users and\\nitems, and uses the pairwise Bayesian Personalized Ranking (BPR)\\nloss to optimize the model.\\nNFM [9] is an advanced factorization model that subsumes FM [24]\\nunder neural networks.\\nLightGCN [10] facilitates message propagation between users and\\nitems by simplifying GCN [16].\\nCKE [46] is an embedding-based method that uses TransR to guide\\nentity representation in KGs to enhance performance.\\nRippleNet [28] automatically discovers users’ hierarchical interests by iteratively propagating users’ preferences in the KG.\\nKGAT [34] designs an attentive message passing scheme over the\\nknowledge-aware collaborative graph for node embedding fusion.\\nKGIN [35] adopts an adaptive aggregation method to capture finegrained user intentions.\\nKGCL [44] uses contrastive learning for knowledge graphs to reduce potential noise and guide user preference learning.\\n\\n1 https://grouplens.org/datasets/movielens/\\n2 https://msnews.github.io/\\n3 https://grouplens.org/datasets/hetrec-2011/\\n\\n\\x0c\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nTable 2: Performance comparison of different methods, where R denotes Recall and N denotes NDCG. The best results are bolded,\\nand the second best results are underlined. The results show our improvement is statistically significant with a significance\\nlevel of 0.01.\\nMovieLens\\n\\nModel\\n\\nLast-FM\\n\\nMIND\\n\\nFunds\\n\\nR@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20\\nBPR-MF\\n0.1257 0.3100 0.2048 0.3062 0.1307 0.1352 0.1971 0.1685 0.0315 0.0238 0.0537 0.0310 0.4514 0.3402 0.5806 0.3809\\nNFM\\n0.1346 0.3558 0.2129 0.3379 0.2246 0.2327 0.3273 0.2830 0.0495 0.0356 0.0802 0.0458 0.4388 0.3187 0.5756 0.3651\\nLightGCN 0.1598 0.3901 0.2512 0.3769 0.2589 0.2799 0.3642 0.3321 0.0624 0.0492 0.0998 0.0609 0.4992 0.3778 0.6353 0.4204\\nCKE\\n0.1524 0.3783 0.2373 0.3609 0.2342 0.2545 0.3266 0.3001 0.0526 0.0417 0.0822 0.0510 0.4926 0.3702 0.6294 0.4130\\nRippleNet 0.1415 0.3669 0.2201 0.3423 0.2267 0.2341 0.3248 0.2861 0.0472 0.0364 0.0785 0.0451 0.4764 0.3591 0.6124 0.4003\\nKGAT\\n0.1536 0.3782 0.2451 0.3661 0.2470 0.2595 0.3433 0.3075 0.0594 0.0456 0.0955 0.0571 0.5037 0.3751 0.6418 0.4182\\nKGIN\\n0.1631 0.3959 0.2562 0.3831 0.2562 0.2742 0.3611 0.3215 0.0640 0.0518 0.1022 0.0639 0.5079 0.3857 0.6428 0.4259\\nKGCL\\n0.1554 0.3797 0.2465 0.3677 0.2599 0.2763 0.3652 0.3284 0.0671 0.0543 0.1059 0.0670 0.5071 0.3877 0.6355 0.4273\\nKGRec\\n0.1640 0.3968 0.2571 0.3842 0.2571 0.2748 0.3617 0.3251 0.0627 0.0506 0.1003 0.0625 0.5104 0.3913 0.6467 0.4304\\nRLMRec\\n0.1613 0.3920 0.2524 0.3787 0.2597 0.2812 0.3651 0.3335 0.0619 0.0486 0.0990 0.0602 0.4988 0.3784 0.6351 0.4210\\nKAR\\n0.1582 0.3869 0.2511 0.3722 0.2532 0.2770 0.3612 0.3324 0.0615 0.0480 0.1002 0.0613 0.5033 0.3812 0.6312 0.4175\\nCLLM4Rec 0.1563 0.3841 0.2433 0.3637 0.2571 0.2793 0.3642 0.3268 0.0631 0.0494 0.1012 0.0628 0.4996 0.3791 0.6273 0.4103\\nCoLaKG 0.1699 0.4130 0.2642 0.3974 0.2738 0.2948 0.3803 0.3471 0.0698 0.0562 0.1087 0.0684 0.5273 0.4012 0.6524 0.4392\\n\\nTable 3: Validation of the generalizability of our method:\\nExperimental results of integrating CoLaKG with various\\nrecommendation backbones.\\nModel\\n\\nMovieLens\\n\\nLast-FM\\n\\nMIND\\n\\nR@20 N@20 R@20 N@20 R@20 N@20\\nBPR-MF\\nBPR-MF+Ours\\n\\n0.2048 0.3062 0.1971 0.1685 0.0537 0.0310\\n0.2213 0.3255 0.2104 0.1812 0.0609 0.3986\\n\\nNFM\\nNFM+Ours\\n\\n0.2129 0.3379 0.3273 0.2830 0.0802 0.0458\\n0.2285 0.3527 0.3478 0.2996 0.0859 0.0487\\n\\nLightGCN\\n0.2512 0.3769 0.3642 0.3321 0.0998 0.0609\\nLightGCN+Ours 0.2642 0.3974 0.3803 0.3471 0.1087 0.0684\\n\\nKGRec [43] is a state-of-the-art KG-based recommendation model\\nwhich devises a self-supervised rationalization method to identify\\ninformative knowledge connections.\\nRLMRec [22] is an LLM-based model. It directly utilizes LLMs\\nto generate text profiles and combine them with recommendation\\nmodels through contrastive learning. Since their method is modelagnostic, to ensure a fair comparison, we chose LightGCN as its\\nbackbone model, consistent with our method.\\nKAR [41] is an LLM-based model, which utilizes LLMs to enhance\\nrecommender systems with open-world knowledge\\nCLLM4Rec [52] is also an LLM-based method. It combines ID\\ninformation and semantic information and uses the LLM directly\\nas the recommender to generate the recommendation results.\\n5.1.4 Implementation Details. We implement all baseline methods according to their released code. The embedding size 𝑑 for all\\nrecommendation methods is set to 64 for a fair comparison. All\\nexperiments are conducted with a single V100 GPU. We set the\\nbatch size to 1024 for the Last-FM dataset and 4096 for the other\\ndatasets to expedite training. The Dropout rate is chosen from the\\n\\nset {0.2, 0.4, 0.6, 0.8} for both the embedding layer and the hidden\\nlayers. We employ the Adam optimizer with a learning rate of 0.001.\\nThe maximum number of epochs is set to 2000. The number of\\nhidden layers for the recommendation model 𝐿 is set to 3. For the\\nLLM, we select DeepSeek-V2, a robust large language model that\\ndemonstrates exceptional performance on both standard benchmarks and open-ended generation evaluations. For more detailed\\ninformation about DeepSeek, please refer to their official website4 .\\nSpecifically, we utilize DeepSeek-V2 by invoking its API5 . To reduce\\ntext randomness of the LLM, we set the temperature 𝜏 to 0 and\\nthe top-𝑝 to 0.001. For the text embedding model P, we use the\\npre-trained sup-simcse-roberta-large6 [7]. We use identical settings\\nfor the baselines that also involve LLMs and text embeddings to\\nensure fairness in comparison.\\n\\n5.2\\n\\nComparison Results\\n\\nWe compare 12 baseline methods across four datasets and run each\\nexperiment five times. The average results are reported in Table 2.\\nBased on the results, we make the following observations:\\n• Our method consistently outperforms all the baseline models\\nacross all four datasets. The performance ceiling of traditional\\nmethods (BPR-MF, NFM, LightGCN) is generally lower than that\\nof KG-based methods, as the former rely solely on collaborative\\nsignals without incorporating semantic knowledge. However,\\nsome KG-based methods do not perform as well as LightGCN,\\nindicating that effectively leveraging KG is a challenging task.\\n• Among the KG-based baselines, KGCL and KGRec are notable for\\nincorporating self-supervised learning into general KG-based recommendation frameworks. However, they struggle with missing\\nfacts, understanding semantic information, and modeling higherorder item associations within the KG. In contrast, our method\\nleverages LLMs to address these challenges without requiring\\n4 https://github.com/deepseek-ai/DeepSeek-V2\\n5 https://api-docs.deepseek.com/\\n6 https://huggingface.co/princeton-nlp/sup-simcse-roberta-large\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nTable 4: Ablation study on all four datasets.\\nMetric w/o s𝑣 w/o s𝑢 w/o N𝑘 (𝑣) w/o D𝑣′ CoLaKG\\nML\\n\\nR@20 0.2553 0.2613\\nN@20 0.3811 0.3948\\n\\n0.2603\\n0.3902\\n\\n0.2628\\n0.3960\\n\\n0.2642\\n0.3974\\n\\nLast-FM\\n\\nR@20 0.3628 0.3785\\nN@20 0.3278 0.3465\\n\\n0.3725\\n0.3403\\n\\n0.3789\\n0.3459\\n\\n0.3803\\n0.3471\\n\\nMIND\\n\\nR@20 0.1043 0.1048\\nN@20 0.0640 0.0658\\n\\n0.1064\\n0.0662\\n\\n0.1076\\n0.0671\\n\\n0.1087\\n0.0684\\n\\nFunds\\n\\nR@20 0.6382 0.6481\\nN@20 0.4247 0.4351\\n\\n0.6455\\n0.4305\\n\\n0.6499\\n0.4378\\n\\n0.6524\\n0.4392\\n\\nself-supervised tasks, leading to significant improvements across\\nall datasets and metrics.\\n• For LLM-based recommendation methods, we have selected several representative approaches: RLMRec, KAR, and CLLM4Rec.\\nIt is evident that these methods exhibit only marginal improvements over traditional techniques. In contrast, our method demonstrates a substantial performance enhancement compared to\\nthese LLM-based baselines, thereby further validating the superiority of our approach. Our method effectively leverages LLMs to\\ncomprehend both the local subgraphs and global relationships\\nwithin knowledge graphs, resulting in significant performance\\nimprovements.\\n\\n5.3\\n\\nValidation of the Generalizability\\n\\nIn this section, we validate the versatility of CoLaKG. Specifically,\\nwe integrate our method into three different classical recommendation model backbones and observe the performance improvements\\nof these models across three public datasets. The results, as shown\\nin Table 3, clearly indicate significant performance improvements\\nwhen our method is combined with various recommendation backbones. This experiment demonstrates that our approach, which\\nleverages LLMs to understand KGs and enhance recommendation\\nmodels, can be flexibly applied to different recommendation models\\nto improve their performance.\\n\\n5.4\\n\\nAblation Study\\n\\nIn this section, we demonstrate the effectiveness of our model by\\ncomparing its performance with four different versions across all\\nfour datasets. The results are shown in Table 4, where “w/o s𝑣 ”\\ndenotes removing the semantic embeddings of items, “w/o s𝑢 ” denotes removing the semantic embeddings of users, “w/o N𝑘 (𝑣)”\\nmeans removing the neighbor augmentation of items based on the\\nconstructed item-item graph, and “w/o D𝑣′ ” means removing the\\nsecond-order triples from the LLM’s prompts. When the semantic embeddings of items are removed, the model’s performance\\nsignificantly decreases across all datasets, underscoring the critical role of semantic information captured by LLMs from the KG.\\nSimilarly, the removal of user semantic embeddings also results\\nin a performance decline, affirming that LLMs can effectively infer user preferences from the KG. Furthermore, removing N𝑘 (𝑣)\\nleads to a performance drop across all datasets, highlighting the\\nsignificance of the item representation augmentation module based\\non the constructed semantic-relational item-item graph. Without\\nthis module, the model can only capture local KG information from\\n\\nFigure 4: Hyperparameter study of the number of retrieved\\nneighbors (𝑘) and sampled number of 2-hop items within the\\nprompt (𝑚) on four datasets.\\nitem-centered subgraphs and cannot leverage the semantic relations\\npresent in the global KG. The inclusion of this module facilitates\\nthe effective integration of both local and global KG information.\\nLastly, removing second-order KG triples from the prompts causes\\na slight performance decline. This finding suggests that incorporating second-order information from the KG allows the LLMs to\\nproduce a higher-quality comprehension of the local KG.\\n\\n5.5\\n\\nHyperparameter Study\\n\\nIn this section, we investigate the impact of the hyperparameter\\n𝑘 and 𝑚 across four datasets. Here, 𝑘 represents the number of\\nsemantically related neighbors, as defined in Section 4.1.2, 𝑚 is\\nthe number of second-order neighbors used in the prompt, which\\nis defined in Section 4.1.1. The results are presented in Figure 4.\\nWe observe that as 𝑘 increases, both Recall@20 and NDCG@20\\ninitially rise and then slightly decline across all datasets. The performance is worst when 𝑘 = 0 and best when 𝑘 is between 10 and\\n30. When 𝑘 = 0, no neighbors are used, which is equivalent to the\\nablation study without N𝑘 (𝑣), thereby not incorporating any global\\nsemantic associations from the KG. When 𝑘 > 0, the introduction\\nof semantically related items enhances the item’s representations,\\nleading to a noticeable improvement. However, as 𝑘 continues to\\nincrease, some noise may be introduced because the relevance of\\nneighbors decreases with their ranking. Consequently, items with\\nlower relevance may interfere with the recommendation performance. Our findings suggest that a range of 10-30 neighbors is\\noptimal. As the value of 𝑚 increases, the metrics initially rise and\\nthen slightly decline. When 𝑚 = 0, the prompt used to understand\\nthe KG subgraph includes only first-order neighbors, resulting in\\nthe poorest performance. This indicates the positive impact of incorporating second-order neighbors. However, as 𝑚 continues to\\ngrow, the marginal benefits diminish, and additional noise may be\\nintroduced, leading to a slight decrease in performance.\\n\\n5.6\\n\\nRobustness to Varying Degrees of Sparsity\\n\\nOne of the key functions of KGs is to alleviate the issue of data\\nsparsity. To further examine the robustness of our model against\\nusers with varying levels of activity, particularly its performance\\nwith less active users, we sort users based on their interaction\\n\\n\\x0c\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\nDisconnected or Distant Paths\\nto “Apollo 13” in original KG\\n\\nCase 1\\n\\nConnected to “Apollo 13” by\\nshared genres in original KG\\n\\nApollo 13\\n\\nThe Right Stuff\\n\\nBirdy\\n\\nThe Great Escape\\n\\nTop Gun\\n\\nStar Trek: First Contact\\n\\nNeighbors of “Apollo 13”\\nConnected to “A Little Princess”\\nby shared genres in original KG\\n\\nDisconnected or Distant Paths to\\n“A Little Princess” in original KG\\n\\nA Simple Wish Quest for Camelot Now and Then\\n\\nThe Story of Cinderella The Princess Bride\\n\\nCase 2\\n\\nFigure 5: Performance comparison on different user groups,\\nwhere a smaller group ID indicates fewer interaction records.\\n\\nA Little Princess\\n\\nNeighbors of “A Little Princess”\\n\\nFigure 7: Case study.\\n\\nFigure 6: Performance comparison using complete KG and\\nincomplete KG on two datasets. Models with the \"-r\" suffix\\nindicate their performance in the presence of missing facts.\\nfrequency and divide them into four equal groups. A lower group\\nID indicates lower user activity (01 being the lowest, 04 the highest).\\nWe analyze the evaluation results on two relatively sparse datasets,\\nLast-FM and MIND, as shown in Figure 5. By comparing our model\\nwith three representative and strong baseline models, we observe\\nthat our model consistently outperforms the baselines in each user\\ngroup. Notably, the improvement ratio of our model in the sparser\\ngroups (01 and 02) is higher compared to the denser groups (03 and\\n04). For the group with the most limited data (Group 01), our model\\nachieves the most significant lead. This indicates that the average\\nimprovement of our model is primarily driven by enhancements in\\nthe sparser groups, demonstrating the positive impact of CoLaKG\\nin addressing data sparsity.\\n\\n5.7\\n\\nRobustness to Missing Facts\\n\\nTo further demonstrate the robustness of our proposed ColaKG in\\nscenarios where facing the challenge of missing facts in KG, we\\nconducted comparative experiments on the Movielens and Last-FM\\ndatasets. Specifically, we randomly dropped 30% of the KG entities and relations to construct datasets with significant missing\\nfacts based on the original datasets. The comparison results of\\nour method with representative KG-based baselines on the constructed datasets are shown in Figure 6. From the experimental\\nresults, we can see that removing KG facts has a negative impact on\\nall KG-based methods. However, in the case of an incomplete KG,\\nour method, CoLaKG, still outperforms the other baselines. Furthermore, by comparing the performance degradation of different\\nmethods on incomplete and complete KGs, we find that our method\\nexperiences the smallest proportion of performance decline. This\\ndemonstrates that our method can effectively mitigate the impact\\nof missing facts on performance, further proving the robustness\\nand superiority of the proposed approach.\\n\\n5.8\\n\\nCase Study\\n\\nIn this section, we conduct an in-depth analysis of the rationality\\nof our method through two real cases. In the first case, we present\\n\\nthe movie “Apollo 13” and its five semantically related neighbor\\nitems in the item-item graph identified by our method. The first\\nthree movies belong to the same genre as “Apollo 13”, making them\\n2-hop neighbors in the KG. In contrast, the other two movies, “Top\\nGun” and “Star Trek”, do not share any genre or other attributes\\nwith “Apollo 13”, indicating they are distant or unconnected in the\\nKG. However, “Top Gun” and “Star Trek” are semantically related to\\n“Apollo 13” as they all highlight themes of human resilience, courage,\\nand the spirit of adventure. Traditional KG-based recommendation\\nmethods, which rely on layer-by-layer information propagation,\\nstruggle to capture such high-order neighbors. In contrast, our\\nmethod leverages similarity calculations based on item-centered KG\\nsemantic embeddings, successfully identifying these two strongly\\nrelated movies. This demonstrates that our approach can effectively\\nand efficiently capture semantically relevant information from the\\nglobal KG. In the second case, we examine the movie “A Little\\nPrincess” and its related neighbors. Among the five related movies\\nidentified, “The Story of Cinderella” and “The Princess Bride” should\\nshare the same genre as “A Little Princess”. However, due to missing\\ngenres in the KG, these movies lack a path to “A Little Princess”\\nwithin the KG. Despite this, our method successfully identifies these\\ntwo movies. This demonstrates that our approach, by leveraging\\nLLMs to complete and interpret the KG, can effectively address\\nchallenges posed by missing key attributes.\\n\\n6\\n\\nConclusion\\n\\nIn this paper, we analyze the limitations of existing KG-based recommendation methods and propose a novel approach, CoLaKG,\\nto address these issues. CoLaKG comprehends item-centered KG\\nsubgraphs to obtain semantic embeddings for both items and users.\\nThese semantic embeddings are then used to construct a semantic\\nrelational item-item graph, effectively leveraging global KG information. We conducted extensive experiments on four datasets to\\nvalidate the effectiveness and robustness of our method. The results demonstrate that our approach significantly enhances the\\nperformance of recommendation models.\\n\\n7\\n\\nAcknowledgments\\n\\nThis work was supported by the Early Career Scheme (No. CityU\\n21219323) and the General Research Fund (No. CityU 11220324)\\nof the University Grants Committee (UGC), and the NSFC Young\\nScientists Fund (No. 9240127).\\n\\n\\x0cSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nZiqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma\\n\\nReferences\\n[1] Arkadeep Acharya, Brijraj Singh, and Naoyuki Onoe. 2023. Llm based generation\\nof item-description for recommendation system. In Proceedings of the 17th ACM\\nConference on Recommender Systems. 1204–1207.\\n[2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan\\nHe. 2023. Tallrec: An effective and efficient tuning framework to align large\\nlanguage model with recommendation. In Proceedings of the 17th ACM Conference\\non Recommender Systems. 1007–1014.\\n[3] Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019.\\nUnifying knowledge graph learning and recommendation: Towards a better\\nunderstanding of user preferences. In The world wide web conference. 151–161.\\n[4] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao\\nPu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, et al. 2023. When large language\\nmodels meet personalization: Perspectives of challenges and opportunities. arXiv\\npreprint arXiv:2307.16376 (2023).\\n[5] Djork-Arné Clevert. 2015. Fast and accurate deep network learning by exponential\\nlinear units (elus). arXiv preprint arXiv:1511.07289 (2015).\\n[6] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang\\nTang, and Qing Li. 2023. Recommender systems in the era of large language\\nmodels (llms). arXiv preprint arXiv:2307.02046 (2023).\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive\\nLearning of Sentence Embeddings. In Empirical Methods in Natural Language\\nProcessing (EMNLP).\\n[8] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong,\\nand Qing He. 2020. A survey on knowledge graph-based recommender systems.\\nIEEE Transactions on Knowledge and Data Engineering 34, 8 (2020), 3549–3568.\\n[9] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse\\npredictive analytics. In Proceedings of the 40th International ACM SIGIR conference\\non Research and Development in Information Retrieval. 355–364.\\n[10] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network for\\nrecommendation. In Proceedings of the 43rd International ACM SIGIR conference\\non research and development in Information Retrieval. 639–648.\\n[11] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng,\\nBodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large\\nlanguage models as zero-shot conversational recommenders. In Proceedings of the\\n32nd ACM international conference on information and knowledge management.\\n720–730.\\n[12] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,\\nand Wayne Xin Zhao. 2024. Large language models are zero-shot rankers for\\nrecommender systems. In European Conference on Information Retrieval. Springer,\\n364–381.\\n[13] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. 2018. Leveraging metapath based context for top-n recommendation with a neural co-attention model.\\nIn Proceedings of the 24th ACM SIGKDD international conference on knowledge\\ndiscovery & data mining. 1531–1540.\\n[14] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan,\\nAng Li, Zuoli Tang, and Jun Zhou. 2024. Enhancing sequential recommendation\\nvia llm-based semantic embedding learning. In Companion Proceedings of the\\nACM on Web Conference 2024. 103–111.\\n[15] Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and\\nChanyoung Park. 2024. Large Language Models meet Collaborative Filtering: An\\nEfficient All-round LLM-based Recommender System. In Proceedings of the 30th\\nACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1395–1406.\\n[16] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph\\nconvolutional networks. arXiv preprint arXiv:1609.02907 (2016).\\n[17] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023. Large language models\\nfor generative recommendation: A survey and visionary discussions. arXiv\\npreprint arXiv:2309.01157 (2023).\\n[18] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan,\\nRuiming Tang, Yong Yu, and Weinan Zhang. 2024. Rella: Retrieval-enhanced\\nlarge language models for lifelong sequential behavior comprehension in recommendation. In Proceedings of the ACM on Web Conference 2024. 3497–3508.\\n[19] Xiaolin Lin, Jinwei Luo, Junwei Pan, Weike Pan, Zhong Ming, Xun Liu, Shudong\\nHuang, and Jie Jiang. 2024. Multi-sequence attentive user representation learning\\nfor side-information integrated sequential recommendation. In Proceedings of the\\n17th ACM International Conference on Web Search and Data Mining. 414–423.\\n[20] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning\\nentity and relation embeddings for knowledge graph completion. In Proceedings\\nof the AAAI conference on artificial intelligence, Vol. 29.\\n[21] Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang.\\n2018. Deepinf: Social influence prediction with deep learning. In Proceedings of\\nthe 24th ACM SIGKDD international conference on knowledge discovery & data\\nmining. 2110–2119.\\n[22] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei\\nYin, and Chao Huang. 2024. Representation learning with large language models\\n\\nfor recommendation. In Proceedings of the ACM on Web Conference 2024. 3464–\\n3475.\\n[23] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\\n2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint\\narXiv:1205.2618 (2012).\\n[24] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme.\\n2011. Fast context-aware recommendations with factorization machines. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. 635–644.\\n[25] Yu Tian, Yuhao Yang, Xudong Ren, Pengfei Wang, Fangzhao Wu, Qian Wang, and\\nChenliang Li. 2021. Joint knowledge pruning and recurrent graph convolution\\nfor news recommendation. In Proceedings of the 44th international ACM SIGIR\\nconference on research and development in information retrieval. 51–60.\\n[26] Riku Togashi, Mayu Otani, and Shin’ichi Satoh. 2021. Alleviating cold-start\\nproblems in recommendation through pseudo-labelling over knowledge graph.\\nIn Proceedings of the 14th ACM international conference on web search and data\\nmining. 931–939.\\n[27] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\\narXiv:1710.10903 (2017).\\n[28] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie,\\nand Minyi Guo. 2018. Ripplenet: Propagating user preferences on the knowledge\\ngraph for recommender systems. In Proceedings of the 27th ACM international\\nconference on information and knowledge management. 417–426.\\n[29] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep\\nknowledge-aware network for news recommendation. In Proceedings of the 2018\\nworld wide web conference. 1835–1844.\\n[30] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge\\ngraph convolutional networks for recommender systems. In The world wide web\\nconference. 3307–3313.\\n[31] Lei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using\\nlarge pretrained language models. arXiv preprint arXiv:2304.03153 (2023).\\n[32] Maolin Wang, Dun Zeng, Zenglin Xu, Ruocheng Guo, and Xiangyu Zhao. 2023.\\nFederated knowledge graph completion via latent embedding sharing and tensor\\nfactorization. In 2023 IEEE International Conference on Data Mining (ICDM). IEEE,\\n1361–1366.\\n[33] Shuyao Wang, Yongduo Sui, Chao Wang, and Hui Xiong. 2024. Unleashing\\nthe Power of Knowledge Graph for Recommendation via Invariant Learning. In\\nProceedings of the ACM on Web Conference 2024. 3745–3755.\\n[34] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat:\\nKnowledge graph attention network for recommendation. In Proceedings of the\\n25th ACM SIGKDD international conference on knowledge discovery & data mining.\\n950–958.\\n[35] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu,\\nXiangnan He, and Tat-Seng Chua. 2021. Learning intents behind interactions\\nwith knowledge graph for recommendation. In Proceedings of the web conference\\n2021. 878–887.\\n[36] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng\\nChua. 2019. Explainable reasoning over knowledge graphs for recommendation.\\nIn Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5329–5336.\\n[37] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models\\nwith graph augmentation for recommendation. In Proceedings of the 17th ACM\\nInternational Conference on Web Search and Data Mining. 806–815.\\n[38] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. 2024.\\nExploring large language model for graph data understanding in online job\\nrecommendations. In Proceedings of the AAAI Conference on Artificial Intelligence,\\nVol. 38. 9178–9186.\\n[39] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,\\nChuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A survey on large\\nlanguage models for recommendation. arXiv preprint arXiv:2305.19860 (2023).\\n[40] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,\\nChuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2024. A survey on large\\nlanguage models for recommendation. World Wide Web 27, 5 (2024), 60.\\n[41] Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo\\nChen, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. Towards open-world\\nrecommendation with knowledge augmentation from large language models. In\\nProceedings of the 18th ACM Conference on Recommender Systems. 12–22.\\n[42] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin\\nDing, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In\\n2022 IEEE 38th international conference on data engineering (ICDE). IEEE, 1259–\\n1273.\\n[43] Yuhao Yang, Chao Huang, Lianghao Xia, and Chunzhen Huang. 2023. Knowledge\\ngraph self-supervised rationalization for recommendation. In Proceedings of the\\n29th ACM SIGKDD conference on knowledge discovery and data mining. 3046–3056.\\n[44] Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. 2022. Knowledge\\ngraph contrastive learning for recommendation. In Proceedings of the 45th international ACM SIGIR conference on research and development in information\\n\\n\\x0c\\n\\nretrieval. 1434–1443.\\n[45] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation:\\nA heterogeneous information network approach. In Proceedings of the 7th ACM\\ninternational conference on Web search and data mining. 283–292.\\n[46] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.\\n2016. Collaborative knowledge base embedding for recommender systems. In\\nProceedings of the 22nd ACM SIGKDD international conference on knowledge\\ndiscovery and data mining. 353–362.\\n[47] Haotian Zhang, Shuanghong Shen, Bihan Xu, Zhenya Huang, Jinze Wu, Jing\\nSha, and Shijin Wang. 2024. Item-Difficulty-Aware Learning Path Recommendation: From a Real Walking Perspective. In Proceedings of the 30th ACM SIGKDD\\nConference on Knowledge Discovery and Data Mining. 4167–4178.\\n[48] Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang, and Lihong Gu. 2024. Breaking\\nthe Barrier: Utilizing Large Language Models for Industrial Recommendation\\n\\nSIGIR ’25, July 13–18, 2025, Padua, Italy\\n\\nSystems through an Inferential Knowledge Graph. arXiv preprint arXiv:2402.13750\\n(2024).\\n[49] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\\n[50] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen\\nWen, Fei Wang, Xiangyu Zhao, Jiliang Tang, et al. 2023. Recommender systems\\nin the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023).\\n[51] Xinjun Zhu, Yuntao Du, Yuren Mao, Lu Chen, Yujia Hu, and Yunjun Gao. 2023.\\nKnowledge-refined Denoising Network for Robust Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development\\nin Information Retrieval. 362–371.\\n[52] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2024. Collaborative large language model for recommender systems. In Proceedings of the\\nACM on Web Conference 2024. 3162–3172.\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUFQLnsbXqeL",
        "outputId": "804bed92-bd4d-4e97-e772-10e62f96563a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/TTS/api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
            "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Downloading model to /root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 422M/425M [00:19<00:00, 29.3MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Model's license - CC BY-NC-ND 4.0\n",
            " > Check https://creativecommons.org/licenses/by-nc-nd/4.0/ for more info.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Model fully restored. \n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:64\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:512\n",
            " | > power:1.5\n",
            " | > preemphasis:0.97\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:False\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000.0\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:False\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:True\n",
            " | > db_level:-27.0\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:160\n",
            " | > win_length:400\n",
            " > External Speaker Encoder Loaded !!\n",
            " > initialization of language-embedding layers.\n",
            " > Model fully restored. \n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:64\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:512\n",
            " | > power:1.5\n",
            " | > preemphasis:0.97\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:False\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000.0\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:False\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:True\n",
            " | > db_level:-27.0\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:160\n",
            " | > win_length:400\n",
            "Splitter used: Regex fallback\n",
            "Sentence count: 604\n"
          ]
        }
      ],
      "source": [
        "# === 3) Decide sentence splitter (TTS built-in vs regex) ===\n",
        "\n",
        "sentences = None\n",
        "tts = None\n",
        "use_tts_splitter = False\n",
        "\n",
        "if TRY_TTS_SPLITTER:\n",
        "    try:\n",
        "        from TTS.api import TTS as _TTS\n",
        "\n",
        "        # Only create an instance if we need built-in splitting OR audio.\n",
        "        # Warning: constructing may download a model.\n",
        "        tts = _TTS(MODEL_NAME, gpu=USE_GPU and MAKE_AUDIO)\n",
        "        # Try the built-in splitter if available\n",
        "        if hasattr(tts, \"split_text\"):\n",
        "            sentences = tts.split_text(text_source)\n",
        "            use_tts_splitter = True\n",
        "        else:\n",
        "            sentences = fallback_split_sentences(text_source)\n",
        "    except Exception as e:\n",
        "        print(f\"[Info] Built-in TTS splitter unavailable ({e}). Using regex fallback.\")\n",
        "        sentences = fallback_split_sentences(text_source)\n",
        "else:\n",
        "    sentences = fallback_split_sentences(text_source)\n",
        "\n",
        "print(f\"Splitter used: {'TTS built-in' if use_tts_splitter else 'Regex fallback'}\")\n",
        "print(f\"Sentence count: {len(sentences)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Ri8hgiXvbO",
        "outputId": "9ae65cfa-72e9-4635-9b44-8292d35f2e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Wrote 31 paragraph files to /content/out4\n"
          ]
        }
      ],
      "source": [
        "# === 4) Build paragraphs and save as para_XXX.txt ===\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "out_dir = Path(OUTPUT_DIR)\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "paragraphs = pack_sentences(\n",
        "    sentences,\n",
        "    max_chars=MAX_CHARS,\n",
        "    max_sentences=MAX_SENTENCES\n",
        ")\n",
        "width = max(3, int(math.log10(max(1, len(paragraphs))) + 1))\n",
        "\n",
        "for i, para in enumerate(paragraphs, 1):\n",
        "    (out_dir / f\"para_{i:0{width}d}.txt\").write_text(para, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"📝 Wrote {len(paragraphs)} paragraph files to {out_dir.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhkkrjyjYH5o",
        "outputId": "63779b3e-7c3a-4c89-f449-553e1870ebfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Abstract In recent years, the introduction of knowledge graphs (KGs) has significantly advanced recommender systems by facilitating the discovery of potential associations between items. However, existing methods still face several limitations. First, most KGs suffer from missing facts or limited scopes. Second, existing methods convert textual information in KGs into IDs, resulting in the loss of natural semantic connections between different items. Third, existing methods struggle to capture high-order connections in the global KG. To address these limitations, we propose a novel method called CoLaKG, which leverages large language models (LLMs) to improve KG-based recommendations. The extensive knowledge and remarkable reasoning capabilities of LLMs enable our method to supplement missing facts in KGs, and their powerful text understanding abilities allow for better utilization of semantic information. Specifically, CoLaKG extracts useful information from KGs at both local and global levels. By employing the item-centered subgraph extraction and prompt engineering, it can accurately understand the local information. In addition, through the semantic-based retrieval module, each item is enriched by related items from the entire knowledge graph, effectively harnessing global information. Furthermore, the local and global information are effectively integrated into the recommendation model through a representation fusion module and a retrieval-augmented representation learning module, respectively. Extensive experiments on four real-world datasets ∗ Work done during an internship at Tencent. † Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’25, Padua, Italy © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.',\n",
              " 'ACM ISBN 979-8-4007-1592-1/2025/07 https://doi.org/10.1145/3726302.3729932 demonstrate the superiority of our method. The code of our method is available at https://github.com/ziqiangcui/CoLaKG. CCS Concepts • Information systems → Recommender systems. Keywords Large Language Models, Knowledge Graphs, Recommendation 1 Introduction The rapid advancement of online platforms has led to an increasingly critical issue of information overload. Recommender systems address this problem by modeling user preferences based on historical data. Collaborative filtering (CF) [9, 10, 23], as one of the most classic and efficient methods, has been extensively employed in existing recommender systems. However, CF-based methods exclusively rely on user-item interaction records, often suffering from the data sparsity issue [26]. To address this issue, recent studies [34, 44, 46] have incorporated knowledge graphs (KGs) as external knowledge sources into recommendation models, achieving significant progress. Typically, these methods capture diverse and high-order relationships between items by modeling the structure and attribute information in KGs, thereby enhancing the learning process of user and item representations [44]. Despite the effectiveness of existing KG-enhanced recommendation methods, they still face several challenges. i) First, many KGs suffer from missing facts and limited scopes [8, 32], as constructing KGs often requires manual effort and domain expertise. The absence of key attributes, such as the genres of a movie, can cause items SIGIR ’25, July 13–18, 2025, Padua, Italy Item node Attribute node x-hop 𝑟! a. Missing facts: 2-hop H F Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma A 𝑟% D E 𝑟! 𝑟$ 1-hop 𝑟! 𝑟\" A 𝑟$ 𝑟# P G 𝑟$ C 𝑟! B 𝑟! C b. Textual Entities (e.g., Title/Tag/Genre..) Similar semantic B D Horror ID: 51 𝑟! Thriller ID: 320 E Different IDs c.',\n",
              " 'Distant neighbors Target item B • • C G H A 2-hop A x-hop H B, C, G have no connections with A in the KG H and A are too distant in the KG Figure 1: An illustrative diagram demonstrating the potential issues of existing KG-based recommendation methods. that originally share the same attribute to lose their connections. As illustrated in Figure 1, items A and B should have a connecting path (A-P-B) in the KG. However, due to item A missing the P attribute, A and B are now not associated with each other. Such biased knowledge will lead to suboptimal performance of the recommendation model. ii) Second, existing methods [33–35, 43, 44] convert textual entities and relations into IDs for use, which can result in losing natural semantic connections between items. For instance, in Figure 1, “horror” and “thriller” are two semantically related attributes of Item F and Item G, respectively. However, similar semantics are not reflected in different entity IDs (51 and 320), which further results in the related items F and G having no connections in the KG. iii) Third, existing methods [25, 33–35, 43, 44] struggle to capture high-order relationships in the entire KG. Most of them propagate and aggregate information by stacking multiple layers of graph neural networks (GNNs). The layer-by-layer propagation is not only inefficient but also accumulates a large amount of irrelevant node information, leading to the over-smoothing issue [8, 33]. For instance, assumming that points A and H in Figure 1 have a strong semantic connection, the considerable distance between them in the KG presents big challenges for existing methods in capturing this relation. As shown in the lower left corner of Figure 1, due to the aforementioned three limitations, existing methods fail to identify the potential connections between historically interacted items and the target item through the KG, resulting in suboptimal recommendations. Empowered by extensive knowledge and remarkable reasoning abilities, large language models (LLMs) have demonstrated significant promise in semantic understanding and knowledge extraction. Recently, many studies have leveraged LLMs to improve recommendation models. For example, some studies utilize LLMs to generate semantic representations of item profiles [22], while other studies [48] employ LLMs to determine whether a complementary relationship exists between two items, thereby recommending complementary products based on users’ historical behaviors. However, these methods do not fully exploit the semantic and structural information of KGs. As one of the most common and important sources of knowledge, KGs contain a wealth of semantic associations among entities and relations, which are often overlooked by existing methods that typically consider only item profiles. Additionally, KGs serve as task-relevant knowledge repositories, effectively aiding LLMs in acquiring task-specific knowledge and mitigating the issue of hallucinations caused by excessive divergence. Nevertheless, leveraging LLMs to handle the diverse semantic relationships in KGs is highly challenging, as it is unrealistic to input the vast number of entities and connections in KGs into a language model.',\n",
              " 'To bridge this gap, in this paper, we propose a novel method named Comprehending Knowledge Graphs with Large Language Models for Recommendation (CoLaKG). The core idea is to leverage LLMs for understanding the semantic and structural information of KGs to enhance the representation learning of items and users. Our method comprises two stages: i) Comprehending KGs with LLMs. Given the impracticality of inputting the entire KG into an LLM, CoLaKG leverages KG information by two components: a Local KG Comprehension module which involves extracting item-centered KG subgraphs for analysis by LLMs, and a Retrieval-based Global KG Utilization module which retrieves semantically related items from the entire KG to effectively leverage global KG information. ii) Incorporating semantic embeddings into the recommendation model. This stage integrates the semantic embeddings of KGs with the ID embeddings, thereby leveraging both collaborative signals and semantic information. First, we align and fuse the item ID representations with the item-centered KG subgraph representations. Then, we employ a retrieval-augmented representation method to further enhance item representations with semantically similar neighbors from the entire KG. It is important to note that these two stages are decoupled, meaning that our method does not involve LLM inference during the recommendation process. This allows our method to be efficiently applied in real-world recommendation scenarios. Our contributions are summarized as follows: • We propose a novel method that utilizes LLMs to comprehend and transform the semantic and structural information of KGs. This approach addresses the issues of missing facts and the inability to leverage semantic information from text in current KG-based recommendation methods. • We propose leveraging both local KG information and global KG information with LLMs. In addition, we design a retrievalaugmented method to enhance item representation learning. • Extensive experiments are conducted on four real-world datasets to validate the superiority of our method. Further analysis demonstrates the rationale behind our approach. 2 Related Work 2.1 Knowledge-aware Recommendation Existing knowledge-aware recommendation methods can be categorized into three types [8]: embedding-based methods, path-based methods, and GNN-based methods. Embedding-based methods [3, 19, 29, 46] enhance the representations of users and items by leveraging the relations and entities within the KGs. Notable examples include CKE [46], which integrates various types of side information into a collaborative filtering framework using TransR [20]. Another example is DKN [29], which improves news representations by combining textual embeddings of sentences and knowledge-level embeddings of entities.',\n",
              " 'Path-based methods leverage KGs to explore long-range connectivity [13, 36, 45, 47]. For example, Personalized Entity Recommendation (PER) [45] treats a Item-centered KG Subgraphs 𝑟$ … Th eR ea de r Ka te 𝑟& … … n io pt ce In … |𝒰| Prompt Engineering 𝑣\" 𝑟($! ,$\") 𝑟 𝑁% (𝑣& ) Adapter ❄ 𝑠\\' 𝑠\"! 𝑟* Acted by |𝒱| 𝑟) Directed 𝑟( Is the genre of User 𝑠\\' Item 𝒫(#) Text Embedding Model 𝑒\" 𝑠\" 𝑣! User Preference Comprehension Retrieval-Augmented Representation Learning ❄ 𝑣! LLM … Retrieval-based Global KG Utilization 𝑟(\"! ,\"\" ) 𝑟+ Genre Text Embedding Model Assume you are a film expert … 𝑟# 𝑟! Relations 𝑟, Directed by 𝑠\" 𝒫(#) KG-based user interactions: {“Titanic”: (Titanic, Directed by, James Cameron); (Titanic, Acted by, Leonardo | Kate); (Titanic, Genres, Romance | Disaster);… } {“Inception”: (Inception, Directed by, Nolan); (Inception, Acted by, Leonardo DiCaprio | Ellen Page); (Inception, Genres, Science Fiction | Action);… } … 𝑟\" Local KG Comprehension ℎ\" 𝑠\"( 𝑠\\'( Entities other than items Addition |𝒰| Inner product ❄ Freeze U-I Modeling ℱ(-) ℎ)\" ℎ\"( … … … 𝑟# 𝑟! LLM 𝑟\\' Starred in |𝒱| User-centered Subgraphs 𝑟! 𝑟\" Prompt Engineering Assume you are a film expert … Assume you are a film expert … One-hop knowledge in the format of triples : (Titanic, Directed by, James Cameron); (Titanic, Acted by, Leonardo DiCaprio …); (Titanic, Genres, Romance); (Titanic, plot, …); … Two-hop knowledge: James Cameron also directed… ; Leonardo DiCaprio also starred in… ; Movies in the similar genres also include … ; Movies with the similar plot also include … ; ... ℎ\\' 𝛼!\" 𝑒\\' ℎ\\' ℎ)\\' 𝑦+\\'\" … t Alien lie s Ju Av 𝑟$ … & ala eo James n nc m o er he Ro m a 𝑟& C 𝑟& 𝑟& 𝑟! Disaster e 𝑟\" c 𝑟& an 𝑟\" m 𝑟# 𝑟# Ro Titanic Leo 𝑟% 𝑟% nard o 𝑟% 𝑟% User-Item-Entity Graph SIGIR ’25, July 13–18, 2025, Padua, Italy 𝑠\"\" Figure 2: The framework of our proposed CoLaKG. Knowledge Graph (KG) as a heterogeneous information network and extracts meta-path-based latent features to represent the connectivity between users and items along various types of relational paths. MCRec [13] constructs meta-paths and learns the explicit representations of meta-paths to depict the interaction context of user-item pairs. Despite their effectiveness, these approaches heavily rely on domain knowledge and human effort for metapath design. Recently, GNN-based methods have been proposed, which enhance entity and relation representations by aggregating embeddings from multi-hop neighbors [30, 34, 35]. For instance, KGAT [34] employs the graph attention mechanism to propagate embeddings and utilizes multi-layer perceptrons to generate final recommendation scores in an end-to-end manner. Similarly, KGIN [35] adopts an adaptive aggregation method to capture fine-grained user intentions. Additionally, some methods [33, 43, 44, 51] employ contrastive learning to mitigate potential knowledge noise and identify informative knowledge connections. 2.2 LLMs for Recommendation In light of the emergence of large language models and their remarkable achievements in the field of NLP, scholars have begun to explore the potential application of LLMs in recommender systems [4, 6, 39, 49].',\n",
              " 'Due to the powerful reasoning capabilities and extensive world knowledge of LLMs, they have been already naturally applied to zero-shot [11, 12, 31] and few-shot recommendation scenarios [2, 18]. In these studies, LLMs are directly used as a recommendation model [17, 50], where the output of LLMs is expected to offer a reasonable recommendation result [40]. However, when the dataset is sufficiently large, their performance often falls short of that achieved by traditional recommendation models. Another line of research involves leveraging LLMs as feature extractors. These methods [1, 14, 15, 22, 22, 37, 38, 52] generate intermediate decision results or semantic embeddings of users and items, which are then input into traditional recommendation models to produce the final recommendations. Unlike existing methods, our approach aims to leverage the extensive knowledge and reasoning capabilities of LLMs to understand KGs and transform them into semantic embeddings, thereby addressing existing issues in KG-based recommender systems. 3 Preliminaries User-Item Interaction Graph. Let U and V denote the user set and item set, respectively, in a recommender system. We construct a user-item bipartite graph G = {(𝑢, 𝑦𝑢𝑣 , 𝑣)|𝑢 ∈ U, 𝑣 ∈ V} to represent the collaborative signals between users and items. Here, 𝑦𝑢𝑣 = 1 if user 𝑢 interacted with item 𝑣, and vice versa. Knowledge Graph. We capture real-world knowledge about items using a heterogeneous graph composed of triplets, represented as G𝑘 = {(ℎ, 𝑟, 𝑡)}. In this context, ℎ and 𝑡 are knowledge entities belonging to the set E, while 𝑟 is a relation from the set R that links them, as exemplified by the triplet (James Cameron, directed, Titanic). Notably, the item set is a subset of the entity set, denoted as V ⊂ E. This form of knowledge graph enables us to model the intricate relationships between items and entities. Task Formulation. Following the task format of most KG-aware recommendation models, we formulate the task as follows: Given the user-item interaction graph G and the corresponding knowledge graph G𝑘 , our objective is to learn a recommendation model that predicts the probability of user 𝑢 interacting with item 𝑣. 4 Methodology In this section, we introduce our proposed method CoLaKG in detail. An overview of our method is illustrated in Figure 2. CoLaKG extracts useful information from the KG at both local and SIGIR ’25, July 13–18, 2025, Padua, Italy Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma Assume you are an expert in movie recommendation.',\n",
              " 'You will be given a certain movie with its first-order information (in the form of triples) and some second-order relationships (movies related to this movie). Please complete the missing knowledge, summarize the movie and analyze what kind of users would like it. First-order info: (Titanic, Directed by, James Cameron), (Titanic, Acted by, Leonardo DiCaprio), … Second-order info: The movies with the same director (James Cameron) also include: Aliens, … The movies with the same actor (Leonardo DiCaprio) also include: Inception, … … Figure 3: The prompt template for Local KG comprehension. global levels. By employing item-centered subgraph extraction and prompt engineering, it accurately captures the local KG. Subsequently, through retrieval-based neighbor enhancement, it supplements the current item by capturing related items from the entire KG, thereby effectively utilizing global information. The local and global information extracted by the LLM are effectively integrated into the recommendation model through a representation fusion module and a retrieval-augmented representation learning module, respectively, thereby improving recommendation performance. 4.1 KG Comprehension with LLMs Knowledge graphs have been widely utilized in recommender systems to provide semantic information and model latent associations between items. However, KGs are predominantly manually curated, leading to missing facts and limited knowledge scopes. Additionally, the highly structured nature of KGs poses challenges for utilizing textual information. To address these limitations, we propose the use of LLMs to enhance the understanding and refinement of KGs for improved recommendations. To fully leverage KGs, our approach comprises two components: 1) Local KG Comprehension, which involves extracting item-centered KG subgraphs for analysis by LLMs, and 2) Retrieval-based Global KG Utilization, which utilizes the semantic similarity of KG subgraphs to retrieve semantically related items from the entire KG, thereby leveraging global KG information. 4.1.1 Local KG Comprehension. In this part, we introduce using LLMs to comprehend local KG information. Here, the local KG information of an item refers to the KG subgraph centered on that item, including connections within two hops. First, we represent the first-order KG subgraph centered on each item (i.e., ego network [21]) using triples. Specifically, given an item 𝑣 ∈ V, we use T𝑣 = {(𝑣, 𝑟, 𝑒)|(𝑣, 𝑟, 𝑒) ∈ G𝑘 } to denote the set of triplets where 𝑣 is the head entity. In the context of recommendation, the first-order neighboring entities of an item in a KG are usually attributes. Therefore, we use 𝑒 to represent these attribute entities to distinguish them from those item entities 𝑣. During generating triples, in cases where the attribute or relation is absent, the term “missing” is employed as a placeholder.',\n",
              " 'Next, we consider the second-order relations in KGs. The number of entities in an ego network centered on a single entity increases exponentially with the growth of the radius. However, the input length of an LLM is strictly limited. Consequently, including all second-order neighbors associated with the central item in the prompt becomes impractical. To address this issue, we adopt a simple but effective strategy, random sampling, to explore second-order connections of 𝑣. Let E𝑣 = {𝑒 | (𝑣, 𝑟, 𝑒) ∈ T𝑣 } denote the set of first-order connected neighbors of 𝑣. For each 𝑒 ∈ E𝑣 , we randomly sample 𝑚 triples from the set T𝑒 to construct the triples of second-order connections, denoted as T𝑒𝑚 . Here, T𝑒 = {(𝑒, 𝑟, 𝑣 ′ ) | (𝑒, 𝑟, 𝑣 ′ ) ∈ G𝑘 , 𝑣 ′ ≠ 𝑣 } represents the set of triples where 𝑒 ∈ E𝑣 is the head entity. After converting first-order and second-order relationships into triples, we transform these triples into textual form. For first-order relations, we concatenate all the first-order triples in T𝑣 to form a single text, denoted as D𝑣 . For second-order relations, we use a template to transform the second-order triples T𝑒𝑚 into coherent sentences D𝑣′ , facilitating the understanding of the LLM. In addition to D𝑣 and D𝑣′ , we have carefully designed a system prompt I𝑣 as the instruction to guide the generation. By combining I𝑣 , D𝑣 , and D𝑣′ , we obtain the prompt, which is shown in Figure 3. The prompt enables the LLM to fully understand, complete, and refine the KG, thereby generating the final comprehension for the 𝑣-centered KG subgraph. This process can be formulated as follows: C𝑣 = LLMs(I𝑣 , D𝑣 , D𝑣′ ). (1) Once we have obtained the LLM’s comprehension of the KG subgraphs, we need to convert these textual answers into continuous vectors for utilization in downstream recommendation models. Here, we employ a pre-trained text embedding model P to transform C𝑣 into embedding vectors s𝑣 , which can be formulated as: s𝑣 = P (C𝑣 ). (2) 4.1.2 Rerieval-based Global KG Utilization. This section introduces the utilization of global KG information. In recommendation scenarios, items that are distant in the KG can still have close semantic associations.',\n",
              " 'However, the number of KG nodes increases exponentially with the number of hops, and the input length of LLMs is limited by the number of tokens. This makes it impractical to input the entire KG into an LLM directly. To address this challenge, we propose a method called retrieval-based global KG utilization. For each item, we have obtained the semantic embedding corresponding to its local KG. Based on this, we can directly compute the semantic relationships between any two item-centered KG subgraphs. specifically, we employ the cosine similarity as a metric to quantify the relations. Given two different items 𝑣𝑖 and 𝑣 𝑗 , their semantical relation 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) in the global KG is computed as: 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) = sim(s𝑣𝑖 , s𝑣 𝑗 ), 𝑣𝑖 , 𝑣 𝑗 ∈ V and 𝑣𝑖 ≠ 𝑣 𝑗 , (3) where sim denotes the cosine similarity function. Once we obtain the semantic associations between any two items in the entire KG, we treat the semantic similarity between the two items as the edge weight between them, allowing us to construct an item-item graph: G𝑣 = {(𝑣𝑖 , 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) , 𝑣 𝑗 )|𝑣𝑖 , 𝑣 𝑗 ∈ V, 𝑣𝑖 ≠ 𝑣 𝑗 }. (4) From a high-level perspective, we transform high-order associations between items in the KG into direct semantic connections on the constructed item-item graph G𝑣 . Based on this foundation, it is essential to retrieve items that are semantically strongly related to the given item, as items with lower semantic relevance may introduce noise. Specifically, given an item 𝑣𝑖 , we rank all other items 𝑣 𝑗 ∈ V where 𝑣 𝑗 ≠ 𝑣𝑖 in descending order based on the semantic similarity 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) . Subsequently, we retrieve the top-𝑘 items with the highest similarity scores, forming the neighbor set of 𝑣𝑖 : N𝑘 (𝑣𝑖 ), where 0 < 𝑘 < |V | is an adjustable hyperparameter, representing the number of retrieved neighbors. In this manner, we filter out items with low semantic associations to the current item in the entire KG. Traditional KG-based recommendation methods aggregate items through layer-by-layer information propagation on the KG. High-order relations require many propagation and aggregation steps to be captured. In contrast, our retrieval-based method can directly recall strongly semantically related neighbors of any order across the entire KG. The retrieved neighbors are then leveraged to enhance item representations, which will be introduced in Section 4.3.2. 4.2 User Preference Comprehension The introduction of KGs allows for the expansion of user-item bipartite graphs and enables us to understand user preferences from a knowledge-driven perspective. Given a user 𝑢, we first extract the subgraph corresponding to user 𝑢 from the user-item bipartite graph, denoted as B𝑢 . For each item 𝑣 ∈ B𝑢 , we extract its firstorder KG subgraph and represent it as a set of triples, denoted as T𝑣 .',\n",
              " 'We then concatenate all triples in T𝑣 to form a single text, denoted as D𝑣 . The detailed approach is the same as described in Section 4.1.1. Subsequently, we represent user 𝑢 with all items the user has interacted with in the training set and the corresponding knowledge triples D𝑣 : D𝑢 = ⊕𝑣 ∈ G𝑢 {name𝑣 : D𝑣 }, (5) where ⊕ denotes concatenation operation, and name𝑣 denotes the text name of item 𝑣. Additionally, we have meticulously designed a system prompt, denoted as I𝑢 , to serve as an instruction for guiding the generation of user preferences. By combining D𝑢 and I𝑢 , we enable the LLM to comprehend the user preference for 𝑢, which can be formulated as: C𝑢 = LLMs(I𝑢 , D𝑢 ). (6) Furthermore, we also utilize the text embedding function P to transform the textual answers C𝑢 into embedding vectors s𝑢 , which can be expressed as: s𝑢 = P (C𝑢 ). (7) 4.3 Retrieval-Augmented Representation 4.3.1 Cross-Modal Representation Alignment. In a traditional recommendation model, each item and user is associated with an ID embedding. Let e𝑣 ∈ R𝑑 represent the ID embedding of item 𝑣 and e𝑢 ∈ R𝑑 represent the ID embedding of user 𝑢. In addition, we also obtain the semantic embedding s𝑣 ∈ R𝑑𝑠 w.r.t. the comprehension of 𝑣-centric KG subgraph, and the semantic embedding s𝑢 ∈ R𝑑𝑠 w.r.t. the comprehension of user 𝑢’s preference. Since ID embeddings and semantic embeddings belong to two different modalities and typically possess different embedding dimensions, we employ a learnable adapter network to align the embedding spaces. Specifically, the adapter consists of a linear map and a nonlinear activation function, formulated as: s′𝑣 = 𝜎 (W1 s𝑣 ); s𝑢′ = 𝜎 (W2 s𝑢 ), (8) where both W1 ∈ R𝑑 ×𝑑𝑠 and W2 ∈ R𝑑 ×𝑑𝑠 are are weight matrices, 𝜎 represents the non-linear activation function ELU [5]. Note that during the training process, we fix s𝑣 and s𝑢 , training solely the projection parameters W1 and W2 , and the parameters of the recommendation model. After mapping the representations to the same space, we need to fuse the representations of the two modalities, leveraging both the collaborative signals and the semantic SIGIR ’25, July 13–18, 2025, Padua, Italy information to form a complementary representation. To achieve this, we employ a straightforward mean pooling technique to fuse their embeddings: h𝑣 = 1 (e𝑣 + s′𝑣 ); 2 h𝑢 = 1 (e𝑢 + s𝑢′ ), 2 (9) where h𝑣 ∈ R𝑑 and h𝑢 ∈ R𝑑 represent the merged embeddings of item 𝑣 and user 𝑢, respectively. 4.3.2 Item Representation Augmentation with Retrieved Neighbors. For each item, we have retrieved its semantic-related items in Section 4.1.2. To fully utilize these neighbors, we propose to aggregate their information to enhance item representations.',\n",
              " 'Considering the varying contributions of different neighbors to the central item, we employ the attention mechanism. Specifically, for item 𝑣𝑖 and its top-𝑘 neighbor set N𝑘 (𝑣𝑖 ), we compute attention coefficients that indicate the importance of item 𝑣 𝑗 ∈ N𝑘 (𝑣𝑖 ) to item 𝑣𝑖 as follows: 𝑤𝑖 𝑗 = 𝑎(Ws𝑣𝑖 ∥Ws𝑣 𝑗 ). (10) Here, W ∈ R𝑑𝑎 ×𝑑 is a learnable weight matrix to capture higherlevel features of s𝑣𝑖 and s𝑣 𝑗 , ∥ is the concatenation operation, 𝑎 denotes the attention function: R𝑑𝑎 × R𝑑𝑎 → R, where we adopt a single-layer neural network and apply the LeakyReLU activation function following [27]. Note that the computation of attention weights is exclusively dependent on the semantic representation of items, as our objective is to calculate the semantic associations between items, rather than the associations present in collaborative signals. In addition, we employ the softmax function for easy comparison of coefficients across different items: 𝛼𝑖 𝑗 = softmax 𝑗 (𝑤𝑖 𝑗 ). (11) The attention scores 𝛼𝑖 𝑗 are then utilized to compute a linear combination of the corresponding neighbor embeddings. Finally, the weighted average of neighbor embeddings and the embedding of item 𝑣𝑖 itself are combined to form the final output representation for item 𝑣𝑖 : \\x12 \\x10 \\x11\\x13 ∑︁ 1 h𝑣𝑖 + h′𝑣𝑖 = 𝜎 𝛼 𝑖 𝑗 h𝑣 𝑗 , (12) 𝑗 ∈ N𝑘 (𝑣𝑖 ) 2 where 𝜎 denotes the non-linear activation function. 4.4 User-Item Modeling Having successfully integrated the semantic information from the KG into both user and item representations, we can use them as inputs for traditional recommendation models to generate prediction results. This process can be formulated as follows: 𝑦ˆ𝑢𝑣 = F (h𝑢 , h′𝑣 ), (13) where 𝑦ˆ𝑢𝑣 is the predicted probability of user 𝑢 interacting with item 𝑣, h𝑢 is the representation for user 𝑢, h′𝑣 is the augmented representation for item 𝑣, and F denotes the function of the recommendation model. Specifically, we select the classic model, LightGCN [10], as the architecture for our recommendation method due to its simplicity and effectiveness. The trainable parameters of original LightGCN are only the embeddings of users and items, similar to standard SIGIR ’25, July 13–18, 2025, Padua, Italy Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma Table 1: Dataset statistics. Statistics MovieLens Last-FM MIND Funds # Users # Items # Interactions 6,040 3,260 998,539 1,859 2,813 86,608 44,603 15,174 1,285,064 209,999 5,701 1,225,318 # Entities # Relations # Triples 12,068 12 62,958 32,810 14 307,140 8,111 12 65,697 5 Experiments 5.1 Experimental Settings Knowledge Graph 9,614 2 118,500 matrix factorization. First, we adopt the simple weighted sum aggregator to learn the user-item interaction graph, which is defined as: (𝑙+1) h𝑢 = 1 ∑︁ √︁ 𝑣 ∈ M𝑢 (𝑙 ) |M𝑢 ||M 𝑣 | (𝑙+1) h𝑣 ; h𝑣 = 1 ∑︁ √︁ 𝑢 ∈ M𝑣 |M 𝑣 ||M𝑢 | (𝑙 ) h𝑢 , (14) (𝑙 ) (𝑙 ) where h𝑢 and h𝑣 represent the embeddings of user 𝑢 and item 𝑣 after 𝑙 layers of propagation, respectively. The initial embeddings (0) (0) h𝑢 = h𝑢 and h𝑣 = h′𝑣 are obtained in Section 4.3. M𝑢 denotes the set of items with which user 𝑢 has interacted, while M 𝑣 signifies the set of users who have interacted √︁ with item 𝑣. The symmetric normalization term is given by 1/ |M𝑢 ||M 𝑣 |. Subsequently, the embeddings acquired at each layer are combined to construct the final representation: h̃𝑢 = 𝐿 𝐿 𝑙=0 𝑙=0 1 ∑︁ (𝑙 ) 1 ∑︁ ′ (𝑙 ) h𝑢 ; h̃𝑣 = h𝑣 , 𝐿+1 𝐿+1 (15) where 𝐿 represents the number of hidden layers. Ultimately, the model prediction is determined by the inner product of the final user and item representations: 𝑦ˆ𝑢𝑣 = h̃𝑢⊤ h̃𝑣 . 4.5 (16) Model Training Our approach can be divided into two stages. In the first stage, we employ the LLM to comprehend the KGs, generating corresponding semantic embeddings for each item and user, denoted as s𝑣 and s𝑢 , respectively.',\n",
              " 'In the second stage, these semantic embeddings are integrated into the recommendation model to enhance its performance. Only the second stage necessitates supervised training, where we adopt the widely-used Bayesian Personalized Ranking (BPR) loss: L= ∑︁ −ln𝜎 (𝑦ˆ𝑢𝑣 + − 𝑦ˆ𝑢𝑣 − ) + 𝜆∥Θ∥ 22 . (17) (𝑢,𝑣 + ,𝑣 − ) ∈ O Here, O = {(𝑢, 𝑣 +, 𝑣 − )|(𝑢, 𝑣 + ) ∈ R +, (𝑢, 𝑣 − ) ∈ R − } represents the training set, R + denotes the observed (positive) interactions between user 𝑢 and item 𝑣, while R − indicates the sampled unobserved (negative) interaction set. 𝜎 (·) is the sigmoid function. 𝜆∥Θ∥ 22 is the regularization term, where 𝜆 serves as the weight coefficient and Θ constitutes the model parameter set. 5.1.1 Datasets. We conducted experiments on four real-world datasets, including three public datasets (MovieLens1 , MIND2 , LastFM3 ), and one industrial dataset (Fund). The statistics for these datasets are presented in Table 1. These datasets cover a wide range of application scenarios. Specifically, MovieLens is a wellestablished benchmark that collects movie ratings provided by users. MIND is a large-scale news recommendation dataset constructed from user click logs on Microsoft News. Last-FM is a well-known music recommendation dataset that includes user listening history and artist tags. The Fund dataset is sampled from the data of a large-scale online financial platform aiming to recommend funds for users. We adopt the similar setting as numerous previous studies [10, 42], filtering out items and users with fewer than five interaction records. For each dataset, we randomly select 80% of each user’s historical interactions to form the training set, while the remaining 20% constitute the test set, following [10]. Each observed user-item interaction is considered a positive instance, and we apply a negative sampling strategy by pairing it with one negative item that the user has not interacted with. 5.1.2 Evaluation Metrics. To evaluate the performance of the models, we employ widely recognized evaluation metrics: Recall and Normalized Discounted Cumulative Gain (NDCG), and report values of Recall@k and NDCG@k for k=10 and 20, following [10, 34]. To ensure unbiased evaluation, we adopt the all-ranking protocol. All items that are not interacted by a user are the candidates.',\n",
              " '5.1.3 Baseline Methods. To ensure a comprehensive assessment, we compare our method with 12 baseline methods, which can be divided into three categories: classical methods (BPR-MF, NFM, LightGCN), KG-enhanced methods (CKE, RippleNet, KGAT, KGIN, KGCL, KGRec), and LLM-based methods (RLMRec, KAR, CLLM4Rec). BPR-MF [23] employs matrix factorization to model users and items, and uses the pairwise Bayesian Personalized Ranking (BPR) loss to optimize the model. NFM [9] is an advanced factorization model that subsumes FM [24] under neural networks. LightGCN [10] facilitates message propagation between users and items by simplifying GCN [16]. CKE [46] is an embedding-based method that uses TransR to guide entity representation in KGs to enhance performance. RippleNet [28] automatically discovers users’ hierarchical interests by iteratively propagating users’ preferences in the KG. KGAT [34] designs an attentive message passing scheme over the knowledge-aware collaborative graph for node embedding fusion. KGIN [35] adopts an adaptive aggregation method to capture finegrained user intentions. KGCL [44] uses contrastive learning for knowledge graphs to reduce potential noise and guide user preference learning. 1 https://grouplens.org/datasets/movielens/ 2 https://msnews.github.io/ 3 https://grouplens.org/datasets/hetrec-2011/ SIGIR ’25, July 13–18, 2025, Padua, Italy Table 2: Performance comparison of different methods, where R denotes Recall and N denotes NDCG. The best results are bolded, and the second best results are underlined. The results show our improvement is statistically significant with a significance level of 0.01. MovieLens Model Last-FM MIND Funds R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 BPR-MF 0.1257 0.3100 0.2048 0.3062 0.1307 0.1352 0.1971 0.1685 0.0315 0.0238 0.0537 0.0310 0.4514 0.3402 0.5806 0.3809 NFM 0.1346 0.3558 0.2129 0.3379 0.2246 0.2327 0.3273 0.2830 0.0495 0.0356 0.0802 0.0458 0.4388 0.3187 0.5756 0.3651 LightGCN 0.1598 0.3901 0.2512 0.3769 0.2589 0.2799 0.3642 0.3321 0.0624 0.0492 0.0998 0.0609 0.4992 0.3778 0.6353 0.4204 CKE 0.1524 0.3783 0.2373 0.3609 0.2342 0.2545 0.3266 0.3001 0.0526 0.0417 0.0822 0.0510 0.4926 0.3702 0.6294 0.4130 RippleNet 0.1415 0.3669 0.2201 0.3423 0.2267 0.2341 0.3248 0.2861 0.0472 0.0364 0.0785 0.0451 0.4764 0.3591 0.6124 0.4003 KGAT 0.1536 0.3782 0.2451 0.3661 0.2470 0.2595 0.3433 0.3075 0.0594 0.0456 0.0955 0.0571 0.5037 0.3751 0.6418 0.4182 KGIN 0.1631 0.3959 0.2562 0.3831 0.2562 0.2742 0.3611 0.3215 0.0640 0.0518 0.1022 0.0639 0.5079 0.3857 0.6428 0.4259 KGCL 0.1554 0.3797 0.2465 0.3677 0.2599 0.2763 0.3652 0.3284 0.0671 0.0543 0.1059 0.0670 0.5071 0.3877 0.6355 0.4273 KGRec 0.1640 0.3968 0.2571 0.3842 0.2571 0.2748 0.3617 0.3251 0.0627 0.0506 0.1003 0.0625 0.5104 0.3913 0.6467 0.4304 RLMRec 0.1613 0.3920 0.2524 0.3787 0.2597 0.2812 0.3651 0.3335 0.0619 0.0486 0.0990 0.0602 0.4988 0.3784 0.6351 0.4210 KAR 0.1582 0.3869 0.2511 0.3722 0.2532 0.2770 0.3612 0.3324 0.0615 0.0480 0.1002 0.0613 0.5033 0.3812 0.6312 0.4175 CLLM4Rec 0.1563 0.3841 0.2433 0.3637 0.2571 0.2793 0.3642 0.3268 0.0631 0.0494 0.1012 0.0628 0.4996 0.3791 0.6273 0.4103 CoLaKG 0.1699 0.4130 0.2642 0.3974 0.2738 0.2948 0.3803 0.3471 0.0698 0.0562 0.1087 0.0684 0.5273 0.4012 0.6524 0.4392 Table 3: Validation of the generalizability of our method: Experimental results of integrating CoLaKG with various recommendation backbones. Model MovieLens Last-FM MIND R@20 N@20 R@20 N@20 R@20 N@20 BPR-MF BPR-MF+Ours 0.2048 0.3062 0.1971 0.1685 0.0537 0.0310 0.2213 0.3255 0.2104 0.1812 0.0609 0.3986 NFM NFM+Ours 0.2129 0.3379 0.3273 0.2830 0.0802 0.0458 0.2285 0.3527 0.3478 0.2996 0.0859 0.0487 LightGCN 0.2512 0.3769 0.3642 0.3321 0.0998 0.0609 LightGCN+Ours 0.2642 0.3974 0.3803 0.3471 0.1087 0.0684 KGRec [43] is a state-of-the-art KG-based recommendation model which devises a self-supervised rationalization method to identify informative knowledge connections. RLMRec [22] is an LLM-based model. It directly utilizes LLMs to generate text profiles and combine them with recommendation models through contrastive learning. Since their method is modelagnostic, to ensure a fair comparison, we chose LightGCN as its backbone model, consistent with our method. KAR [41] is an LLM-based model, which utilizes LLMs to enhance recommender systems with open-world knowledge CLLM4Rec [52] is also an LLM-based method. It combines ID information and semantic information and uses the LLM directly as the recommender to generate the recommendation results.',\n",
              " '5.1.4 Implementation Details. We implement all baseline methods according to their released code. The embedding size 𝑑 for all recommendation methods is set to 64 for a fair comparison. All experiments are conducted with a single V100 GPU. We set the batch size to 1024 for the Last-FM dataset and 4096 for the other datasets to expedite training. The Dropout rate is chosen from the set {0.2, 0.4, 0.6, 0.8} for both the embedding layer and the hidden layers. We employ the Adam optimizer with a learning rate of 0.001. The maximum number of epochs is set to 2000. The number of hidden layers for the recommendation model 𝐿 is set to 3. For the LLM, we select DeepSeek-V2, a robust large language model that demonstrates exceptional performance on both standard benchmarks and open-ended generation evaluations. For more detailed information about DeepSeek, please refer to their official website4 . Specifically, we utilize DeepSeek-V2 by invoking its API5 . To reduce text randomness of the LLM, we set the temperature 𝜏 to 0 and the top-𝑝 to 0.001. For the text embedding model P, we use the pre-trained sup-simcse-roberta-large6 [7]. We use identical settings for the baselines that also involve LLMs and text embeddings to ensure fairness in comparison. 5.2 Comparison Results We compare 12 baseline methods across four datasets and run each experiment five times. The average results are reported in Table 2. Based on the results, we make the following observations: • Our method consistently outperforms all the baseline models across all four datasets. The performance ceiling of traditional methods (BPR-MF, NFM, LightGCN) is generally lower than that of KG-based methods, as the former rely solely on collaborative signals without incorporating semantic knowledge. However, some KG-based methods do not perform as well as LightGCN, indicating that effectively leveraging KG is a challenging task.',\n",
              " '• Among the KG-based baselines, KGCL and KGRec are notable for incorporating self-supervised learning into general KG-based recommendation frameworks. However, they struggle with missing facts, understanding semantic information, and modeling higherorder item associations within the KG. In contrast, our method leverages LLMs to address these challenges without requiring 4 https://github.com/deepseek-ai/DeepSeek-V2 5 https://api-docs.deepseek.com/ 6 https://huggingface.co/princeton-nlp/sup-simcse-roberta-large SIGIR ’25, July 13–18, 2025, Padua, Italy Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma Table 4: Ablation study on all four datasets. Metric w/o s𝑣 w/o s𝑢 w/o N𝑘 (𝑣) w/o D𝑣′ CoLaKG ML R@20 0.2553 0.2613 N@20 0.3811 0.3948 0.2603 0.3902 0.2628 0.3960 0.2642 0.3974 Last-FM R@20 0.3628 0.3785 N@20 0.3278 0.3465 0.3725 0.3403 0.3789 0.3459 0.3803 0.3471 MIND R@20 0.1043 0.1048 N@20 0.0640 0.0658 0.1064 0.0662 0.1076 0.0671 0.1087 0.0684 Funds R@20 0.6382 0.6481 N@20 0.4247 0.4351 0.6455 0.4305 0.6499 0.4378 0.6524 0.4392 self-supervised tasks, leading to significant improvements across all datasets and metrics. • For LLM-based recommendation methods, we have selected several representative approaches: RLMRec, KAR, and CLLM4Rec. It is evident that these methods exhibit only marginal improvements over traditional techniques. In contrast, our method demonstrates a substantial performance enhancement compared to these LLM-based baselines, thereby further validating the superiority of our approach. Our method effectively leverages LLMs to comprehend both the local subgraphs and global relationships within knowledge graphs, resulting in significant performance improvements. 5.3 Validation of the Generalizability In this section, we validate the versatility of CoLaKG. Specifically, we integrate our method into three different classical recommendation model backbones and observe the performance improvements of these models across three public datasets. The results, as shown in Table 3, clearly indicate significant performance improvements when our method is combined with various recommendation backbones. This experiment demonstrates that our approach, which leverages LLMs to understand KGs and enhance recommendation models, can be flexibly applied to different recommendation models to improve their performance. 5.4 Ablation Study In this section, we demonstrate the effectiveness of our model by comparing its performance with four different versions across all four datasets. The results are shown in Table 4, where “w/o s𝑣 ” denotes removing the semantic embeddings of items, “w/o s𝑢 ” denotes removing the semantic embeddings of users, “w/o N𝑘 (𝑣)” means removing the neighbor augmentation of items based on the constructed item-item graph, and “w/o D𝑣′ ” means removing the second-order triples from the LLM’s prompts. When the semantic embeddings of items are removed, the model’s performance significantly decreases across all datasets, underscoring the critical role of semantic information captured by LLMs from the KG. Similarly, the removal of user semantic embeddings also results in a performance decline, affirming that LLMs can effectively infer user preferences from the KG. Furthermore, removing N𝑘 (𝑣) leads to a performance drop across all datasets, highlighting the significance of the item representation augmentation module based on the constructed semantic-relational item-item graph. Without this module, the model can only capture local KG information from Figure 4: Hyperparameter study of the number of retrieved neighbors (𝑘) and sampled number of 2-hop items within the prompt (𝑚) on four datasets. item-centered subgraphs and cannot leverage the semantic relations present in the global KG. The inclusion of this module facilitates the effective integration of both local and global KG information.',\n",
              " 'Lastly, removing second-order KG triples from the prompts causes a slight performance decline. This finding suggests that incorporating second-order information from the KG allows the LLMs to produce a higher-quality comprehension of the local KG. 5.5 Hyperparameter Study In this section, we investigate the impact of the hyperparameter 𝑘 and 𝑚 across four datasets. Here, 𝑘 represents the number of semantically related neighbors, as defined in Section 4.1.2, 𝑚 is the number of second-order neighbors used in the prompt, which is defined in Section 4.1.1. The results are presented in Figure 4. We observe that as 𝑘 increases, both Recall@20 and NDCG@20 initially rise and then slightly decline across all datasets. The performance is worst when 𝑘 = 0 and best when 𝑘 is between 10 and 30. When 𝑘 = 0, no neighbors are used, which is equivalent to the ablation study without N𝑘 (𝑣), thereby not incorporating any global semantic associations from the KG. When 𝑘 > 0, the introduction of semantically related items enhances the item’s representations, leading to a noticeable improvement. However, as 𝑘 continues to increase, some noise may be introduced because the relevance of neighbors decreases with their ranking. Consequently, items with lower relevance may interfere with the recommendation performance. Our findings suggest that a range of 10-30 neighbors is optimal. As the value of 𝑚 increases, the metrics initially rise and then slightly decline. When 𝑚 = 0, the prompt used to understand the KG subgraph includes only first-order neighbors, resulting in the poorest performance. This indicates the positive impact of incorporating second-order neighbors. However, as 𝑚 continues to grow, the marginal benefits diminish, and additional noise may be introduced, leading to a slight decrease in performance. 5.6 Robustness to Varying Degrees of Sparsity One of the key functions of KGs is to alleviate the issue of data sparsity. To further examine the robustness of our model against users with varying levels of activity, particularly its performance with less active users, we sort users based on their interaction SIGIR ’25, July 13–18, 2025, Padua, Italy Disconnected or Distant Paths to “Apollo 13” in original KG Case 1 Connected to “Apollo 13” by shared genres in original KG Apollo 13 The Right Stuff Birdy The Great Escape Top Gun Star Trek: First Contact Neighbors of “Apollo 13” Connected to “A Little Princess” by shared genres in original KG Disconnected or Distant Paths to “A Little Princess” in original KG A Simple Wish Quest for Camelot Now and Then The Story of Cinderella The Princess Bride Case 2 Figure 5: Performance comparison on different user groups, where a smaller group ID indicates fewer interaction records. A Little Princess Neighbors of “A Little Princess” Figure 7: Case study. Figure 6: Performance comparison using complete KG and incomplete KG on two datasets.',\n",
              " 'Models with the \"-r\" suffix indicate their performance in the presence of missing facts. frequency and divide them into four equal groups. A lower group ID indicates lower user activity (01 being the lowest, 04 the highest). We analyze the evaluation results on two relatively sparse datasets, Last-FM and MIND, as shown in Figure 5. By comparing our model with three representative and strong baseline models, we observe that our model consistently outperforms the baselines in each user group. Notably, the improvement ratio of our model in the sparser groups (01 and 02) is higher compared to the denser groups (03 and 04). For the group with the most limited data (Group 01), our model achieves the most significant lead. This indicates that the average improvement of our model is primarily driven by enhancements in the sparser groups, demonstrating the positive impact of CoLaKG in addressing data sparsity. 5.7 Robustness to Missing Facts To further demonstrate the robustness of our proposed ColaKG in scenarios where facing the challenge of missing facts in KG, we conducted comparative experiments on the Movielens and Last-FM datasets. Specifically, we randomly dropped 30% of the KG entities and relations to construct datasets with significant missing facts based on the original datasets. The comparison results of our method with representative KG-based baselines on the constructed datasets are shown in Figure 6. From the experimental results, we can see that removing KG facts has a negative impact on all KG-based methods. However, in the case of an incomplete KG, our method, CoLaKG, still outperforms the other baselines. Furthermore, by comparing the performance degradation of different methods on incomplete and complete KGs, we find that our method experiences the smallest proportion of performance decline. This demonstrates that our method can effectively mitigate the impact of missing facts on performance, further proving the robustness and superiority of the proposed approach. 5.8 Case Study In this section, we conduct an in-depth analysis of the rationality of our method through two real cases. In the first case, we present the movie “Apollo 13” and its five semantically related neighbor items in the item-item graph identified by our method. The first three movies belong to the same genre as “Apollo 13”, making them 2-hop neighbors in the KG. In contrast, the other two movies, “Top Gun” and “Star Trek”, do not share any genre or other attributes with “Apollo 13”, indicating they are distant or unconnected in the KG. However, “Top Gun” and “Star Trek” are semantically related to “Apollo 13” as they all highlight themes of human resilience, courage, and the spirit of adventure.',\n",
              " 'Traditional KG-based recommendation methods, which rely on layer-by-layer information propagation, struggle to capture such high-order neighbors. In contrast, our method leverages similarity calculations based on item-centered KG semantic embeddings, successfully identifying these two strongly related movies. This demonstrates that our approach can effectively and efficiently capture semantically relevant information from the global KG. In the second case, we examine the movie “A Little Princess” and its related neighbors. Among the five related movies identified, “The Story of Cinderella” and “The Princess Bride” should share the same genre as “A Little Princess”. However, due to missing genres in the KG, these movies lack a path to “A Little Princess” within the KG. Despite this, our method successfully identifies these two movies. This demonstrates that our approach, by leveraging LLMs to complete and interpret the KG, can effectively address challenges posed by missing key attributes. 6 Conclusion In this paper, we analyze the limitations of existing KG-based recommendation methods and propose a novel approach, CoLaKG, to address these issues. CoLaKG comprehends item-centered KG subgraphs to obtain semantic embeddings for both items and users. These semantic embeddings are then used to construct a semantic relational item-item graph, effectively leveraging global KG information. We conducted extensive experiments on four datasets to validate the effectiveness and robustness of our method. The results demonstrate that our approach significantly enhances the performance of recommendation models. 7 Acknowledgments This work was supported by the Early Career Scheme (No. CityU 21219323) and the General Research Fund (No. CityU 11220324) of the University Grants Committee (UGC), and the NSFC Young Scientists Fund (No. 9240127). SIGIR ’25, July 13–18, 2025, Padua, Italy Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma References [1] Arkadeep Acharya, Brijraj Singh, and Naoyuki Onoe. 2023. Llm based generation of item-description for recommendation system.',\n",
              " 'In Proceedings of the 17th ACM Conference on Recommender Systems. 1204–1207. [2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. 1007–1014. [3] Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019. Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences. In The world wide web conference. 151–161. [4] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, et al. 2023. When large language models meet personalization: Perspectives of challenges and opportunities. arXiv preprint arXiv:2307.16376 (2023). [5] Djork-Arné Clevert. 2015. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289 (2015).',\n",
              " '[6] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023). [7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Empirical Methods in Natural Language Processing (EMNLP). [8] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. 2020. A survey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge and Data Engineering 34, 8 (2020), 3549–3568. [9] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. 355–364. [10] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation.',\n",
              " 'In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639–648. [11] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large language models as zero-shot conversational recommenders. In Proceedings of the 32nd ACM international conference on information and knowledge management. 720–730. [12] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2024. Large language models are zero-shot rankers for recommender systems. In European Conference on Information Retrieval. Springer, 364–381. [13] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. 2018. Leveraging metapath based context for top-n recommendation with a neural co-attention model. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 1531–1540. [14] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, and Jun Zhou. 2024. Enhancing sequential recommendation via llm-based semantic embedding learning.',\n",
              " 'In Companion Proceedings of the ACM on Web Conference 2024. 103–111. [15] Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and Chanyoung Park. 2024. Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1395–1406. [16] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [17] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023. Large language models for generative recommendation: A survey and visionary discussions. arXiv preprint arXiv:2309.01157 (2023). [18] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation. In Proceedings of the ACM on Web Conference 2024. 3497–3508.',\n",
              " '[19] Xiaolin Lin, Jinwei Luo, Junwei Pan, Weike Pan, Zhong Ming, Xun Liu, Shudong Huang, and Jie Jiang. 2024. Multi-sequence attentive user representation learning for side-information integrated sequential recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining. 414–423. [20] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the AAAI conference on artificial intelligence, Vol. 29. [21] Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. 2018. Deepinf: Social influence prediction with deep learning. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 2110–2119. [22] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Representation learning with large language models for recommendation. In Proceedings of the ACM on Web Conference 2024. 3464– 3475.',\n",
              " '[23] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [24] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2011. Fast context-aware recommendations with factorization machines. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. 635–644. [25] Yu Tian, Yuhao Yang, Xudong Ren, Pengfei Wang, Fangzhao Wu, Qian Wang, and Chenliang Li. 2021. Joint knowledge pruning and recurrent graph convolution for news recommendation. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 51–60. [26] Riku Togashi, Mayu Otani, and Shin’ichi Satoh. 2021. Alleviating cold-start problems in recommendation through pseudo-labelling over knowledge graph. In Proceedings of the 14th ACM international conference on web search and data mining. 931–939. [27] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.',\n",
              " '2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017). [28] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018. Ripplenet: Propagating user preferences on the knowledge graph for recommender systems. In Proceedings of the 27th ACM international conference on information and knowledge management. 417–426. [29] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep knowledge-aware network for news recommendation. In Proceedings of the 2018 world wide web conference. 1835–1844. [30] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge graph convolutional networks for recommender systems. In The world wide web conference. 3307–3313. [31] Lei Wang and Ee-Peng Lim. 2023.',\n",
              " 'Zero-shot next-item recommendation using large pretrained language models. arXiv preprint arXiv:2304.03153 (2023). [32] Maolin Wang, Dun Zeng, Zenglin Xu, Ruocheng Guo, and Xiangyu Zhao. 2023. Federated knowledge graph completion via latent embedding sharing and tensor factorization. In 2023 IEEE International Conference on Data Mining (ICDM). IEEE, 1361–1366. [33] Shuyao Wang, Yongduo Sui, Chao Wang, and Hui Xiong. 2024. Unleashing the Power of Knowledge Graph for Recommendation via Invariant Learning. In Proceedings of the ACM on Web Conference 2024. 3745–3755. [34] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat: Knowledge graph attention network for recommendation. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 950–958. [35] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat-Seng Chua. 2021. Learning intents behind interactions with knowledge graph for recommendation.',\n",
              " 'In Proceedings of the web conference 2021. 878–887. [36] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng Chua. 2019. Explainable reasoning over knowledge graphs for recommendation. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5329–5336. [37] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models with graph augmentation for recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining. 806–815. [38] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. 2024. Exploring large language model for graph data understanding in online job recommendations. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 9178–9186. [39] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al.',\n",
              " '2023. A survey on large language models for recommendation. arXiv preprint arXiv:2305.19860 (2023). [40] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2024. A survey on large language models for recommendation. World Wide Web 27, 5 (2024), 60. [41] Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. Towards open-world recommendation with knowledge augmentation from large language models. In Proceedings of the 18th ACM Conference on Recommender Systems. 12–22. [42] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In 2022 IEEE 38th international conference on data engineering (ICDE). IEEE, 1259– 1273. [43] Yuhao Yang, Chao Huang, Lianghao Xia, and Chunzhen Huang. 2023. Knowledge graph self-supervised rationalization for recommendation.',\n",
              " 'In Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining. 3046–3056. [44] Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. 2022. Knowledge graph contrastive learning for recommendation. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval. 1434–1443. [45] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation: A heterogeneous information network approach. In Proceedings of the 7th ACM international conference on Web search and data mining. 283–292. [46] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. 2016. Collaborative knowledge base embedding for recommender systems. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 353–362. [47] Haotian Zhang, Shuanghong Shen, Bihan Xu, Zhenya Huang, Jinze Wu, Jing Sha, and Shijin Wang. 2024. Item-Difficulty-Aware Learning Path Recommendation: From a Real Walking Perspective.',\n",
              " 'In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 4167–4178. [48] Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang, and Lihong Gu. 2024. Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation SIGIR ’25, July 13–18, 2025, Padua, Italy Systems through an Inferential Knowledge Graph. arXiv preprint arXiv:2402.13750 (2024). [49] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [50] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, et al. 2023. Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023). [51] Xinjun Zhu, Yuntao Du, Yuren Mao, Lu Chen, Yujia Hu, and Yunjun Gao. 2023. Knowledge-refined Denoising Network for Robust Recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 362–371. [52] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li.',\n",
              " '2024. Collaborative large language model for recommender systems. In Proceedings of the ACM on Web Conference 2024. 3162–3172.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71KVWpW2XzSy",
        "outputId": "1074fae2-5456-4344-baa6-d7a775ad01b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Text splitted to sentences.\n",
            "['Abstract In recent years, the introduction of knowledge graphs (KGs) has significantly advanced recommender systems by facilitating the discovery of potential associations between items.', 'However, existing methods still face several limitations.', 'First, most KGs suffer from missing facts or limited scopes.', 'Second, existing methods convert textual information in KGs into IDs, resulting in the loss of natural semantic connections between different items.', 'Third, existing methods struggle to capture high-order connections in the global KG.', 'To address these limitations, we propose a novel method called CoLaKG, which leverages large language models (LLMs) to improve KG-based recommendations.', 'The extensive knowledge and remarkable reasoning capabilities of LLMs enable our method to supplement missing facts in KGs, and their powerful text understanding abilities allow for better utilization of semantic information.', 'Specifically, CoLaKG extracts useful information from KGs at both local and global levels.', 'By employing the item-centered subgraph extraction and prompt engineering, it can accurately understand the local information.', 'In addition, through the semantic-based retrieval module, each item is enriched by related items from the entire knowledge graph, effectively harnessing global information.', 'Furthermore, the local and global information are effectively integrated into the recommendation model through a representation fusion module and a retrieval-augmented representation learning module, respectively.', 'Extensive experiments on four real-world datasets ∗ Work done during an internship at Tencent.', '† Corresponding author.', 'Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.', 'Copyrights for components of this work owned by others than the author(s) must be honored.', 'Abstracting with credit is permitted.', 'To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.', 'Request permissions from permissions@acm.org.', 'SIGIR ’25, Padua, Italy © 2025 Copyright held by the owner/author(s).', 'Publication rights licensed to ACM.']\n",
            "extensive experiments on four real world datasets ∗ work done during an internship at tencent.\n",
            " [!] Character '∗' not found in the vocabulary. Discarding it.\n",
            "† corresponding author.\n",
            " [!] Character '†' not found in the vocabulary. Discarding it.\n",
            "to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.\n",
            " [!] Character '/' not found in the vocabulary. Discarding it.\n",
            "request permissions from permissions@acm.org.\n",
            " [!] Character '@' not found in the vocabulary. Discarding it.\n",
            "sigir ’25, padua, italy © 2025 copyright held by the owner/authors.\n",
            " [!] Character '’' not found in the vocabulary. Discarding it.\n",
            "sigir ’25, padua, italy © 2025 copyright held by the owner/authors.\n",
            " [!] Character '2' not found in the vocabulary. Discarding it.\n",
            "sigir ’25, padua, italy © 2025 copyright held by the owner/authors.\n",
            " [!] Character '5' not found in the vocabulary. Discarding it.\n",
            "sigir ’25, padua, italy © 2025 copyright held by the owner/authors.\n",
            " [!] Character '©' not found in the vocabulary. Discarding it.\n",
            "sigir ’25, padua, italy © 2025 copyright held by the owner/authors.\n",
            " [!] Character '0' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 7.831524848937988\n",
            " > Real-time factor: 0.04585524070155976\n",
            " > Text splitted to sentences.\n",
            "['ACM ISBN 979-8-4007-1592-1/2025/07 https://doi.org/10.1145/3726302.3729932 demonstrate the superiority of our method.', 'The code of our method is available at https://github.com/ziqiangcui/CoLaKG.', 'CCS Concepts • Information systems → Recommender systems.', 'Keywords Large Language Models, Knowledge Graphs, Recommendation 1 Introduction The rapid advancement of online platforms has led to an increasingly critical issue of information overload.', 'Recommender systems address this problem by modeling user preferences based on historical data.', 'Collaborative filtering (CF) [9, 10, 23], as one of the most classic and efficient methods, has been extensively employed in existing recommender systems.', 'However, CF-based methods exclusively rely on user-item interaction records, often suffering from the data sparsity issue [26].', 'To address this issue, recent studies [34, 44, 46] have incorporated knowledge graphs (KGs) as external knowledge sources into recommendation models, achieving significant progress.', 'Typically, these methods capture diverse and high-order relationships between items by modeling the structure and attribute information in KGs, thereby enhancing the learning process of user and item representations [44].', 'Despite the effectiveness of existing KG-enhanced recommendation methods, they still face several challenges.', 'i) First, many KGs suffer from missing facts and limited scopes [8, 32], as constructing KGs often requires manual effort and domain expertise.', 'The absence of key attributes, such as the genres of a movie, can cause items SIGIR ’25, July 13–18, 2025, Padua, Italy Item node Attribute node x-hop 𝑟!', 'a. Missing facts: 2-hop H F Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma A 𝑟% D E 𝑟!', '𝑟$ 1-hop 𝑟!', '𝑟\" A 𝑟$ 𝑟# P G 𝑟$ C 𝑟!', 'B 𝑟!', 'C', 'b. Textual Entities (e.g., Title/Tag/Genre..) Similar semantic B D Horror ID: 51 𝑟!', 'Thriller ID: 320 E Different IDs', 'c.']\n",
            "acm isbn 979 8 4007 1592 1/2025/07 https,//doi.org/10.1145/3726302.3729932 demonstrate the superiority of our method.\n",
            " [!] Character '9' not found in the vocabulary. Discarding it.\n",
            "acm isbn 979 8 4007 1592 1/2025/07 https,//doi.org/10.1145/3726302.3729932 demonstrate the superiority of our method.\n",
            " [!] Character '7' not found in the vocabulary. Discarding it.\n",
            "acm isbn 979 8 4007 1592 1/2025/07 https,//doi.org/10.1145/3726302.3729932 demonstrate the superiority of our method.\n",
            " [!] Character '8' not found in the vocabulary. Discarding it.\n",
            "acm isbn 979 8 4007 1592 1/2025/07 https,//doi.org/10.1145/3726302.3729932 demonstrate the superiority of our method.\n",
            " [!] Character '4' not found in the vocabulary. Discarding it.\n",
            "acm isbn 979 8 4007 1592 1/2025/07 https,//doi.org/10.1145/3726302.3729932 demonstrate the superiority of our method.\n",
            " [!] Character '1' not found in the vocabulary. Discarding it.\n",
            "acm isbn 979 8 4007 1592 1/2025/07 https,//doi.org/10.1145/3726302.3729932 demonstrate the superiority of our method.\n",
            " [!] Character '3' not found in the vocabulary. Discarding it.\n",
            "acm isbn 979 8 4007 1592 1/2025/07 https,//doi.org/10.1145/3726302.3729932 demonstrate the superiority of our method.\n",
            " [!] Character '6' not found in the vocabulary. Discarding it.\n",
            "ccs concepts • information systems → recommender systems.\n",
            " [!] Character '•' not found in the vocabulary. Discarding it.\n",
            "ccs concepts • information systems → recommender systems.\n",
            " [!] Character '→' not found in the vocabulary. Discarding it.\n",
            "the absence of key attributes, such as the genres of a movie, can cause items sigir ’25, july 13–18, 2025, padua, italy item node attribute node x hop 𝑟!\n",
            " [!] Character '𝑟' not found in the vocabulary. Discarding it.\n",
            "a. missing facts, 2 hop h f ziqiang cui, yunpeng weng, xing tang, fuyuan lyu, dugang liu, xiuqiang he, and chen ma a 𝑟% d e 𝑟!\n",
            " [!] Character '%' not found in the vocabulary. Discarding it.\n",
            "𝑟$ 1 hop 𝑟!\n",
            " [!] Character '$' not found in the vocabulary. Discarding it.\n",
            "𝑟 a 𝑟$ 𝑟# p g 𝑟$ c 𝑟!\n",
            " [!] Character '#' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 1.9870858192443848\n",
            " > Real-time factor: 0.013917895800607856\n",
            " > Text splitted to sentences.\n",
            "['Distant neighbors Target item B • • C G H A 2-hop A x-hop H B, C, G have no connections with A in the KG H and A are too distant in the KG Figure 1: An illustrative diagram demonstrating the potential issues of existing KG-based recommendation methods.', 'that originally share the same attribute to lose their connections.', 'As illustrated in Figure 1, items A and B should have a connecting path (A-P-B) in the KG.', 'However, due to item A missing the P attribute, A and B are now not associated with each other.', 'Such biased knowledge will lead to suboptimal performance of the recommendation model.', 'ii) Second, existing methods [33–35, 43, 44] convert textual entities and relations into IDs for use, which can result in losing natural semantic connections between items.', 'For instance, in Figure 1, “horror” and “thriller” are two semantically related attributes of Item F and Item G, respectively.', 'However, similar semantics are not reflected in different entity IDs (51 and 320), which further results in the related items F and G having no connections in the KG.', 'iii) Third, existing methods [25, 33–35, 43, 44] struggle to capture high-order relationships in the entire KG.', 'Most of them propagate and aggregate information by stacking multiple layers of graph neural networks (GNNs).', 'The layer-by-layer propagation is not only inefficient but also accumulates a large amount of irrelevant node information, leading to the over-smoothing issue [8, 33].', 'For instance, assumming that points A and H in Figure 1 have a strong semantic connection, the considerable distance between them in the KG presents big challenges for existing methods in capturing this relation.', 'As shown in the lower left corner of Figure 1, due to the aforementioned three limitations, existing methods fail to identify the potential connections between historically interacted items and the target item through the KG, resulting in suboptimal recommendations.', 'Empowered by extensive knowledge and remarkable reasoning abilities, large language models (LLMs) have demonstrated significant promise in semantic understanding and knowledge extraction.', 'Recently, many studies have leveraged LLMs to improve recommendation models.', 'For example, some studies utilize LLMs to generate semantic representations of item profiles [22], while other studies [48] employ LLMs to determine whether a complementary relationship exists between two items, thereby recommending complementary products based on users’ historical behaviors.', 'However, these methods do not fully exploit the semantic and structural information of KGs.', 'As one of the most common and important sources of knowledge, KGs contain a wealth of semantic associations among entities and relations, which are often overlooked by existing methods that typically consider only item profiles.', 'Additionally, KGs serve as task-relevant knowledge repositories, effectively aiding LLMs in acquiring task-specific knowledge and mitigating the issue of hallucinations caused by excessive divergence.', 'Nevertheless, leveraging LLMs to handle the diverse semantic relationships in KGs is highly challenging, as it is unrealistic to input the vast number of entities and connections in KGs into a language model.']\n",
            "for instance, in figure 1, “horror” and “thriller” are two semantically related attributes of item f and item g, respectively.\n",
            " [!] Character '“' not found in the vocabulary. Discarding it.\n",
            "for instance, in figure 1, “horror” and “thriller” are two semantically related attributes of item f and item g, respectively.\n",
            " [!] Character '”' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 2.2741127014160156\n",
            " > Real-time factor: 0.010183384537677621\n",
            " > Text splitted to sentences.\n",
            "['To bridge this gap, in this paper, we propose a novel method named Comprehending Knowledge Graphs with Large Language Models for Recommendation (CoLaKG).', 'The core idea is to leverage LLMs for understanding the semantic and structural information of KGs to enhance the representation learning of items and users.', 'Our method comprises two stages:', 'i) Comprehending KGs with LLMs.', 'Given the impracticality of inputting the entire KG into an LLM, CoLaKG leverages KG information by two components: a Local KG Comprehension module which involves extracting item-centered KG subgraphs for analysis by LLMs, and a Retrieval-based Global KG Utilization module which retrieves semantically related items from the entire KG to effectively leverage global KG information.', 'ii) Incorporating semantic embeddings into the recommendation model.', 'This stage integrates the semantic embeddings of KGs with the ID embeddings, thereby leveraging both collaborative signals and semantic information.', 'First, we align and fuse the item ID representations with the item-centered KG subgraph representations.', 'Then, we employ a retrieval-augmented representation method to further enhance item representations with semantically similar neighbors from the entire KG.', 'It is important to note that these two stages are decoupled, meaning that our method does not involve LLM inference during the recommendation process.', 'This allows our method to be efficiently applied in real-world recommendation scenarios.', 'Our contributions are summarized as follows: • We propose a novel method that utilizes LLMs to comprehend and transform the semantic and structural information of KGs.', 'This approach addresses the issues of missing facts and the inability to leverage semantic information from text in current KG-based recommendation methods.', '• We propose leveraging both local KG information and global KG information with LLMs.', 'In addition, we design a retrievalaugmented method to enhance item representation learning.', '• Extensive experiments are conducted on four real-world datasets to validate the superiority of our method.', 'Further analysis demonstrates the rationale behind our approach.', '2 Related Work 2.1 Knowledge-aware Recommendation Existing knowledge-aware recommendation methods can be categorized into three types [8]: embedding-based methods, path-based methods, and GNN-based methods.', 'Embedding-based methods [3, 19, 29, 46] enhance the representations of users and items by leveraging the relations and entities within the KGs.', 'Notable examples include CKE [46], which integrates various types of side information into a collaborative filtering framework using TransR [20].', 'Another example is DKN [29], which improves news representations by combining textual embeddings of sentences and knowledge-level embeddings of entities.']\n",
            " > Processing time: 2.625232458114624\n",
            " > Real-time factor: 0.01313213741309608\n",
            " > Text splitted to sentences.\n",
            "['Path-based methods leverage KGs to explore long-range connectivity [13, 36, 45, 47].', 'For example, Personalized Entity Recommendation (PER) [45] treats a Item-centered KG Subgraphs 𝑟$ … Th eR ea de r Ka te 𝑟& … … n io pt ce In … |𝒰| Prompt Engineering 𝑣\" 𝑟($! ,$\") 𝑟 𝑁% (𝑣& ) Adapter ❄ 𝑠\\' 𝑠\"! 𝑟* Acted by |𝒱| 𝑟) Directed 𝑟( Is the genre of User 𝑠\\' Item 𝒫(#) Text Embedding Model 𝑒\" 𝑠\" 𝑣! User Preference Comprehension Retrieval-Augmented Representation Learning ❄ 𝑣! LLM … Retrieval-based Global KG Utilization 𝑟(\"! ,\"\" ) 𝑟+ Genre Text Embedding Model Assume you are a film expert … 𝑟# 𝑟! Relations 𝑟, Directed by 𝑠\" 𝒫(#) KG-based user interactions: {“Titanic”: (Titanic, Directed by, James Cameron); (Titanic, Acted by, Leonardo | Kate); (Titanic, Genres, Romance | Disaster);… } {“Inception”: (Inception, Directed by, Nolan); (Inception, Acted by, Leonardo DiCaprio | Ellen Page); (Inception, Genres, Science Fiction | Action);… } … 𝑟\" Local KG Comprehension ℎ\" 𝑠\"( 𝑠\\'( Entities other than items Addition |𝒰| Inner product ❄ Freeze U-I Modeling ℱ(-) ℎ)\" ℎ\"( … … … 𝑟# 𝑟! LLM 𝑟\\' Starred in |𝒱| User-centered Subgraphs 𝑟! 𝑟\" Prompt Engineering Assume you are a film expert … Assume you are a film expert … One-hop knowledge in the format of triples : (Titanic, Directed by, James Cameron); (Titanic, Acted by, Leonardo DiCaprio …); (Titanic, Genres, Romance); (Titanic, plot, …); … Two-hop knowledge: James Cameron also directed… ; Leonardo DiCaprio also starred in… ; Movies in the similar genres also include … ; Movies with the similar plot also include … ; ... ℎ\\' 𝛼!\" 𝑒\\' ℎ\\' ℎ)\\' 𝑦+\\'\" … t Alien lie s Ju Av 𝑟$ … & ala eo James n nc m o er he Ro m a 𝑟& C 𝑟& 𝑟& 𝑟!', 'Disaster e 𝑟\" c 𝑟& an 𝑟\" m 𝑟# 𝑟# Ro Titanic Leo 𝑟% 𝑟% nard o 𝑟% 𝑟% User-Item-Entity Graph SIGIR ’25, July 13–18, 2025, Padua, Italy 𝑠\"\" Figure 2: The framework of our proposed CoLaKG.', 'Knowledge Graph (KG) as a heterogeneous information network and extracts meta-path-based latent features to represent the connectivity between users and items along various types of relational paths.', 'MCRec [13] constructs meta-paths and learns the explicit representations of meta-paths to depict the interaction context of user-item pairs.', 'Despite their effectiveness, these approaches heavily rely on domain knowledge and human effort for metapath design.', 'Recently, GNN-based methods have been proposed, which enhance entity and relation representations by aggregating embeddings from multi-hop neighbors [30, 34, 35].', 'For instance, KGAT [34] employs the graph attention mechanism to propagate embeddings and utilizes multi-layer perceptrons to generate final recommendation scores in an end-to-end manner.', 'Similarly, KGIN [35] adopts an adaptive aggregation method to capture fine-grained user intentions.', 'Additionally, some methods [33, 43, 44, 51] employ contrastive learning to mitigate potential knowledge noise and identify informative knowledge connections.', '2.2 LLMs for Recommendation In light of the emergence of large language models and their remarkable achievements in the field of NLP, scholars have begun to explore the potential application of LLMs in recommender systems [4, 6, 39, 49].']\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '…' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '&' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '|' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '𝒰' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '𝑣' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '𝑁' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '❄' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '𝑠' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '*' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '𝒱' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '𝒫' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '𝑒' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '+' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '{' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '}' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character 'ℎ' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character 'ℱ' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '𝛼' not found in the vocabulary. Discarding it.\n",
            "for example, personalized entity recommendation per 45 treats a item centered kg subgraphs 𝑟$ … th er ea de r ka te 𝑟& … … n io pt ce in … |𝒰| prompt engineering 𝑣 𝑟$! ,$ 𝑟 𝑁% 𝑣& adapter ❄ 𝑠' 𝑠! 𝑟* acted by |𝒱| 𝑟 directed 𝑟 is the genre of user 𝑠' item 𝒫# text embedding model 𝑒 𝑠 𝑣! user preference comprehension retrieval augmented representation learning ❄ 𝑣! llm … retrieval based global kg utilization 𝑟! , 𝑟+ genre text embedding model assume you are a film expert … 𝑟# 𝑟! relations 𝑟, directed by 𝑠 𝒫# kg based user interactions, {“titanic”, titanic, directed by, james cameron, titanic, acted by, leonardo | kate, titanic, genres, romance | disaster,… } {“inception”, inception, directed by, nolan, inception, acted by, leonardo dicaprio | ellen page, inception, genres, science fiction | action,… } … 𝑟 local kg comprehension ℎ 𝑠 𝑠' entities other than items addition |𝒰| inner product ❄ freeze u i modeling ℱ ℎ ℎ … … … 𝑟# 𝑟! llm 𝑟' starred in |𝒱| user centered subgraphs 𝑟! 𝑟 prompt engineering assume you are a film expert … assume you are a film expert … one hop knowledge in the format of triples , titanic, directed by, james cameron, titanic, acted by, leonardo dicaprio …, titanic, genres, romance, titanic, plot, …, … two hop knowledge, james cameron also directed… , leonardo dicaprio also starred in… , movies in the similar genres also include … , movies with the similar plot also include … , ... ℎ' 𝛼! 𝑒' ℎ' ℎ' 𝑦+' … t alien lie s ju av 𝑟$ … & ala eo james n nc m o er he ro m a 𝑟& c 𝑟& 𝑟& 𝑟!\n",
            " [!] Character '𝑦' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 1.9469780921936035\n",
            " > Real-time factor: 0.009451579369371119\n",
            " > Text splitted to sentences.\n",
            "['Due to the powerful reasoning capabilities and extensive world knowledge of LLMs, they have been already naturally applied to zero-shot [11, 12, 31] and few-shot recommendation scenarios [2, 18].', 'In these studies, LLMs are directly used as a recommendation model [17, 50], where the output of LLMs is expected to offer a reasonable recommendation result [40].', 'However, when the dataset is sufficiently large, their performance often falls short of that achieved by traditional recommendation models.', 'Another line of research involves leveraging LLMs as feature extractors.', 'These methods [1, 14, 15, 22, 22, 37, 38, 52] generate intermediate decision results or semantic embeddings of users and items, which are then input into traditional recommendation models to produce the final recommendations.', 'Unlike existing methods, our approach aims to leverage the extensive knowledge and reasoning capabilities of LLMs to understand KGs and transform them into semantic embeddings, thereby addressing existing issues in KG-based recommender systems.', '3 Preliminaries User-Item Interaction Graph.', 'Let U and V denote the user set and item set, respectively, in a recommender system.', 'We construct a user-item bipartite graph G = {(𝑢, 𝑦𝑢𝑣 , 𝑣)|𝑢 ∈ U, 𝑣 ∈ V} to represent the collaborative signals between users and items.', 'Here, 𝑦𝑢𝑣 = 1 if user 𝑢 interacted with item 𝑣, and vice versa.', 'Knowledge Graph.', 'We capture real-world knowledge about items using a heterogeneous graph composed of triplets, represented as G𝑘 = {(ℎ, 𝑟, 𝑡)}.', 'In this context, ℎ and 𝑡 are knowledge entities belonging to the set E, while 𝑟 is a relation from the set R that links them, as exemplified by the triplet (James Cameron, directed, Titanic).', 'Notably, the item set is a subset of the entity set, denoted as V ⊂ E. This form of knowledge graph enables us to model the intricate relationships between items and entities.', 'Task Formulation.', 'Following the task format of most KG-aware recommendation models, we formulate the task as follows: Given the user-item interaction graph G and the corresponding knowledge graph G𝑘 , our objective is to learn a recommendation model that predicts the probability of user 𝑢 interacting with item 𝑣.', '4 Methodology In this section, we introduce our proposed method CoLaKG in detail.', 'An overview of our method is illustrated in Figure 2.', 'CoLaKG extracts useful information from the KG at both local and SIGIR ’25, July 13–18, 2025, Padua, Italy Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma Assume you are an expert in movie recommendation.']\n",
            "we construct a user item bipartite graph g = {𝑢, 𝑦𝑢𝑣 , 𝑣|𝑢 ∈ u, 𝑣 ∈ v} to represent the collaborative signals between users and items.\n",
            " [!] Character '=' not found in the vocabulary. Discarding it.\n",
            "we construct a user item bipartite graph g = {𝑢, 𝑦𝑢𝑣 , 𝑣|𝑢 ∈ u, 𝑣 ∈ v} to represent the collaborative signals between users and items.\n",
            " [!] Character '𝑢' not found in the vocabulary. Discarding it.\n",
            "we construct a user item bipartite graph g = {𝑢, 𝑦𝑢𝑣 , 𝑣|𝑢 ∈ u, 𝑣 ∈ v} to represent the collaborative signals between users and items.\n",
            " [!] Character '∈' not found in the vocabulary. Discarding it.\n",
            "we capture real world knowledge about items using a heterogeneous graph composed of triplets, represented as g𝑘 = {ℎ, 𝑟, 𝑡}.\n",
            " [!] Character '𝑘' not found in the vocabulary. Discarding it.\n",
            "we capture real world knowledge about items using a heterogeneous graph composed of triplets, represented as g𝑘 = {ℎ, 𝑟, 𝑡}.\n",
            " [!] Character '𝑡' not found in the vocabulary. Discarding it.\n",
            "notably, the item set is a subset of the entity set, denoted as v ⊂ e. this form of knowledge graph enables us to model the intricate relationships between items and entities.\n",
            " [!] Character '⊂' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 2.0636587142944336\n",
            " > Real-time factor: 0.011526050805082766\n",
            " > Text splitted to sentences.\n",
            "['You will be given a certain movie with its first-order information (in the form of triples) and some second-order relationships (movies related to this movie).', 'Please complete the missing knowledge, summarize the movie and analyze what kind of users would like it.', 'First-order info: (Titanic, Directed by, James Cameron), (Titanic, Acted by, Leonardo DiCaprio), … Second-order info: The movies with the same director (James Cameron) also include: Aliens, … The movies with the same actor (Leonardo DiCaprio) also include: Inception, … … Figure 3: The prompt template for Local KG comprehension.', 'global levels.', 'By employing item-centered subgraph extraction and prompt engineering, it accurately captures the local KG.', 'Subsequently, through retrieval-based neighbor enhancement, it supplements the current item by capturing related items from the entire KG, thereby effectively utilizing global information.', 'The local and global information extracted by the LLM are effectively integrated into the recommendation model through a representation fusion module and a retrieval-augmented representation learning module, respectively, thereby improving recommendation performance.', '4.1 KG Comprehension with LLMs Knowledge graphs have been widely utilized in recommender systems to provide semantic information and model latent associations between items.', 'However, KGs are predominantly manually curated, leading to missing facts and limited knowledge scopes.', 'Additionally, the highly structured nature of KGs poses challenges for utilizing textual information.', 'To address these limitations, we propose the use of LLMs to enhance the understanding and refinement of KGs for improved recommendations.', 'To fully leverage KGs, our approach comprises two components:', '1) Local KG Comprehension, which involves extracting item-centered KG subgraphs for analysis by LLMs, and', '2) Retrieval-based Global KG Utilization, which utilizes the semantic similarity of KG subgraphs to retrieve semantically related items from the entire KG, thereby leveraging global KG information.', '4.1.1 Local KG Comprehension.', 'In this part, we introduce using LLMs to comprehend local KG information.', 'Here, the local KG information of an item refers to the KG subgraph centered on that item, including connections within two hops.', 'First, we represent the first-order KG subgraph centered on each item (i.e., ego network [21]) using triples.', 'Specifically, given an item 𝑣 ∈ V, we use T𝑣 = {(𝑣, 𝑟, 𝑒)|(𝑣, 𝑟, 𝑒) ∈ G𝑘 } to denote the set of triplets where 𝑣 is the head entity.', 'In the context of recommendation, the first-order neighboring entities of an item in a KG are usually attributes.', 'Therefore, we use 𝑒 to represent these attribute entities to distinguish them from those item entities 𝑣.', 'During generating triples, in cases where the attribute or relation is absent, the term “missing” is employed as a placeholder.']\n",
            " > Processing time: 2.3341188430786133\n",
            " > Real-time factor: 0.011155329543766492\n",
            " > Text splitted to sentences.\n",
            "['Next, we consider the second-order relations in KGs.', 'The number of entities in an ego network centered on a single entity increases exponentially with the growth of the radius.', 'However, the input length of an LLM is strictly limited.', 'Consequently, including all second-order neighbors associated with the central item in the prompt becomes impractical.', 'To address this issue, we adopt a simple but effective strategy, random sampling, to explore second-order connections of 𝑣.', 'Let E𝑣 = {𝑒 | (𝑣, 𝑟, 𝑒) ∈ T𝑣 } denote the set of first-order connected neighbors of 𝑣.', 'For each 𝑒 ∈ E𝑣 , we randomly sample 𝑚 triples from the set T𝑒 to construct the triples of second-order connections, denoted as T𝑒𝑚 .', 'Here, T𝑒 = {(𝑒, 𝑟, 𝑣 ′ ) | (𝑒, 𝑟, 𝑣 ′ ) ∈ G𝑘 , 𝑣 ′ ≠ 𝑣 } represents the set of triples where 𝑒 ∈ E𝑣 is the head entity.', 'After converting first-order and second-order relationships into triples, we transform these triples into textual form.', 'For first-order relations, we concatenate all the first-order triples in T𝑣 to form a single text, denoted as D𝑣 .', 'For second-order relations, we use a template to transform the second-order triples T𝑒𝑚 into coherent sentences D𝑣′ , facilitating the understanding of the LLM.', 'In addition to D𝑣 and D𝑣′ , we have carefully designed a system prompt I𝑣 as the instruction to guide the generation.', 'By combining I𝑣 , D𝑣 , and D𝑣′ , we obtain the prompt, which is shown in Figure 3.', 'The prompt enables the LLM to fully understand, complete, and refine the KG, thereby generating the final comprehension for the 𝑣-centered KG subgraph.', 'This process can be formulated as follows: C𝑣 = LLMs(I𝑣 , D𝑣 , D𝑣′ ).', '(1) Once we have obtained the LLM’s comprehension of the KG subgraphs, we need to convert these textual answers into continuous vectors for utilization in downstream recommendation models.', 'Here, we employ a pre-trained text embedding model P to transform C𝑣 into embedding vectors s𝑣 , which can be formulated as: s𝑣 = P (C𝑣 ).', '(2) 4.1.2 Rerieval-based Global KG Utilization.', 'This section introduces the utilization of global KG information.', 'In recommendation scenarios, items that are distant in the KG can still have close semantic associations.']\n",
            "for each 𝑒 ∈ e𝑣 , we randomly sample 𝑚 triples from the set t𝑒 to construct the triples of second order connections, denoted as t𝑒𝑚 .\n",
            " [!] Character '𝑚' not found in the vocabulary. Discarding it.\n",
            "here, t𝑒 = {𝑒, 𝑟, 𝑣 ′ | 𝑒, 𝑟, 𝑣 ′ ∈ g𝑘 , 𝑣 ′ ≠ 𝑣 } represents the set of triples where 𝑒 ∈ e𝑣 is the head entity.\n",
            " [!] Character '′' not found in the vocabulary. Discarding it.\n",
            "here, t𝑒 = {𝑒, 𝑟, 𝑣 ′ | 𝑒, 𝑟, 𝑣 ′ ∈ g𝑘 , 𝑣 ′ ≠ 𝑣 } represents the set of triples where 𝑒 ∈ e𝑣 is the head entity.\n",
            " [!] Character '≠' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 2.0149734020233154\n",
            " > Real-time factor: 0.013143988271515429\n",
            " > Text splitted to sentences.\n",
            "['However, the number of KG nodes increases exponentially with the number of hops, and the input length of LLMs is limited by the number of tokens.', 'This makes it impractical to input the entire KG into an LLM directly.', 'To address this challenge, we propose a method called retrieval-based global KG utilization.', 'For each item, we have obtained the semantic embedding corresponding to its local KG.', 'Based on this, we can directly compute the semantic relationships between any two item-centered KG subgraphs.', 'specifically, we employ the cosine similarity as a metric to quantify the relations.', 'Given two different items 𝑣𝑖 and 𝑣 𝑗 , their semantical relation 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) in the global KG is computed as: 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) = sim(s𝑣𝑖 , s𝑣 𝑗 ), 𝑣𝑖 , 𝑣 𝑗 ∈ V and 𝑣𝑖 ≠ 𝑣 𝑗 , (3) where sim denotes the cosine similarity function.', 'Once we obtain the semantic associations between any two items in the entire KG, we treat the semantic similarity between the two items as the edge weight between them, allowing us to construct an item-item graph: G𝑣 = {(𝑣𝑖 , 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) , 𝑣 𝑗 )|𝑣𝑖 , 𝑣 𝑗 ∈ V, 𝑣𝑖 ≠ 𝑣 𝑗 }.', '(4) From a high-level perspective, we transform high-order associations between items in the KG into direct semantic connections on the constructed item-item graph G𝑣 .', 'Based on this foundation, it is essential to retrieve items that are semantically strongly related to the given item, as items with lower semantic relevance may introduce noise.', 'Specifically, given an item 𝑣𝑖 , we rank all other items 𝑣 𝑗 ∈ V where 𝑣 𝑗 ≠ 𝑣𝑖 in descending order based on the semantic similarity 𝑟 (𝑣𝑖 ,𝑣 𝑗 ) .', 'Subsequently, we retrieve the top-𝑘 items with the highest similarity scores, forming the neighbor set of 𝑣𝑖 : N𝑘 (𝑣𝑖 ), where 0 < 𝑘 < |V | is an adjustable hyperparameter, representing the number of retrieved neighbors.', 'In this manner, we filter out items with low semantic associations to the current item in the entire KG.', 'Traditional KG-based recommendation methods aggregate items through layer-by-layer information propagation on the KG.', 'High-order relations require many propagation and aggregation steps to be captured.', 'In contrast, our retrieval-based method can directly recall strongly semantically related neighbors of any order across the entire KG.', 'The retrieved neighbors are then leveraged to enhance item representations, which will be introduced in Section 4.3.2.', '4.2 User Preference Comprehension The introduction of KGs allows for the expansion of user-item bipartite graphs and enables us to understand user preferences from a knowledge-driven perspective.', 'Given a user 𝑢, we first extract the subgraph corresponding to user 𝑢 from the user-item bipartite graph, denoted as B𝑢 .', 'For each item 𝑣 ∈ B𝑢 , we extract its firstorder KG subgraph and represent it as a set of triples, denoted as T𝑣 .']\n",
            "given two different items 𝑣𝑖 and 𝑣 𝑗 , their semantical relation 𝑟 𝑣𝑖 ,𝑣 𝑗 in the global kg is computed as, 𝑟 𝑣𝑖 ,𝑣 𝑗 = sims𝑣𝑖 , s𝑣 𝑗 , 𝑣𝑖 , 𝑣 𝑗 ∈ v and 𝑣𝑖 ≠ 𝑣 𝑗 , 3 where sim denotes the cosine similarity function.\n",
            " [!] Character '𝑖' not found in the vocabulary. Discarding it.\n",
            "given two different items 𝑣𝑖 and 𝑣 𝑗 , their semantical relation 𝑟 𝑣𝑖 ,𝑣 𝑗 in the global kg is computed as, 𝑟 𝑣𝑖 ,𝑣 𝑗 = sims𝑣𝑖 , s𝑣 𝑗 , 𝑣𝑖 , 𝑣 𝑗 ∈ v and 𝑣𝑖 ≠ 𝑣 𝑗 , 3 where sim denotes the cosine similarity function.\n",
            " [!] Character '𝑗' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 2.466007947921753\n",
            " > Real-time factor: 0.012767981505238444\n",
            " > Text splitted to sentences.\n",
            "['We then concatenate all triples in T𝑣 to form a single text, denoted as D𝑣 .', 'The detailed approach is the same as described in Section 4.1.1.', 'Subsequently, we represent user 𝑢 with all items the user has interacted with in the training set and the corresponding knowledge triples D𝑣 : D𝑢 = ⊕𝑣 ∈ G𝑢 {name𝑣 : D𝑣 }, (5) where ⊕ denotes concatenation operation, and name𝑣 denotes the text name of item 𝑣.', 'Additionally, we have meticulously designed a system prompt, denoted as I𝑢 , to serve as an instruction for guiding the generation of user preferences.', 'By combining D𝑢 and I𝑢 , we enable the LLM to comprehend the user preference for 𝑢, which can be formulated as: C𝑢 = LLMs(I𝑢 , D𝑢 ).', '(6) Furthermore, we also utilize the text embedding function P to transform the textual answers C𝑢 into embedding vectors s𝑢 , which can be expressed as: s𝑢 = P (C𝑢 ).', '(7) 4.3 Retrieval-Augmented Representation 4.3.1 Cross-Modal Representation Alignment.', 'In a traditional recommendation model, each item and user is associated with an ID embedding.', 'Let e𝑣 ∈ R𝑑 represent the ID embedding of item 𝑣 and e𝑢 ∈ R𝑑 represent the ID embedding of user 𝑢.', 'In addition, we also obtain the semantic embedding s𝑣 ∈ R𝑑𝑠 w.r.t. the comprehension of 𝑣-centric KG subgraph, and the semantic embedding s𝑢 ∈ R𝑑𝑠 w.r.t. the comprehension of user 𝑢’s preference.', 'Since ID embeddings and semantic embeddings belong to two different modalities and typically possess different embedding dimensions, we employ a learnable adapter network to align the embedding spaces.', 'Specifically, the adapter consists of a linear map and a nonlinear activation function, formulated as: s′𝑣 = 𝜎 (W1 s𝑣 ); s𝑢′ = 𝜎 (W2 s𝑢 ), (8) where both W1 ∈ R𝑑 ×𝑑𝑠 and W2 ∈ R𝑑 ×𝑑𝑠 are are weight matrices, 𝜎 represents the non-linear activation function ELU [5].', 'Note that during the training process, we fix s𝑣 and s𝑢 , training solely the projection parameters W1 and W2 , and the parameters of the recommendation model.', 'After mapping the representations to the same space, we need to fuse the representations of the two modalities, leveraging both the collaborative signals and the semantic SIGIR ’25, July 13–18, 2025, Padua, Italy information to form a complementary representation.', 'To achieve this, we employ a straightforward mean pooling technique to fuse their embeddings: h𝑣 = 1 (e𝑣 + s′𝑣 ); 2 h𝑢 = 1 (e𝑢 + s𝑢′ ), 2 (9) where h𝑣 ∈ R𝑑 and h𝑢 ∈ R𝑑 represent the merged embeddings of item 𝑣 and user 𝑢, respectively.', '4.3.2 Item Representation Augmentation with Retrieved Neighbors.', 'For each item, we have retrieved its semantic-related items in Section 4.1.2.', 'To fully utilize these neighbors, we propose to aggregate their information to enhance item representations.']\n",
            "subsequently, we represent user 𝑢 with all items the user has interacted with in the training set and the corresponding knowledge triples d𝑣 , d𝑢 = ⊕𝑣 ∈ g𝑢 {name𝑣 , d𝑣 }, 5 where ⊕ denotes concatenation operation, and name𝑣 denotes the text name of item 𝑣.\n",
            " [!] Character '⊕' not found in the vocabulary. Discarding it.\n",
            "let e𝑣 ∈ r𝑑 represent the id embedding of item 𝑣 and e𝑢 ∈ r𝑑 represent the id embedding of user 𝑢.\n",
            " [!] Character '𝑑' not found in the vocabulary. Discarding it.\n",
            "specifically, the adapter consists of a linear map and a nonlinear activation function, formulated as, s′𝑣 = 𝜎 w1 s𝑣 , s𝑢′ = 𝜎 w2 s𝑢 , 8 where both w1 ∈ r𝑑 ×𝑑𝑠 and w2 ∈ r𝑑 ×𝑑𝑠 are are weight matrices, 𝜎 represents the non linear activation function elu 5.\n",
            " [!] Character '𝜎' not found in the vocabulary. Discarding it.\n",
            "specifically, the adapter consists of a linear map and a nonlinear activation function, formulated as, s′𝑣 = 𝜎 w1 s𝑣 , s𝑢′ = 𝜎 w2 s𝑢 , 8 where both w1 ∈ r𝑑 ×𝑑𝑠 and w2 ∈ r𝑑 ×𝑑𝑠 are are weight matrices, 𝜎 represents the non linear activation function elu 5.\n",
            " [!] Character '×' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 1.9339878559112549\n",
            " > Real-time factor: 0.01039320222219911\n",
            " > Text splitted to sentences.\n",
            "['Considering the varying contributions of different neighbors to the central item, we employ the attention mechanism.', 'Specifically, for item 𝑣𝑖 and its top-𝑘 neighbor set N𝑘 (𝑣𝑖 ), we compute attention coefficients that indicate the importance of item 𝑣 𝑗 ∈ N𝑘 (𝑣𝑖 ) to item 𝑣𝑖 as follows: 𝑤𝑖 𝑗 = 𝑎(Ws𝑣𝑖 ∥Ws𝑣 𝑗 ).', '(10)', 'Here, W ∈ R𝑑𝑎 ×𝑑 is a learnable weight matrix to capture higherlevel features of s𝑣𝑖 and s𝑣 𝑗 , ∥ is the concatenation operation, 𝑎 denotes the attention function: R𝑑𝑎 × R𝑑𝑎 → R, where we adopt a single-layer neural network and apply the LeakyReLU activation function following [27].', 'Note that the computation of attention weights is exclusively dependent on the semantic representation of items, as our objective is to calculate the semantic associations between items, rather than the associations present in collaborative signals.', 'In addition, we employ the softmax function for easy comparison of coefficients across different items: 𝛼𝑖 𝑗 = softmax 𝑗 (𝑤𝑖 𝑗 ).', '(11)', 'The attention scores 𝛼𝑖 𝑗 are then utilized to compute a linear combination of the corresponding neighbor embeddings.', 'Finally, the weighted average of neighbor embeddings and the embedding of item 𝑣𝑖 itself are combined to form the final output representation for item 𝑣𝑖 : \\x12 \\x10 \\x11\\x13 ∑︁ 1 h𝑣𝑖 + h′𝑣𝑖 = 𝜎 𝛼 𝑖 𝑗 h𝑣 𝑗 , (12) 𝑗 ∈ N𝑘 (𝑣𝑖 ) 2 where 𝜎 denotes the non-linear activation function.', '4.4 User-Item Modeling Having successfully integrated the semantic information from the KG into both user and item representations, we can use them as inputs for traditional recommendation models to generate prediction results.', 'This process can be formulated as follows: 𝑦ˆ𝑢𝑣 = F (h𝑢 , h′𝑣 ), (13) where 𝑦ˆ𝑢𝑣 is the predicted probability of user 𝑢 interacting with item 𝑣, h𝑢 is the representation for user 𝑢, h′𝑣 is the augmented representation for item 𝑣, and F denotes the function of the recommendation model.', 'Specifically, we select the classic model, LightGCN [10], as the architecture for our recommendation method due to its simplicity and effectiveness.', 'The trainable parameters of original LightGCN are only the embeddings of users and items, similar to standard SIGIR ’25, July 13–18, 2025, Padua, Italy Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma Table 1: Dataset statistics.', 'Statistics MovieLens Last-FM MIND Funds # Users # Items # Interactions 6,040 3,260 998,539 1,859 2,813 86,608 44,603 15,174 1,285,064 209,999 5,701 1,225,318 # Entities # Relations # Triples 12,068 12 62,958 32,810 14 307,140 8,111 12 65,697 5 Experiments 5.1 Experimental Settings Knowledge Graph 9,614 2 118,500 matrix factorization.', 'First, we adopt the simple weighted sum aggregator to learn the user-item interaction graph, which is defined as: (𝑙+1) h𝑢 = 1 ∑︁ √︁ 𝑣 ∈ M𝑢 (𝑙 ) |M𝑢 ||M 𝑣 | (𝑙+1) h𝑣 ; h𝑣 = 1 ∑︁ √︁ 𝑢 ∈ M𝑣 |M 𝑣 ||M𝑢 | (𝑙 ) h𝑢 , (14) (𝑙 ) (𝑙 ) where h𝑢 and h𝑣 represent the embeddings of user 𝑢 and item 𝑣 after 𝑙 layers of propagation, respectively.', 'The initial embeddings (0) (0) h𝑢 = h𝑢 and h𝑣 = h′𝑣 are obtained in Section 4.3.', 'M𝑢 denotes the set of items with which user 𝑢 has interacted, while M 𝑣 signifies the set of users who have interacted √︁ with item 𝑣.', 'The symmetric normalization term is given by 1/ |M𝑢 ||M 𝑣 |.', 'Subsequently, the embeddings acquired at each layer are combined to construct the final representation: h̃𝑢 = 𝐿 𝐿 𝑙=0 𝑙=0 1 ∑︁ (𝑙 ) 1 ∑︁ ′ (𝑙 ) h𝑢 ; h̃𝑣 = h𝑣 , 𝐿+1 𝐿+1 (15) where 𝐿 represents the number of hidden layers.', 'Ultimately, the model prediction is determined by the inner product of the final user and item representations: 𝑦ˆ𝑢𝑣 = h̃𝑢⊤ h̃𝑣 .', '4.5 (16) Model Training Our approach can be divided into two stages.', 'In the first stage, we employ the LLM to comprehend the KGs, generating corresponding semantic embeddings for each item and user, denoted as s𝑣 and s𝑢 , respectively.']\n",
            "specifically, for item 𝑣𝑖 and its top 𝑘 neighbor set n𝑘 𝑣𝑖 , we compute attention coefficients that indicate the importance of item 𝑣 𝑗 ∈ n𝑘 𝑣𝑖 to item 𝑣𝑖 as follows, 𝑤𝑖 𝑗 = 𝑎ws𝑣𝑖 ∥ws𝑣 𝑗 .\n",
            " [!] Character '𝑤' not found in the vocabulary. Discarding it.\n",
            "specifically, for item 𝑣𝑖 and its top 𝑘 neighbor set n𝑘 𝑣𝑖 , we compute attention coefficients that indicate the importance of item 𝑣 𝑗 ∈ n𝑘 𝑣𝑖 to item 𝑣𝑖 as follows, 𝑤𝑖 𝑗 = 𝑎ws𝑣𝑖 ∥ws𝑣 𝑗 .\n",
            " [!] Character '𝑎' not found in the vocabulary. Discarding it.\n",
            "specifically, for item 𝑣𝑖 and its top 𝑘 neighbor set n𝑘 𝑣𝑖 , we compute attention coefficients that indicate the importance of item 𝑣 𝑗 ∈ n𝑘 𝑣𝑖 to item 𝑣𝑖 as follows, 𝑤𝑖 𝑗 = 𝑎ws𝑣𝑖 ∥ws𝑣 𝑗 .\n",
            " [!] Character '∥' not found in the vocabulary. Discarding it.\n",
            "finally, the weighted average of neighbor embeddings and the embedding of item 𝑣𝑖 itself are combined to form the final output representation for item 𝑣𝑖 , \u0012 \u0010 \u0011\u0013 ∑︁ 1 h𝑣𝑖 + h′𝑣𝑖 = 𝜎 𝛼 𝑖 𝑗 h𝑣 𝑗 , 12 𝑗 ∈ n𝑘 𝑣𝑖 2 where 𝜎 denotes the non linear activation function.\n",
            " [!] Character '\\x12' not found in the vocabulary. Discarding it.\n",
            "finally, the weighted average of neighbor embeddings and the embedding of item 𝑣𝑖 itself are combined to form the final output representation for item 𝑣𝑖 , \u0012 \u0010 \u0011\u0013 ∑︁ 1 h𝑣𝑖 + h′𝑣𝑖 = 𝜎 𝛼 𝑖 𝑗 h𝑣 𝑗 , 12 𝑗 ∈ n𝑘 𝑣𝑖 2 where 𝜎 denotes the non linear activation function.\n",
            " [!] Character '\\x10' not found in the vocabulary. Discarding it.\n",
            "finally, the weighted average of neighbor embeddings and the embedding of item 𝑣𝑖 itself are combined to form the final output representation for item 𝑣𝑖 , \u0012 \u0010 \u0011\u0013 ∑︁ 1 h𝑣𝑖 + h′𝑣𝑖 = 𝜎 𝛼 𝑖 𝑗 h𝑣 𝑗 , 12 𝑗 ∈ n𝑘 𝑣𝑖 2 where 𝜎 denotes the non linear activation function.\n",
            " [!] Character '\\x11' not found in the vocabulary. Discarding it.\n",
            "finally, the weighted average of neighbor embeddings and the embedding of item 𝑣𝑖 itself are combined to form the final output representation for item 𝑣𝑖 , \u0012 \u0010 \u0011\u0013 ∑︁ 1 h𝑣𝑖 + h′𝑣𝑖 = 𝜎 𝛼 𝑖 𝑗 h𝑣 𝑗 , 12 𝑗 ∈ n𝑘 𝑣𝑖 2 where 𝜎 denotes the non linear activation function.\n",
            " [!] Character '\\x13' not found in the vocabulary. Discarding it.\n",
            "finally, the weighted average of neighbor embeddings and the embedding of item 𝑣𝑖 itself are combined to form the final output representation for item 𝑣𝑖 , \u0012 \u0010 \u0011\u0013 ∑︁ 1 h𝑣𝑖 + h′𝑣𝑖 = 𝜎 𝛼 𝑖 𝑗 h𝑣 𝑗 , 12 𝑗 ∈ n𝑘 𝑣𝑖 2 where 𝜎 denotes the non linear activation function.\n",
            " [!] Character '∑' not found in the vocabulary. Discarding it.\n",
            "finally, the weighted average of neighbor embeddings and the embedding of item 𝑣𝑖 itself are combined to form the final output representation for item 𝑣𝑖 , \u0012 \u0010 \u0011\u0013 ∑︁ 1 h𝑣𝑖 + h′𝑣𝑖 = 𝜎 𝛼 𝑖 𝑗 h𝑣 𝑗 , 12 𝑗 ∈ n𝑘 𝑣𝑖 2 where 𝜎 denotes the non linear activation function.\n",
            " [!] Character '︁' not found in the vocabulary. Discarding it.\n",
            "this process can be formulated as follows, 𝑦ˆ𝑢𝑣 = f h𝑢 , h′𝑣 , 13 where 𝑦ˆ𝑢𝑣 is the predicted probability of user 𝑢 interacting with item 𝑣, h𝑢 is the representation for user 𝑢, h′𝑣 is the augmented representation for item 𝑣, and f denotes the function of the recommendation model.\n",
            " [!] Character 'ˆ' not found in the vocabulary. Discarding it.\n",
            "first, we adopt the simple weighted sum aggregator to learn the user item interaction graph, which is defined as, 𝑙+1 h𝑢 = 1 ∑︁ √︁ 𝑣 ∈ m𝑢 𝑙 |m𝑢 ||m 𝑣 | 𝑙+1 h𝑣 , h𝑣 = 1 ∑︁ √︁ 𝑢 ∈ m𝑣 |m 𝑣 ||m𝑢 | 𝑙 h𝑢 , 14 𝑙 𝑙 where h𝑢 and h𝑣 represent the embeddings of user 𝑢 and item 𝑣 after 𝑙 layers of propagation, respectively.\n",
            " [!] Character '𝑙' not found in the vocabulary. Discarding it.\n",
            "first, we adopt the simple weighted sum aggregator to learn the user item interaction graph, which is defined as, 𝑙+1 h𝑢 = 1 ∑︁ √︁ 𝑣 ∈ m𝑢 𝑙 |m𝑢 ||m 𝑣 | 𝑙+1 h𝑣 , h𝑣 = 1 ∑︁ √︁ 𝑢 ∈ m𝑣 |m 𝑣 ||m𝑢 | 𝑙 h𝑢 , 14 𝑙 𝑙 where h𝑢 and h𝑣 represent the embeddings of user 𝑢 and item 𝑣 after 𝑙 layers of propagation, respectively.\n",
            " [!] Character '√' not found in the vocabulary. Discarding it.\n",
            "subsequently, the embeddings acquired at each layer are combined to construct the final representation, h̃𝑢 = 𝐿 𝐿 𝑙=0 𝑙=0 1 ∑︁ 𝑙 1 ∑︁ ′ 𝑙 h𝑢 , h̃𝑣 = h𝑣 , 𝐿+1 𝐿+1 15 where 𝐿 represents the number of hidden layers.\n",
            " [!] Character '̃' not found in the vocabulary. Discarding it.\n",
            "subsequently, the embeddings acquired at each layer are combined to construct the final representation, h̃𝑢 = 𝐿 𝐿 𝑙=0 𝑙=0 1 ∑︁ 𝑙 1 ∑︁ ′ 𝑙 h𝑢 , h̃𝑣 = h𝑣 , 𝐿+1 𝐿+1 15 where 𝐿 represents the number of hidden layers.\n",
            " [!] Character '𝐿' not found in the vocabulary. Discarding it.\n",
            "ultimately, the model prediction is determined by the inner product of the final user and item representations, 𝑦ˆ𝑢𝑣 = h̃𝑢⊤ h̃𝑣 .\n",
            " [!] Character '⊤' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 2.4758474826812744\n",
            " > Real-time factor: 0.010336445658013218\n",
            " > Text splitted to sentences.\n",
            "['In the second stage, these semantic embeddings are integrated into the recommendation model to enhance its performance.', 'Only the second stage necessitates supervised training, where we adopt the widely-used Bayesian Personalized Ranking (BPR) loss: L= ∑︁ −ln𝜎 (𝑦ˆ𝑢𝑣 + − 𝑦ˆ𝑢𝑣 − ) + 𝜆∥Θ∥ 22 .', '(17) (𝑢,𝑣 + ,𝑣 − ) ∈ O Here, O = {(𝑢, 𝑣 +, 𝑣 − )|(𝑢, 𝑣 + ) ∈ R +, (𝑢, 𝑣 − ) ∈ R − } represents the training set, R + denotes the observed (positive) interactions between user 𝑢 and item 𝑣, while R − indicates the sampled unobserved (negative) interaction set.', '𝜎 (·) is the sigmoid function.', '𝜆∥Θ∥ 22 is the regularization term, where 𝜆 serves as the weight coefficient and Θ constitutes the model parameter set.', '5.1.1 Datasets.', 'We conducted experiments on four real-world datasets, including three public datasets (MovieLens1 , MIND2 , LastFM3 ), and one industrial dataset (Fund).', 'The statistics for these datasets are presented in Table 1.', 'These datasets cover a wide range of application scenarios.', 'Specifically, MovieLens is a wellestablished benchmark that collects movie ratings provided by users.', 'MIND is a large-scale news recommendation dataset constructed from user click logs on Microsoft News.', 'Last-FM is a well-known music recommendation dataset that includes user listening history and artist tags.', 'The Fund dataset is sampled from the data of a large-scale online financial platform aiming to recommend funds for users.', 'We adopt the similar setting as numerous previous studies [10, 42], filtering out items and users with fewer than five interaction records.', 'For each dataset, we randomly select 80% of each user’s historical interactions to form the training set, while the remaining 20% constitute the test set, following [10].', 'Each observed user-item interaction is considered a positive instance, and we apply a negative sampling strategy by pairing it with one negative item that the user has not interacted with.', '5.1.2 Evaluation Metrics.', 'To evaluate the performance of the models, we employ widely recognized evaluation metrics: Recall and Normalized Discounted Cumulative Gain (NDCG), and report values of Recall@k and NDCG@k for k=10 and 20, following [10, 34].', 'To ensure unbiased evaluation, we adopt the all-ranking protocol.', 'All items that are not interacted by a user are the candidates.']\n",
            "only the second stage necessitates supervised training, where we adopt the widely used bayesian personalized ranking bpr loss, l= ∑︁ −ln𝜎 𝑦ˆ𝑢𝑣 + − 𝑦ˆ𝑢𝑣 − + 𝜆∥θ∥ 22 .\n",
            " [!] Character '−' not found in the vocabulary. Discarding it.\n",
            "only the second stage necessitates supervised training, where we adopt the widely used bayesian personalized ranking bpr loss, l= ∑︁ −ln𝜎 𝑦ˆ𝑢𝑣 + − 𝑦ˆ𝑢𝑣 − + 𝜆∥θ∥ 22 .\n",
            " [!] Character '𝜆' not found in the vocabulary. Discarding it.\n",
            "only the second stage necessitates supervised training, where we adopt the widely used bayesian personalized ranking bpr loss, l= ∑︁ −ln𝜎 𝑦ˆ𝑢𝑣 + − 𝑦ˆ𝑢𝑣 − + 𝜆∥θ∥ 22 .\n",
            " [!] Character 'θ' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 1.9297525882720947\n",
            " > Real-time factor: 0.01194242510750857\n",
            " > Text splitted to sentences.\n",
            "['5.1.3 Baseline Methods.', 'To ensure a comprehensive assessment, we compare our method with 12 baseline methods, which can be divided into three categories: classical methods (BPR-MF, NFM, LightGCN), KG-enhanced methods (CKE, RippleNet, KGAT, KGIN, KGCL, KGRec), and LLM-based methods (RLMRec, KAR, CLLM4Rec).', 'BPR-MF [23] employs matrix factorization to model users and items, and uses the pairwise Bayesian Personalized Ranking (BPR) loss to optimize the model.', 'NFM [9] is an advanced factorization model that subsumes FM [24] under neural networks.', 'LightGCN [10] facilitates message propagation between users and items by simplifying GCN [16].', 'CKE [46] is an embedding-based method that uses TransR to guide entity representation in KGs to enhance performance.', 'RippleNet [28] automatically discovers users’ hierarchical interests by iteratively propagating users’ preferences in the KG.', 'KGAT [34] designs an attentive message passing scheme over the knowledge-aware collaborative graph for node embedding fusion.', 'KGIN [35] adopts an adaptive aggregation method to capture finegrained user intentions.', 'KGCL [44] uses contrastive learning for knowledge graphs to reduce potential noise and guide user preference learning.', '1 https://grouplens.org/datasets/movielens/ 2 https://msnews.github.io/ 3 https://grouplens.org/datasets/hetrec-2011/ SIGIR ’25, July 13–18, 2025, Padua, Italy Table 2: Performance comparison of different methods, where R denotes Recall and N denotes NDCG.', 'The best results are bolded, and the second best results are underlined.', 'The results show our improvement is statistically significant with a significance level of 0.01.', 'MovieLens Model Last-FM MIND Funds R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 BPR-MF 0.1257 0.3100 0.2048 0.3062 0.1307 0.1352 0.1971 0.1685 0.0315 0.0238 0.0537 0.0310 0.4514 0.3402 0.5806 0.3809 NFM 0.1346 0.3558 0.2129 0.3379 0.2246 0.2327 0.3273 0.2830 0.0495 0.0356 0.0802 0.0458 0.4388 0.3187 0.5756 0.3651 LightGCN 0.1598 0.3901 0.2512 0.3769 0.2589 0.2799 0.3642 0.3321 0.0624 0.0492 0.0998 0.0609 0.4992 0.3778 0.6353 0.4204 CKE 0.1524 0.3783 0.2373 0.3609 0.2342 0.2545 0.3266 0.3001 0.0526 0.0417 0.0822 0.0510 0.4926 0.3702 0.6294 0.4130 RippleNet 0.1415 0.3669 0.2201 0.3423 0.2267 0.2341 0.3248 0.2861 0.0472 0.0364 0.0785 0.0451 0.4764 0.3591 0.6124 0.4003 KGAT 0.1536 0.3782 0.2451 0.3661 0.2470 0.2595 0.3433 0.3075 0.0594 0.0456 0.0955 0.0571 0.5037 0.3751 0.6418 0.4182 KGIN 0.1631 0.3959 0.2562 0.3831 0.2562 0.2742 0.3611 0.3215 0.0640 0.0518 0.1022 0.0639 0.5079 0.3857 0.6428 0.4259 KGCL 0.1554 0.3797 0.2465 0.3677 0.2599 0.2763 0.3652 0.3284 0.0671 0.0543 0.1059 0.0670 0.5071 0.3877 0.6355 0.4273 KGRec 0.1640 0.3968 0.2571 0.3842 0.2571 0.2748 0.3617 0.3251 0.0627 0.0506 0.1003 0.0625 0.5104 0.3913 0.6467 0.4304 RLMRec 0.1613 0.3920 0.2524 0.3787 0.2597 0.2812 0.3651 0.3335 0.0619 0.0486 0.0990 0.0602 0.4988 0.3784 0.6351 0.4210 KAR 0.1582 0.3869 0.2511 0.3722 0.2532 0.2770 0.3612 0.3324 0.0615 0.0480 0.1002 0.0613 0.5033 0.3812 0.6312 0.4175 CLLM4Rec 0.1563 0.3841 0.2433 0.3637 0.2571 0.2793 0.3642 0.3268 0.0631 0.0494 0.1012 0.0628 0.4996 0.3791 0.6273 0.4103 CoLaKG 0.1699 0.4130 0.2642 0.3974 0.2738 0.2948 0.3803 0.3471 0.0698 0.0562 0.1087 0.0684 0.5273 0.4012 0.6524 0.4392 Table 3: Validation of the generalizability of our method: Experimental results of integrating CoLaKG with various recommendation backbones.', 'Model MovieLens Last-FM MIND R@20 N@20 R@20 N@20 R@20 N@20 BPR-MF BPR-MF+Ours 0.2048 0.3062 0.1971 0.1685 0.0537 0.0310 0.2213 0.3255 0.2104 0.1812 0.0609 0.3986 NFM NFM+Ours 0.2129 0.3379 0.3273 0.2830 0.0802 0.0458 0.2285 0.3527 0.3478 0.2996 0.0859 0.0487 LightGCN 0.2512 0.3769 0.3642 0.3321 0.0998 0.0609 LightGCN+Ours 0.2642 0.3974 0.3803 0.3471 0.1087 0.0684 KGRec [43] is a state-of-the-art KG-based recommendation model which devises a self-supervised rationalization method to identify informative knowledge connections.', 'RLMRec [22] is an LLM-based model.', 'It directly utilizes LLMs to generate text profiles and combine them with recommendation models through contrastive learning.', 'Since their method is modelagnostic, to ensure a fair comparison, we chose LightGCN as its backbone model, consistent with our method.', 'KAR [41] is an LLM-based model, which utilizes LLMs to enhance recommender systems with open-world knowledge CLLM4Rec [52] is also an LLM-based method.', 'It combines ID information and semantic information and uses the LLM directly as the recommender to generate the recommendation results.']\n",
            " > Processing time: 2.621671438217163\n",
            " > Real-time factor: 0.012829947333939332\n",
            " > Text splitted to sentences.\n",
            "['5.1.4 Implementation Details.', 'We implement all baseline methods according to their released code.', 'The embedding size 𝑑 for all recommendation methods is set to 64 for a fair comparison.', 'All experiments are conducted with a single V100 GPU.', 'We set the batch size to 1024 for the Last-FM dataset and 4096 for the other datasets to expedite training.', 'The Dropout rate is chosen from the set {0.2, 0.4, 0.6, 0.8} for both the embedding layer and the hidden layers.', 'We employ the Adam optimizer with a learning rate of 0.001.', 'The maximum number of epochs is set to 2000.', 'The number of hidden layers for the recommendation model 𝐿 is set to 3.', 'For the LLM, we select DeepSeek-V2, a robust large language model that demonstrates exceptional performance on both standard benchmarks and open-ended generation evaluations.', 'For more detailed information about DeepSeek, please refer to their official website4 .', 'Specifically, we utilize DeepSeek-V2 by invoking its API5 .', 'To reduce text randomness of the LLM, we set the temperature 𝜏 to 0 and the top-𝑝 to 0.001.', 'For the text embedding model P, we use the pre-trained sup-simcse-roberta-large6 [7].', 'We use identical settings for the baselines that also involve LLMs and text embeddings to ensure fairness in comparison.', '5.2 Comparison Results We compare 12 baseline methods across four datasets and run each experiment five times.', 'The average results are reported in Table 2.', 'Based on the results, we make the following observations: • Our method consistently outperforms all the baseline models across all four datasets.', 'The performance ceiling of traditional methods (BPR-MF, NFM, LightGCN) is generally lower than that of KG-based methods, as the former rely solely on collaborative signals without incorporating semantic knowledge.', 'However, some KG-based methods do not perform as well as LightGCN, indicating that effectively leveraging KG is a challenging task.']\n",
            "to reduce text randomness of the llm, we set the temperature 𝜏 to 0 and the top 𝑝 to 0.001.\n",
            " [!] Character '𝜏' not found in the vocabulary. Discarding it.\n",
            "to reduce text randomness of the llm, we set the temperature 𝜏 to 0 and the top 𝑝 to 0.001.\n",
            " [!] Character '𝑝' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 1.8199739456176758\n",
            " > Real-time factor: 0.013586356308173399\n",
            " > Text splitted to sentences.\n",
            "['• Among the KG-based baselines, KGCL and KGRec are notable for incorporating self-supervised learning into general KG-based recommendation frameworks.', 'However, they struggle with missing facts, understanding semantic information, and modeling higherorder item associations within the KG.', 'In contrast, our method leverages LLMs to address these challenges without requiring 4 https://github.com/deepseek-ai/DeepSeek-V2 5 https://api-docs.deepseek.com/ 6 https://huggingface.co/princeton-nlp/sup-simcse-roberta-large SIGIR ’25, July 13–18, 2025, Padua, Italy Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma Table 4: Ablation study on all four datasets.', 'Metric w/o s𝑣 w/o s𝑢 w/o N𝑘 (𝑣) w/o D𝑣′ CoLaKG ML R@20 0.2553 0.2613 N@20 0.3811 0.3948 0.2603 0.3902 0.2628 0.3960 0.2642 0.3974 Last-FM R@20 0.3628 0.3785 N@20 0.3278 0.3465 0.3725 0.3403 0.3789 0.3459 0.3803 0.3471 MIND R@20 0.1043 0.1048 N@20 0.0640 0.0658 0.1064 0.0662 0.1076 0.0671 0.1087 0.0684 Funds R@20 0.6382 0.6481 N@20 0.4247 0.4351 0.6455 0.4305 0.6499 0.4378 0.6524 0.4392 self-supervised tasks, leading to significant improvements across all datasets and metrics.', '• For LLM-based recommendation methods, we have selected several representative approaches: RLMRec, KAR, and CLLM4Rec.', 'It is evident that these methods exhibit only marginal improvements over traditional techniques.', 'In contrast, our method demonstrates a substantial performance enhancement compared to these LLM-based baselines, thereby further validating the superiority of our approach.', 'Our method effectively leverages LLMs to comprehend both the local subgraphs and global relationships within knowledge graphs, resulting in significant performance improvements.', '5.3 Validation of the Generalizability In this section, we validate the versatility of CoLaKG.', 'Specifically, we integrate our method into three different classical recommendation model backbones and observe the performance improvements of these models across three public datasets.', 'The results, as shown in Table 3, clearly indicate significant performance improvements when our method is combined with various recommendation backbones.', 'This experiment demonstrates that our approach, which leverages LLMs to understand KGs and enhance recommendation models, can be flexibly applied to different recommendation models to improve their performance.', '5.4 Ablation Study In this section, we demonstrate the effectiveness of our model by comparing its performance with four different versions across all four datasets.', 'The results are shown in Table 4, where “w/o s𝑣 ” denotes removing the semantic embeddings of items, “w/o s𝑢 ” denotes removing the semantic embeddings of users, “w/o N𝑘 (𝑣)” means removing the neighbor augmentation of items based on the constructed item-item graph, and “w/o D𝑣′ ” means removing the second-order triples from the LLM’s prompts.', 'When the semantic embeddings of items are removed, the model’s performance significantly decreases across all datasets, underscoring the critical role of semantic information captured by LLMs from the KG.', 'Similarly, the removal of user semantic embeddings also results in a performance decline, affirming that LLMs can effectively infer user preferences from the KG.', 'Furthermore, removing N𝑘 (𝑣) leads to a performance drop across all datasets, highlighting the significance of the item representation augmentation module based on the constructed semantic-relational item-item graph.', 'Without this module, the model can only capture local KG information from Figure 4: Hyperparameter study of the number of retrieved neighbors (𝑘) and sampled number of 2-hop items within the prompt (𝑚) on four datasets.', 'item-centered subgraphs and cannot leverage the semantic relations present in the global KG.', 'The inclusion of this module facilitates the effective integration of both local and global KG information.']\n",
            " > Processing time: 2.331500768661499\n",
            " > Real-time factor: 0.009196371028626478\n",
            " > Text splitted to sentences.\n",
            "['Lastly, removing second-order KG triples from the prompts causes a slight performance decline.', 'This finding suggests that incorporating second-order information from the KG allows the LLMs to produce a higher-quality comprehension of the local KG.', '5.5 Hyperparameter Study In this section, we investigate the impact of the hyperparameter 𝑘 and 𝑚 across four datasets.', 'Here, 𝑘 represents the number of semantically related neighbors, as defined in Section 4.1.2, 𝑚 is the number of second-order neighbors used in the prompt, which is defined in Section 4.1.1.', 'The results are presented in Figure 4.', 'We observe that as 𝑘 increases, both Recall@20 and NDCG@20 initially rise and then slightly decline across all datasets.', 'The performance is worst when 𝑘 = 0 and best when 𝑘 is between 10 and 30.', 'When 𝑘 = 0, no neighbors are used, which is equivalent to the ablation study without N𝑘 (𝑣), thereby not incorporating any global semantic associations from the KG.', 'When 𝑘 > 0, the introduction of semantically related items enhances the item’s representations, leading to a noticeable improvement.', 'However, as 𝑘 continues to increase, some noise may be introduced because the relevance of neighbors decreases with their ranking.', 'Consequently, items with lower relevance may interfere with the recommendation performance.', 'Our findings suggest that a range of 10-30 neighbors is optimal.', 'As the value of 𝑚 increases, the metrics initially rise and then slightly decline.', 'When 𝑚 = 0, the prompt used to understand the KG subgraph includes only first-order neighbors, resulting in the poorest performance.', 'This indicates the positive impact of incorporating second-order neighbors.', 'However, as 𝑚 continues to grow, the marginal benefits diminish, and additional noise may be introduced, leading to a slight decrease in performance.', '5.6 Robustness to Varying Degrees of Sparsity One of the key functions of KGs is to alleviate the issue of data sparsity.', 'To further examine the robustness of our model against users with varying levels of activity, particularly its performance with less active users, we sort users based on their interaction SIGIR ’25, July 13–18, 2025, Padua, Italy Disconnected or Distant Paths to “Apollo 13” in original KG Case 1 Connected to “Apollo 13” by shared genres in original KG Apollo 13 The Right Stuff Birdy The Great Escape Top Gun Star Trek: First Contact Neighbors of “Apollo 13” Connected to “A Little Princess” by shared genres in original KG Disconnected or Distant Paths to “A Little Princess” in original KG A Simple Wish Quest for Camelot Now and Then The Story of Cinderella The Princess Bride Case 2 Figure 5: Performance comparison on different user groups, where a smaller group ID indicates fewer interaction records.', 'A Little Princess Neighbors of “A Little Princess” Figure 7: Case study.', 'Figure 6: Performance comparison using complete KG and incomplete KG on two datasets.']\n",
            " > Processing time: 2.6115119457244873\n",
            " > Real-time factor: 0.013056254103212114\n",
            " > Text splitted to sentences.\n",
            "['Models with the \"-r\" suffix indicate their performance in the presence of missing facts.', 'frequency and divide them into four equal groups.', 'A lower group ID indicates lower user activity (01 being the lowest, 04 the highest).', 'We analyze the evaluation results on two relatively sparse datasets, Last-FM and MIND, as shown in Figure', '5. By comparing our model with three representative and strong baseline models, we observe that our model consistently outperforms the baselines in each user group.', 'Notably, the improvement ratio of our model in the sparser groups (01 and 02) is higher compared to the denser groups (03 and 04).', 'For the group with the most limited data (Group 01), our model achieves the most significant lead.', 'This indicates that the average improvement of our model is primarily driven by enhancements in the sparser groups, demonstrating the positive impact of CoLaKG in addressing data sparsity.', '5.7 Robustness to Missing Facts To further demonstrate the robustness of our proposed ColaKG in scenarios where facing the challenge of missing facts in KG, we conducted comparative experiments on the Movielens and Last-FM datasets.', 'Specifically, we randomly dropped 30% of the KG entities and relations to construct datasets with significant missing facts based on the original datasets.', 'The comparison results of our method with representative KG-based baselines on the constructed datasets are shown in Figure', '6. From the experimental results, we can see that removing KG facts has a negative impact on all KG-based methods.', 'However, in the case of an incomplete KG, our method, CoLaKG, still outperforms the other baselines.', 'Furthermore, by comparing the performance degradation of different methods on incomplete and complete KGs, we find that our method experiences the smallest proportion of performance decline.', 'This demonstrates that our method can effectively mitigate the impact of missing facts on performance, further proving the robustness and superiority of the proposed approach.', '5.8 Case Study In this section, we conduct an in-depth analysis of the rationality of our method through two real cases.', 'In the first case, we present the movie “Apollo 13” and its five semantically related neighbor items in the item-item graph identified by our method.', 'The first three movies belong to the same genre as “Apollo 13”, making them 2-hop neighbors in the KG.', 'In contrast, the other two movies, “Top Gun” and “Star Trek”, do not share any genre or other attributes with “Apollo 13”, indicating they are distant or unconnected in the KG.', 'However, “Top Gun” and “Star Trek” are semantically related to “Apollo 13” as they all highlight themes of human resilience, courage, and the spirit of adventure.']\n",
            " > Processing time: 2.3408334255218506\n",
            " > Real-time factor: 0.012439331626750188\n",
            " > Text splitted to sentences.\n",
            "['Traditional KG-based recommendation methods, which rely on layer-by-layer information propagation, struggle to capture such high-order neighbors.', 'In contrast, our method leverages similarity calculations based on item-centered KG semantic embeddings, successfully identifying these two strongly related movies.', 'This demonstrates that our approach can effectively and efficiently capture semantically relevant information from the global KG.', 'In the second case, we examine the movie “A Little Princess” and its related neighbors.', 'Among the five related movies identified, “The Story of Cinderella” and “The Princess Bride” should share the same genre as “A Little Princess”.', 'However, due to missing genres in the KG, these movies lack a path to “A Little Princess” within the KG.', 'Despite this, our method successfully identifies these two movies.', 'This demonstrates that our approach, by leveraging LLMs to complete and interpret the KG, can effectively address challenges posed by missing key attributes.', '6 Conclusion In this paper, we analyze the limitations of existing KG-based recommendation methods and propose a novel approach, CoLaKG, to address these issues.', 'CoLaKG comprehends item-centered KG subgraphs to obtain semantic embeddings for both items and users.', 'These semantic embeddings are then used to construct a semantic relational item-item graph, effectively leveraging global KG information.', 'We conducted extensive experiments on four datasets to validate the effectiveness and robustness of our method.', 'The results demonstrate that our approach significantly enhances the performance of recommendation models.', '7 Acknowledgments This work was supported by the Early Career Scheme (No. CityU 21219323) and the General Research Fund (No. CityU 11220324) of the University Grants Committee (UGC), and the NSFC Young Scientists Fund (No. 9240127).', 'SIGIR ’25, July 13–18, 2025, Padua, Italy Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, and Chen Ma References [1] Arkadeep Acharya, Brijraj Singh, and Naoyuki Onoe.', '2023.', 'Llm based generation of item-description for recommendation system.']\n",
            " > Processing time: 1.8750042915344238\n",
            " > Real-time factor: 0.012199355169810886\n",
            " > Text splitted to sentences.\n",
            "['In Proceedings of the 17th ACM Conference on Recommender Systems.', '1204–1207.', '[2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He.', '2023.', 'Tallrec: An effective and efficient tuning framework to align large language model with recommendation.', 'In Proceedings of the 17th ACM Conference on Recommender Systems.', '1007–1014.', '[3] Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua.', '2019.', 'Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences.', 'In The world wide web conference.', '151–161.', '[4] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, et al. 2023.', 'When large language models meet personalization: Perspectives of challenges and opportunities.', 'arXiv preprint arXiv:2307.16376 (2023).', '[5] Djork-Arné Clevert.', '2015.', 'Fast and accurate deep network learning by exponential linear units (elus).', 'arXiv preprint arXiv:1511.07289 (2015).']\n",
            " > Processing time: 1.482104778289795\n",
            " > Real-time factor: 0.01981344034717585\n",
            " > Text splitted to sentences.\n",
            "['[6] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li.', '2023.', 'Recommender systems in the era of large language models (llms).', 'arXiv preprint arXiv:2307.02046 (2023).', '[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen.', '2021.', 'SimCSE: Simple Contrastive Learning of Sentence Embeddings.', 'In Empirical Methods in Natural Language Processing (EMNLP).', '[8] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He.', '2020.', 'A survey on knowledge graph-based recommender systems.', 'IEEE Transactions on Knowledge and Data Engineering 34, 8 (2020), 3549–3568.', '[9] Xiangnan He and Tat-Seng Chua.', '2017.', 'Neural factorization machines for sparse predictive analytics.', 'In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval.', '355–364.', '[10] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang.', '2020.', 'Lightgcn: Simplifying and powering graph convolution network for recommendation.']\n",
            " > Processing time: 1.4409723281860352\n",
            " > Real-time factor: 0.019059472093884387\n",
            " > Text splitted to sentences.\n",
            "['In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval.', '639–648.', '[11] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley.', '2023.', 'Large language models as zero-shot conversational recommenders.', 'In Proceedings of the 32nd ACM international conference on information and knowledge management.', '720–730.', '[12] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao.', '2024.', 'Large language models are zero-shot rankers for recommender systems.', 'In European Conference on Information Retrieval.', 'Springer, 364–381.', '[13] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu.', '2018.', 'Leveraging metapath based context for top-n recommendation with a neural co-attention model.', 'In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining.', '1531–1540.', '[14] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, and Jun Zhou.', '2024.', 'Enhancing sequential recommendation via llm-based semantic embedding learning.']\n",
            " > Processing time: 1.5108449459075928\n",
            " > Real-time factor: 0.016412950787680795\n",
            " > Text splitted to sentences.\n",
            "['In Companion Proceedings of the ACM on Web Conference 2024.', '103–111.', '[15] Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and Chanyoung Park.', '2024.', 'Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System.', 'In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.', '1395–1406.', '[16] Thomas N Kipf and Max Welling.', '2016.', 'Semi-supervised classification with graph convolutional networks.', 'arXiv preprint arXiv:1609.02907 (2016).', '[17] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen.', '2023.', 'Large language models for generative recommendation: A survey and visionary discussions.', 'arXiv preprint arXiv:2309.01157 (2023).', '[18] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang.', '2024.', 'Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation.', 'In Proceedings of the ACM on Web Conference 2024.', '3497–3508.']\n",
            " > Processing time: 1.4465515613555908\n",
            " > Real-time factor: 0.019366887502752516\n",
            " > Text splitted to sentences.\n",
            "['[19] Xiaolin Lin, Jinwei Luo, Junwei Pan, Weike Pan, Zhong Ming, Xun Liu, Shudong Huang, and Jie Jiang.', '2024.', 'Multi-sequence attentive user representation learning for side-information integrated sequential recommendation.', 'In Proceedings of the 17th ACM International Conference on Web Search and Data Mining.', '414–423.', '[20] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu.', '2015.', 'Learning entity and relation embeddings for knowledge graph completion.', 'In Proceedings of the AAAI conference on artificial intelligence, Vol.', '29.', '[21] Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang.', '2018.', 'Deepinf: Social influence prediction with deep learning.', 'In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining.', '2110–2119.', '[22] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang.', '2024.', 'Representation learning with large language models for recommendation.', 'In Proceedings of the ACM on Web Conference 2024.', '3464– 3475.']\n",
            " > Processing time: 1.3988687992095947\n",
            " > Real-time factor: 0.01727766413726588\n",
            " > Text splitted to sentences.\n",
            "['[23] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.', '2012.', 'BPR: Bayesian personalized ranking from implicit feedback.', 'arXiv preprint arXiv:1205.2618 (2012).', '[24] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme.', '2011.', 'Fast context-aware recommendations with factorization machines.', 'In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval.', '635–644.', '[25] Yu Tian, Yuhao Yang, Xudong Ren, Pengfei Wang, Fangzhao Wu, Qian Wang, and Chenliang Li.', '2021.', 'Joint knowledge pruning and recurrent graph convolution for news recommendation.', 'In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval.', '51–60.', '[26] Riku Togashi, Mayu Otani, and Shin’ichi Satoh.', '2021.', 'Alleviating cold-start problems in recommendation through pseudo-labelling over knowledge graph.', 'In Proceedings of the 14th ACM international conference on web search and data mining.', '931–939.', '[27] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.']\n",
            "27 petar veličković, guillem cucurull, arantxa casanova, adriana romero, pietro lio, and yoshua bengio.\n",
            " [!] Character 'č' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 1.9083268642425537\n",
            " > Real-time factor: 0.022439288653432973\n",
            " > Text splitted to sentences.\n",
            "['2017.', 'Graph attention networks.', 'arXiv preprint arXiv:1710.10903 (2017).', '[28] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo.', '2018.', 'Ripplenet: Propagating user preferences on the knowledge graph for recommender systems.', 'In Proceedings of the 27th ACM international conference on information and knowledge management.', '417–426.', '[29] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo.', '2018.', 'DKN: Deep knowledge-aware network for news recommendation.', 'In Proceedings of the 2018 world wide web conference.', '1835–1844.', '[30] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo.', '2019.', 'Knowledge graph convolutional networks for recommender systems.', 'In The world wide web conference.', '3307–3313.', '[31] Lei Wang and Ee-Peng Lim.', '2023.']\n",
            " > Processing time: 1.3751749992370605\n",
            " > Real-time factor: 0.02389946123109247\n",
            " > Text splitted to sentences.\n",
            "['Zero-shot next-item recommendation using large pretrained language models.', 'arXiv preprint arXiv:2304.03153 (2023).', '[32] Maolin Wang, Dun Zeng, Zenglin Xu, Ruocheng Guo, and Xiangyu Zhao.', '2023.', 'Federated knowledge graph completion via latent embedding sharing and tensor factorization.', 'In 2023 IEEE International Conference on Data Mining (ICDM).', 'IEEE, 1361–1366.', '[33] Shuyao Wang, Yongduo Sui, Chao Wang, and Hui Xiong.', '2024.', 'Unleashing the Power of Knowledge Graph for Recommendation via Invariant Learning.', 'In Proceedings of the ACM on Web Conference 2024.', '3745–3755.', '[34] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua.', '2019.', 'Kgat: Knowledge graph attention network for recommendation.', 'In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining.', '950–958.', '[35] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat-Seng Chua.', '2021.', 'Learning intents behind interactions with knowledge graph for recommendation.']\n",
            " > Processing time: 1.44887113571167\n",
            " > Real-time factor: 0.019612203363902618\n",
            " > Text splitted to sentences.\n",
            "['In Proceedings of the web conference 2021.', '878–887.', '[36] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng Chua.', '2019.', 'Explainable reasoning over knowledge graphs for recommendation.', 'In Proceedings of the AAAI conference on artificial intelligence, Vol.', '33.', '5329–5336.', '[37] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang.', '2024.', 'Llmrec: Large language models with graph augmentation for recommendation.', 'In Proceedings of the 17th ACM International Conference on Web Search and Data Mining.', '806–815.', '[38] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen.', '2024.', 'Exploring large language model for graph data understanding in online job recommendations.', 'In Proceedings of the AAAI Conference on Artificial Intelligence, Vol.', '38.', '9178–9186.', '[39] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al.']\n",
            " > Processing time: 1.3844947814941406\n",
            " > Real-time factor: 0.017977649996028418\n",
            " > Text splitted to sentences.\n",
            "['2023.', 'A survey on large language models for recommendation.', 'arXiv preprint arXiv:2305.19860 (2023).', '[40] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2024.', 'A survey on large language models for recommendation.', 'World Wide Web 27, 5 (2024), 60.', '[41] Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, and Yong Yu.', '2024.', 'Towards open-world recommendation with knowledge augmentation from large language models.', 'In Proceedings of the 18th ACM Conference on Recommender Systems.', '12–22.', '[42] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui.', '2022.', 'Contrastive learning for sequential recommendation.', 'In 2022 IEEE 38th international conference on data engineering (ICDE).', 'IEEE, 1259– 1273.', '[43] Yuhao Yang, Chao Huang, Lianghao Xia, and Chunzhen Huang.', '2023.', 'Knowledge graph self-supervised rationalization for recommendation.']\n",
            " > Processing time: 1.374075174331665\n",
            " > Real-time factor: 0.01786857012876195\n",
            " > Text splitted to sentences.\n",
            "['In Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining.', '3046–3056.', '[44] Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li.', '2022.', 'Knowledge graph contrastive learning for recommendation.', 'In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval.', '1434–1443.', '[45] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon Norick, and Jiawei Han.', '2014.', 'Personalized entity recommendation: A heterogeneous information network approach.', 'In Proceedings of the 7th ACM international conference on Web search and data mining.', '283–292.', '[46] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.', '2016.', 'Collaborative knowledge base embedding for recommender systems.', 'In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining.', '353–362.', '[47] Haotian Zhang, Shuanghong Shen, Bihan Xu, Zhenya Huang, Jinze Wu, Jing Sha, and Shijin Wang.', '2024.', 'Item-Difficulty-Aware Learning Path Recommendation: From a Real Walking Perspective.']\n",
            " > Processing time: 1.4323461055755615\n",
            " > Real-time factor: 0.016549730849650616\n",
            " > Text splitted to sentences.\n",
            "['In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.', '4167–4178.', '[48] Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang, and Lihong Gu.', '2024.', 'Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation SIGIR ’25, July 13–18, 2025, Padua, Italy Systems through an Inferential Knowledge Graph.', 'arXiv preprint arXiv:2402.13750 (2024).', '[49] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023.', 'A survey of large language models.', 'arXiv preprint arXiv:2303.18223 (2023).', '[50] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, et al. 2023.', 'Recommender systems in the era of large language models (llms).', 'arXiv preprint arXiv:2307.02046 (2023).', '[51] Xinjun Zhu, Yuntao Du, Yuren Mao, Lu Chen, Yujia Hu, and Yunjun Gao.', '2023.', 'Knowledge-refined Denoising Network for Robust Recommendation.', 'In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval.', '362–371.', '[52] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li.']\n",
            " > Processing time: 1.370807409286499\n",
            " > Real-time factor: 0.015702621014072476\n",
            " > Text splitted to sentences.\n",
            "['2024.', 'Collaborative large language model for recommender systems.', 'In Proceedings of the ACM on Web Conference 2024.', '3162–3172.']\n",
            " > Processing time: 0.3514533042907715\n",
            " > Real-time factor: 0.03702626467454398\n",
            "🔊 Wrote 31 audio files to /content/out4\n"
          ]
        }
      ],
      "source": [
        "# === 5) Optional: synthesize one WAV per paragraph (Coqui TTS) ===\n",
        "# - Uses the same tts instance if already created, else creates one\n",
        "# - Language is used for multilingual models (e.g., XTTS)\n",
        "\n",
        "if MAKE_AUDIO:\n",
        "    import torchaudio\n",
        "\n",
        "    if tts is None:\n",
        "        from TTS.api import TTS as _TTS\n",
        "        tts = _TTS(MODEL_NAME, gpu=USE_GPU)\n",
        "\n",
        "    for i, para in enumerate(paragraphs, 1):\n",
        "        wav_path = out_dir / f\"para_{i:0{width}d}.wav\"\n",
        "        kwargs = dict(text=para, file_path=str(wav_path), speaker_wav=\"john.mp3\")\n",
        "        if LANGUAGE:\n",
        "            kwargs[\"language\"] = LANGUAGE\n",
        "        # Most builds split internally by default; if your model supports it, it will chunk long text safely\n",
        "        tts.tts_to_file(**kwargs)\n",
        "\n",
        "    print(f\"🔊 Wrote {len(paragraphs)} audio files to {out_dir.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIjiiklaaTTC",
        "outputId": "35adfd0a-ab5f-434c-bc6d-6eef346d29e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Merged 31 files into: /content/out4/final_merged.wav\n",
            "   Sample rate: 16000 Hz | Channels: 1 | Samples: 69603808\n"
          ]
        }
      ],
      "source": [
        "# === Merge paragraph WAVs into a single file ===\n",
        "# - Looks for: OUTPUT_DIR/para_*.wav\n",
        "# - Concatenates in filename order\n",
        "# - Resamples and converts to mono if needed\n",
        "# - Writes: OUTPUT_DIR/final_merged.wav\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio.functional import resample\n",
        "from pathlib import Path\n",
        "\n",
        "# Folder where your paragraph WAVs were saved\n",
        "FINAL_NAME = \"final_merged.wav\"    # output file name\n",
        "TARGET_CHANNELS = 1                # force mono\n",
        "SILENCE_SECS = 0.0                 # optional pause between clips (set e.g. 0.2)\n",
        "\n",
        "out_dir = Path(OUTPUT_DIR)\n",
        "wav_paths = sorted(out_dir.glob(\"para_*.wav\"))\n",
        "\n",
        "if not wav_paths:\n",
        "    raise FileNotFoundError(f\"No paragraph WAVs found in {out_dir} (expected para_XXX.wav).\")\n",
        "\n",
        "# Load first file to establish target sample rate\n",
        "first_waveform, first_sr = torchaudio.load(str(wav_paths[0]))  # shape: [C, T]\n",
        "target_sr = int(first_sr)\n",
        "\n",
        "# Helper: make mono\n",
        "def to_mono(wf: torch.Tensor) -> torch.Tensor:\n",
        "    # wf: [C, T]\n",
        "    if wf.size(0) == 1:\n",
        "        return wf\n",
        "    return wf.mean(dim=0, keepdim=True)\n",
        "\n",
        "# Helper: (optional) insert silence between clips\n",
        "def make_silence(seconds: float, sr: int, channels: int = 1) -> torch.Tensor:\n",
        "    if seconds <= 0:\n",
        "        return torch.zeros((channels, 0), dtype=torch.float32)\n",
        "    num_samples = int(round(seconds * sr))\n",
        "    return torch.zeros((channels, num_samples), dtype=torch.float32)\n",
        "\n",
        "clips = []\n",
        "\n",
        "for p in wav_paths:\n",
        "    wf, sr = torchaudio.load(str(p))  # [C, T], sr\n",
        "    # Convert to mono if needed\n",
        "    wf = to_mono(wf)\n",
        "\n",
        "    # Resample if needed\n",
        "    if sr != target_sr:\n",
        "        wf = resample(wf, orig_freq=sr, new_freq=target_sr)\n",
        "\n",
        "    clips.append(wf)\n",
        "    # Optional silence between clips\n",
        "    if SILENCE_SECS > 0:\n",
        "        clips.append(make_silence(SILENCE_SECS, target_sr, TARGET_CHANNELS))\n",
        "\n",
        "# Concatenate along time dimension\n",
        "if len(clips) == 1:\n",
        "    merged = clips[0]\n",
        "else:\n",
        "    merged = torch.cat(clips, dim=1)  # [1, total_T]\n",
        "\n",
        "# Save final merged file\n",
        "final_path = out_dir / FINAL_NAME\n",
        "torchaudio.save(str(final_path), merged, target_sr)\n",
        "print(f\"✅ Merged {len(wav_paths)} files into: {final_path.resolve()}\")\n",
        "print(f\"   Sample rate: {target_sr} Hz | Channels: {merged.size(0)} | Samples: {merged.size(1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcETmIxoe5XV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2e0b37-d3b4-40e2-e9b2-0713d059fca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'1.2410.12229v3 (1).txt'   1.2410.12229v3.txt   john.mp3   out4   sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "provenance": [],
      "authorship_tag": "ABX9TyPaCQNLGiL2RHFXtfQuGcR1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}